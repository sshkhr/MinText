{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P26jqMgJh3Q9"
      },
      "source": [
        "# Data Parallel and Fully Sharded Data Parallel Training\n",
        "\n",
        "[![Open in GitHub](https://img.shields.io/badge/Open%20in-GitHub-181717?style=flat-square&logo=github)](https://github.com/sshkhr/MinText/blob/main/docs/tutorials/2_Data_Parallel_and_FSDP.ipynb)\n",
        "[![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/sshkhr/MinText/blob/main/docs/tutorials/2_Data_Parallel_and_FSDP.ipynb)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the previous tutorial, we explored the basics of JAX parallelization, including device meshes, sharded matrices, and collective operations. In this tutorial, we'll build on those concepts to explore the first parallelism strategy used in scaling models: data parallelism (DP). We will also learn about how to profile distributed machine learning in order to identify potential bottlenecks.\n"
      ],
      "metadata": {
        "id": "Y589kcC_h84I"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C_wGnB2mh3Q9"
      },
      "source": [
        "## 0. Setup\n",
        "\n",
        "Let's start by importing the necessary libraries and initializing our environment."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "si6y3Ea7h3Q-"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "# Force JAX to see 8 devices for this tutorial (only use if not using TPU runtime)\n",
        "#os.environ['XLA_FLAGS'] = '--xla_force_host_platform_device_count=8'\n",
        "\n",
        "import jax\n",
        "import jax.numpy as jnp\n",
        "from jax.sharding import Mesh, PartitionSpec as P, NamedSharding\n",
        "from jax.experimental import mesh_utils\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import time\n",
        "from functools import partial"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Check available devices\n",
        "print(f\"JAX version: {jax.__version__}\")\n",
        "print(f\"Available devices: {jax.devices()[:4]}...\")\n",
        "print(f\"Number of devices: {jax.device_count()}\")"
      ],
      "metadata": {
        "id": "IVxCl7LUigdF",
        "outputId": "3cc01c10-9e85-47de-d57f-daf295b56e40",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "JAX version: 0.5.2\n",
            "Available devices: [TpuDevice(id=0, process_index=0, coords=(0,0,0), core_on_chip=0), TpuDevice(id=1, process_index=0, coords=(0,0,0), core_on_chip=1), TpuDevice(id=2, process_index=0, coords=(1,0,0), core_on_chip=0), TpuDevice(id=3, process_index=0, coords=(1,0,0), core_on_chip=1)]...\n",
            "Number of devices: 8\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3kAeWl08h3Q-"
      },
      "source": [
        "### Model Representation\n",
        "\n",
        "For simplicity, we will start with a simple feed-forward model. The model consists of two fully-connected (or dense) layers:\n",
        "\n",
        "- **W<sub>in</sub>**: `bf16[D, F]` (up-projection)\n",
        "- **W<sub>out</sub>**: `bf16[F, D]` (down-projection)\n",
        "\n",
        "And the input and output are defined as:\n",
        "- **Input**: `bf16[B, D]`\n",
        "- **Out**: `bf16[B, D]`\n",
        "\n",
        "Where:\n",
        "- **D** = d<sub>model</sub> (input/output dimension)\n",
        "- **F** = d<sub>ff</sub> (feed-forward or hidden dimension)\n",
        "- **B** = batch size (total tokens)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "![MLP](https://github.com/jax-ml/scaling-book/blob/main/assets/img/simple-transformer.png?raw=true)\n",
        "\n",
        "<sup> Image Source: [How To Scale Your Model](https://jax-ml.github.io/scaling-book) </sup>"
      ],
      "metadata": {
        "id": "ackK14iBw8Mb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. Why Parallelism?"
      ],
      "metadata": {
        "id": "k7P0-zkOuMNM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Communication vs Computation Trade-offs\n",
        "\n",
        "The goal of scaling is to achieve **strong scaling**: linear increase in throughput with more chips. Performance depends on hiding inter-chip communication by overlapping it with useful FLOPs.\n",
        "\n",
        "We become **compute-bound** when:\n",
        "$$\\frac{T_{\\text{math}}}{T_{\\text{comms}}} > 1$$"
      ],
      "metadata": {
        "id": "HYoUSMnyxS2A"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. Data Parallel\n",
        "\n",
        "The key insight is that computation time scales with batch size, while communication time is often independent of batch size (since we transfer model weights).\""
      ],
      "metadata": {
        "id": "Wk_MulhTxMes"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5BybVQZ6h3Q-"
      },
      "source": [
        "### Data Parallelism Theory\n",
        "\n",
        "**Definition**: Activations sharded along batch dimension, parameters replicated on each device. Communication only occurs during the backward pass.\n",
        "\n",
        "**Mathematical representation**:\n",
        "$$\\text{In}[B_X, D] \\cdot_D W_{\\text{in}}[D, F] \\cdot_F W_{\\text{out}}[F, D] \\rightarrow \\text{Out}[B_X, D]$$\n",
        "\n",
        "where $B_X$ indicates the batch is sharded across $X$ devices.\n",
        "\n",
        "![Data Parallelism](https://github.com/jax-ml/scaling-book/blob/main/assets/img/data-parallelism.png?raw=true)\n",
        "\n",
        "<sup> Image Source: [How To Scale Your Model](https://jax-ml.github.io/scaling-book) </sup>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Algorithm:\n",
        "\n",
        "**Forward pass:**\n",
        "1. Tmp[B<sub>X</sub>, F] = In[B<sub>X</sub>, D] ×<sub>D</sub> W<sub>in</sub>[D, F]\n",
        "2. Out[B<sub>X</sub>, D] = Tmp[B<sub>X</sub>, F] ×<sub>F</sub> W<sub>out</sub>[F, D]\n",
        "\n",
        "**Backward pass:**\n",
        "1. dW<sub>out</sub>[F, D] = **AllReduce**(Tmp[B<sub>X</sub>, F] ×<sub>B</sub> dOut[B<sub>X</sub>, D])\n",
        "2. dW<sub>in</sub>[D, F] = **AllReduce**(In[B<sub>X</sub>, D] ×<sub>B</sub> dTmp[B<sub>X</sub>, F])\n",
        "\n",
        "**Key properties**:\n",
        "- Forward pass requires **no communication**\n",
        "- Backward pass requires **AllReduce on gradients**\n",
        "- Model parameters and optimizer states are fully replicated\n",
        "- Memory usage scales with number of devices"
      ],
      "metadata": {
        "id": "sart6ZiFx2xN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "**When do we become communication-bound?**\n",
        "\n",
        "For TPUv5p with $C = 4.6 \\times 10^{14}$ FLOPs/s and $W = 2 \\times 9 \\times 10^{10}$ bytes/s:\n",
        "\n",
        "$$\\frac{B}{X} > \\frac{C}{W_{\\text{ici}}} = 2550$$\n",
        "\n",
        "So our **batch size per chip must be at least 2,550** to avoid being communication-bound with 1D data parallelism.\n",
        "\n",
        "**Limitations**: Largest model we can train has approximately $\\text{HBM per device} / 10$ parameters (≈9B for TPUv5p with Adam optimizer).\""
      ],
      "metadata": {
        "id": "MqllNMSEyEnr"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LmEAlrsuh3Q-"
      },
      "source": [
        "# 3. Example: 8-way Data Parallel Training with Plain JAX\n",
        "\n",
        "Data parallelism is a strategy where we replicate the model across multiple devices and shard the data batch. Each device processes a portion of the batch using its copy of the model, and then we aggregate the gradients across all devices.\n",
        "\n",
        "Let's implement 8-way data parallel training, following the approach in `jax_data_parallel.py`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z1skQ8igh3Q-"
      },
      "source": [
        "### 3.1 Create (fake) data and define Model\n",
        "\n",
        "First, let's generate our synthetic dataset and simple feed-forward neural network."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "aka1dFaWh3Q-"
      },
      "outputs": [],
      "source": [
        "def get_linear_layer(key, dim_in, dim_hidden):\n",
        "  k1, k2 = jax.random.split(key)\n",
        "  W = jax.random.normal(k1, (dim_in, dim_hidden)) / jnp.sqrt(dim_in)\n",
        "  b = jax.random.normal(k2, (dim_hidden,))\n",
        "  return W, b\n",
        "\n",
        "def get_model_and_data(key, layer_sizes, batch_size):\n",
        "  keys, *keys = jax.random.split(key, len(layer_sizes))\n",
        "\n",
        "  model = list(map(get_linear_layer, keys, layer_sizes[:-1], layer_sizes[1:]))\n",
        "\n",
        "  keys, *keys = jax.random.split(key, 2)\n",
        "  input_data = jax.random.normal(keys[0], (batch_size, layer_sizes[0]))\n",
        "  target_data = jax.random.normal(keys[0], (batch_size, layer_sizes[-1]))\n",
        "\n",
        "  return model, (input_data, target_data)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "layer_sizes = [768, 4*768, 768]\n",
        "batch_size = 8192"
      ],
      "metadata": {
        "id": "y_EO4Cc99Lo6"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model, batch = get_model_and_data(jax.random.key(0), layer_sizes, batch_size)"
      ],
      "metadata": {
        "id": "rUkwAjZr-LnO"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def predict(model, inputs):\n",
        "  for W, b in model:\n",
        "    outputs = jnp.dot(inputs, W) + b\n",
        "    inputs = jnp.maximum(outputs, 0)\n",
        "  return outputs\n",
        "\n",
        "def loss(model, batch):\n",
        "  inputs, targets = batch\n",
        "  predictions = predict(model, inputs)\n",
        "  return jnp.mean(jnp.sum((predictions - targets)**2, axis=-1))"
      ],
      "metadata": {
        "id": "ftK4v74m-ZP9"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loss_jit = jax.jit(loss)\n",
        "gradfun = jax.jit(jax.grad(loss))"
      ],
      "metadata": {
        "id": "CdizzgTi_olY"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nZyEdC3Zh3Q-"
      },
      "source": [
        "### 3.2 Single-Device Baseline\n",
        "\n",
        "Let's first establish a baseline by training on a single device."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "KFq25YzKh3Q-"
      },
      "outputs": [],
      "source": [
        "batch_single = jax.device_put(batch, jax.devices()[0])\n",
        "params_single = jax.device_put(model, jax.devices()[0])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "loss_jit(params_single, batch_single)"
      ],
      "metadata": {
        "id": "a1wHGqYh_0gR",
        "outputId": "a1d32cf1-c1a7-4acb-9082-ffc3e9e2a6aa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Array(2324.8494, dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%timeit -n 5 -r 5 gradfun(params_single, batch_single)[0][0].block_until_ready()"
      ],
      "metadata": {
        "id": "bUvLRRplAUK7",
        "outputId": "64ed6be7-788d-427f-933b-5916cd711d0d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "10.8 ms ± 1.08 ms per loop (mean ± std. dev. of 5 runs, 5 loops each)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wEhK17TPh3Q_"
      },
      "source": [
        "### 3.3 8-way Data Parallel Training\n",
        "\n",
        "Now let's implement 8-way data parallel training where we'll shard the batch across 8 devices."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "mhH5pGm_h3Q_",
        "outputId": "dde9b0a0-1183-450e-a571-83107aff5243",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mesh shape: OrderedDict([('batch', 8)])\n",
            "Mesh axis names: ('batch',)\n"
          ]
        }
      ],
      "source": [
        "# Create an 8-device mesh for data parallelism\n",
        "mesh = jax.make_mesh((8,), ('batch',))\n",
        "print(f\"Mesh shape: {mesh.shape}\")\n",
        "print(f\"Mesh axis names: {mesh.axis_names}\")\n",
        "\n",
        "# Create sharding specifications\n",
        "\n",
        "## Shard data along the batch dimension\n",
        "batch_sharding = NamedSharding(mesh, P('batch'))\n",
        "## Replicate parameters across all devices\n",
        "replicated_sharding = NamedSharding(mesh, P())"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "batch = jax.device_put(batch, batch_sharding)\n",
        "params = jax.device_put(model, replicated_sharding)"
      ],
      "metadata": {
        "id": "6I47F3-tADf6"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "HZ4VRe8bh3Q_",
        "outputId": "21f98245-7b04-444b-9c57-6c2481979433",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Array(2324.849, dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ],
      "source": [
        "loss_jit(params, batch)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%timeit -n 5 -r 5 gradfun(params, batch)[0][0].block_until_ready()"
      ],
      "metadata": {
        "id": "tdS8RpmEAJWh",
        "outputId": "c5594135-dd16-484d-c03b-87f65e6cef74",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3.29 ms ± 60.7 µs per loop (mean ± std. dev. of 5 runs, 5 loops each)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "step_size = 1e-4\n",
        "\n",
        "for _ in range(1000):\n",
        "  grads = gradfun(params, batch)\n",
        "  params = [(W - step_size * dW, b - step_size * db)\n",
        "            for (W, b), (dW, db) in zip(params, grads)]\n",
        "\n",
        "print(loss_jit(params, batch))"
      ],
      "metadata": {
        "id": "3kmUnp4fRw8i",
        "outputId": "d6ad98e4-20ca-44e0-98b1-da6c996b9951",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "527.20404\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T1R8mquih3Q_"
      },
      "source": [
        "### 1.4 Visualizing Data Sharding\n",
        "\n",
        "Let's visualize how the batch is sharded across devices."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "K0X-gnLLh3Q_",
        "outputId": "fd177b0e-0966-4783-a9a5-922e1f2b11ee",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visualizing batch sharding across 8 devices:\n",
            "Original batch shape: (8192, 768)\n",
            "TPU_0(process=0,(0,0,0,0)) slice(0, 1024, None) (1024, 768)\n",
            "TPU_1(process=0,(0,0,0,1)) slice(1024, 2048, None) (1024, 768)\n",
            "TPU_2(process=0,(1,0,0,0)) slice(2048, 3072, None) (1024, 768)\n",
            "TPU_3(process=0,(1,0,0,1)) slice(3072, 4096, None) (1024, 768)\n",
            "TPU_6(process=0,(1,1,0,0)) slice(4096, 5120, None) (1024, 768)\n",
            "TPU_7(process=0,(1,1,0,1)) slice(5120, 6144, None) (1024, 768)\n",
            "TPU_4(process=0,(0,1,0,0)) slice(6144, 7168, None) (1024, 768)\n",
            "TPU_5(process=0,(0,1,0,1)) slice(7168, 8192, None) (1024, 768)\n"
          ]
        }
      ],
      "source": [
        "print(\"Visualizing batch sharding across 8 devices:\")\n",
        "print(\"Original batch shape:\", batch[0].shape)\n",
        "\n",
        "for shard in batch[0].addressable_shards:\n",
        "  print(shard.device, shard.index[0], shard.data.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rLuPnH66h3Q_"
      },
      "source": [
        "## 2. Data Parallel Training with Flax NNX\n",
        "\n",
        "Now let's implement the same 8-way data parallel training using Flax NNX, which provides higher-level abstractions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "4E8YR_rlh3Q_"
      },
      "outputs": [],
      "source": [
        "# Import Flax NNX\n",
        "try:\n",
        "    import flax.nnx as nnx\n",
        "    import optax\n",
        "except ImportError:\n",
        "    !pip install -q flax optax\n",
        "    import flax.nnx as nnx\n",
        "    import optax"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "K9Wlxs67h3Q_"
      },
      "outputs": [],
      "source": [
        "# create model\n",
        "class MLP(nnx.Module):\n",
        "  def __init__(self, din, dmid, dout, *, rngs: nnx.Rngs):\n",
        "    self.linear1 = nnx.Linear(din, dmid, rngs=rngs)\n",
        "    self.linear2 = nnx.Linear(dmid, dout, rngs=rngs)\n",
        "\n",
        "  def __call__(self, x):\n",
        "    return self.linear2(nnx.relu(self.linear1(x)))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = MLP(1, 64, 1, rngs=nnx.Rngs(0))\n",
        "optimizer = nnx.Optimizer(model, optax.adamw(1e-2))\n",
        "\n",
        "# replicate state\n",
        "state = nnx.state((model, optimizer))\n",
        "state = jax.device_put(state, replicated_sharding)\n",
        "nnx.update((model, optimizer), state)"
      ],
      "metadata": {
        "id": "Ittq3UFPZ_zY"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# visualize model sharding\n",
        "print('model sharding')\n",
        "jax.debug.visualize_array_sharding(model.linear1.kernel.value)"
      ],
      "metadata": {
        "id": "4LM_wq6kaLtU",
        "outputId": "40cfacca-602b-4096-d073-685f9d8f8d22",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 216
        }
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "model sharding\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[38;2;255;255;255;48;2;57;59;121m                                                                                \u001b[0m\n",
              "\u001b[38;2;255;255;255;48;2;57;59;121m                                                                                \u001b[0m\n",
              "\u001b[38;2;255;255;255;48;2;57;59;121m                                                                                \u001b[0m\n",
              "\u001b[38;2;255;255;255;48;2;57;59;121m                                                                                \u001b[0m\n",
              "\u001b[38;2;255;255;255;48;2;57;59;121m                                                                                \u001b[0m\n",
              "\u001b[38;2;255;255;255;48;2;57;59;121m                              \u001b[0m\u001b[38;2;255;255;255;48;2;57;59;121mTPU 0,1,2,3,4,5,6,7\u001b[0m\u001b[38;2;255;255;255;48;2;57;59;121m                               \u001b[0m\n",
              "\u001b[38;2;255;255;255;48;2;57;59;121m                                                                                \u001b[0m\n",
              "\u001b[38;2;255;255;255;48;2;57;59;121m                                                                                \u001b[0m\n",
              "\u001b[38;2;255;255;255;48;2;57;59;121m                                                                                \u001b[0m\n",
              "\u001b[38;2;255;255;255;48;2;57;59;121m                                                                                \u001b[0m\n",
              "\u001b[38;2;255;255;255;48;2;57;59;121m                                                                                \u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #393b79\">                                                                                </span>\n",
              "<span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #393b79\">                                                                                </span>\n",
              "<span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #393b79\">                                                                                </span>\n",
              "<span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #393b79\">                                                                                </span>\n",
              "<span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #393b79\">                                                                                </span>\n",
              "<span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #393b79\">                              TPU 0,1,2,3,4,5,6,7                               </span>\n",
              "<span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #393b79\">                                                                                </span>\n",
              "<span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #393b79\">                                                                                </span>\n",
              "<span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #393b79\">                                                                                </span>\n",
              "<span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #393b79\">                                                                                </span>\n",
              "<span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #393b79\">                                                                                </span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "@nnx.jit\n",
        "def train_step(model: MLP, optimizer: nnx.Optimizer, x, y):\n",
        "  def loss_fn(model: MLP):\n",
        "    y_pred = model(x)\n",
        "    return jnp.mean((y - y_pred) ** 2)\n",
        "\n",
        "  loss, grads = nnx.value_and_grad(loss_fn)(model)\n",
        "  optimizer.update(grads)\n",
        "  return loss\n",
        "\n",
        "\n",
        "def dataset(steps, batch_size):\n",
        "  for _ in range(steps):\n",
        "    x = np.random.uniform(-2, 2, size=(batch_size, 1))\n",
        "    y = 0.8 * x**2 + 0.1 + np.random.normal(0, 0.1, size=x.shape)\n",
        "    yield x, y"
      ],
      "metadata": {
        "id": "v1mGmZygaS6V"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for step, (x, y) in enumerate(dataset(1000, 16)):\n",
        "  # shard data\n",
        "  x, y = jax.device_put((x, y), batch_sharding)\n",
        "  # train\n",
        "  loss = train_step(model, optimizer, x, y)\n",
        "\n",
        "  if step == 0:\n",
        "    print('data sharding')\n",
        "    jax.debug.visualize_array_sharding(x)\n",
        "\n",
        "  if step % 100 == 0:\n",
        "    print(f'step={step}, loss={loss}')"
      ],
      "metadata": {
        "id": "OwSJkENmaCpK",
        "outputId": "a1ba28c8-5021-4ae7-c48f-0beafad06de7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 477
        }
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "data sharding\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[38;2;255;255;255;48;2;57;59;121m  \u001b[0m\u001b[38;2;255;255;255;48;2;57;59;121mTPU 0\u001b[0m\u001b[38;2;255;255;255;48;2;57;59;121m  \u001b[0m\n",
              "\u001b[38;2;255;255;255;48;2;57;59;121m         \u001b[0m\n",
              "\u001b[38;2;255;255;255;48;2;214;97;107m  \u001b[0m\u001b[38;2;255;255;255;48;2;214;97;107mTPU 1\u001b[0m\u001b[38;2;255;255;255;48;2;214;97;107m  \u001b[0m\n",
              "\u001b[38;2;255;255;255;48;2;214;97;107m         \u001b[0m\n",
              "\u001b[38;2;255;255;255;48;2;140;162;82m  \u001b[0m\u001b[38;2;255;255;255;48;2;140;162;82mTPU 2\u001b[0m\u001b[38;2;255;255;255;48;2;140;162;82m  \u001b[0m\n",
              "\u001b[38;2;255;255;255;48;2;140;162;82m         \u001b[0m\n",
              "\u001b[38;2;255;255;255;48;2;222;158;214m  \u001b[0m\u001b[38;2;255;255;255;48;2;222;158;214mTPU 3\u001b[0m\u001b[38;2;255;255;255;48;2;222;158;214m  \u001b[0m\n",
              "\u001b[38;2;255;255;255;48;2;222;158;214m         \u001b[0m\n",
              "\u001b[38;2;0;0;0;48;2;231;203;148m  \u001b[0m\u001b[38;2;0;0;0;48;2;231;203;148mTPU 6\u001b[0m\u001b[38;2;0;0;0;48;2;231;203;148m  \u001b[0m\n",
              "\u001b[38;2;0;0;0;48;2;231;203;148m         \u001b[0m\n",
              "\u001b[38;2;255;255;255;48;2;107;110;207m  \u001b[0m\u001b[38;2;255;255;255;48;2;107;110;207mTPU 7\u001b[0m\u001b[38;2;255;255;255;48;2;107;110;207m  \u001b[0m\n",
              "\u001b[38;2;255;255;255;48;2;107;110;207m         \u001b[0m\n",
              "\u001b[38;2;255;255;255;48;2;165;81;148m  \u001b[0m\u001b[38;2;255;255;255;48;2;165;81;148mTPU 4\u001b[0m\u001b[38;2;255;255;255;48;2;165;81;148m  \u001b[0m\n",
              "\u001b[38;2;255;255;255;48;2;165;81;148m         \u001b[0m\n",
              "\u001b[38;2;255;255;255;48;2;140;109;49m  \u001b[0m\u001b[38;2;255;255;255;48;2;140;109;49mTPU 5\u001b[0m\u001b[38;2;255;255;255;48;2;140;109;49m  \u001b[0m\n",
              "\u001b[38;2;255;255;255;48;2;140;109;49m         \u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #393b79\">  TPU 0  </span>\n",
              "<span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #393b79\">         </span>\n",
              "<span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #d6616b\">  TPU 1  </span>\n",
              "<span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #d6616b\">         </span>\n",
              "<span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #8ca252\">  TPU 2  </span>\n",
              "<span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #8ca252\">         </span>\n",
              "<span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #de9ed6\">  TPU 3  </span>\n",
              "<span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #de9ed6\">         </span>\n",
              "<span style=\"color: #000000; text-decoration-color: #000000; background-color: #e7cb94\">  TPU 6  </span>\n",
              "<span style=\"color: #000000; text-decoration-color: #000000; background-color: #e7cb94\">         </span>\n",
              "<span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #6b6ecf\">  TPU 7  </span>\n",
              "<span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #6b6ecf\">         </span>\n",
              "<span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #a55194\">  TPU 4  </span>\n",
              "<span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #a55194\">         </span>\n",
              "<span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #8c6d31\">  TPU 5  </span>\n",
              "<span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #8c6d31\">         </span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step=0, loss=3.0337131023406982\n",
            "step=100, loss=0.04082110896706581\n",
            "step=200, loss=0.03399622440338135\n",
            "step=300, loss=0.016794903203845024\n",
            "step=400, loss=0.02406255342066288\n",
            "step=500, loss=0.013983037322759628\n",
            "step=600, loss=0.01109331101179123\n",
            "step=700, loss=0.024920038878917694\n",
            "step=800, loss=0.012097254395484924\n",
            "step=900, loss=0.008098714053630829\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# dereplicate state\n",
        "state = nnx.state((model, optimizer))\n",
        "state = jax.device_get(state)\n",
        "nnx.update((model, optimizer), state)"
      ],
      "metadata": {
        "id": "CcuXaSshahKZ"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X, Y = next(dataset(1, 1000))\n",
        "x_range = np.linspace(X.min(), X.max(), 100)[:, None]\n",
        "y_pred = model(x_range)\n",
        "\n",
        "# plot\n",
        "plt.scatter(X, Y, label='data')\n",
        "plt.plot(x_range, y_pred, color='black', label='model')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "1FrQzqIoajtL",
        "outputId": "cccaf6b8-a7f8-4d91-ab75-f593861645a3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 430
        }
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2kmQDk0Fh3Q_"
      },
      "source": [
        "### Fully Sharded Data Parallelism (FSDP) Theory\n",
        "\n",
        "**Definition**: Activations, weights, and optimizer states are sharded along batch dimension. Weights are gathered just-in-time before use.\n",
        "\n",
        "**Mathematical representation**:\n",
        "$$\\text{In}[B_X, D] \\cdot_D W_{\\text{in}}[D_X, F] \\cdot_F W_{\\text{out}}[F, D_X] \\rightarrow \\text{Out}[B_X, D]$$\n",
        "\n",
        "where both batch and weight dimensions are sharded across $X$ devices.\n",
        "\n",
        "![FSDP](https://github.com/jax-ml/scaling-book/blob/main/assets/img/fsdp.png?raw=true)\n",
        "\n",
        "<sup> Image Source: [How To Scale Your Model](https://jax-ml.github.io/scaling-book) </sup>\n",
        "\n",
        "**Key properties**:\n",
        "- **Drastically reduces per-device memory usage**\n",
        "- Saves on backward pass FLOPs\n",
        "- Decomposes AllReduce into AllGather + ReduceScatter\n",
        "- Same communication cost as pure data parallelism\n",
        "- Also called \\\"ZeRO sharding\\\" (ZeRO-3 shards parameters, gradients, and optimizer states)\n",
        "\n",
        "**Algorithm**:\n",
        "\n",
        "**Forward pass:**\n",
        "1. W<sub>in</sub>[D, F] = **AllGather**(W<sub>in</sub>[D<sub>X</sub>, F])\n",
        "2. Tmp[B<sub>X</sub>, F] = In[B<sub>X</sub>, D] ×<sub>D</sub> W<sub>in</sub>[D, F]\n",
        "3. W<sub>out</sub>[F, D] = **AllGather**(W<sub>out</sub>[F, D<sub>X</sub>])\n",
        "4. Out[B<sub>X</sub>, D] = Tmp[B<sub>X</sub>, F] ×<sub>F</sub> W<sub>out</sub>[F, D]\n",
        "\n",
        "**Backward pass:**\n",
        "1. dW<sub>out</sub>[F, D<sub>X</sub>] = **ReduceScatter**(Tmp[B<sub>X</sub>, F] ×<sub>B</sub> dOut[B<sub>X</sub>, D])\n",
        "2. dW<sub>in</sub>[D<sub>X</sub>, F] = **ReduceScatter**(dTmp[B<sub>X</sub>, F] ×<sub>B</sub> In[B<sub>X</sub>, D])\n",
        "\n",
        "**Communication Analysis**:\n",
        "\n",
        "FSDP has the **same roofline as pure data parallelism** because:\n",
        "- AllReduce = AllGather + ReduceScatter\n",
        "- Total communication volume is identical\n",
        "- Same condition: $\\frac{B}{X} > \\frac{C}{W_{\\text{ici}}} = 2550$\n",
        "\n",
        "**Benefits**:\n",
        "- Memory reduction: Parameters and optimizer states sharded across devices\n",
        "- Zero overhead: Same FLOPs-to-communication ratio\n",
        "- Can upgrade from data parallelism without performance loss\n",
        "- Essential for models > 9B parameters\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "75jVrHq7h3Q_"
      },
      "source": [
        "## 3. Fully Sharded Data Parallel (FSDP) Training with Flax NNX\n",
        "\n",
        "Now let's implement FSDP where we shard both the data and model parameters across devices. This is especially useful for large models that don't fit on a single device."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-KATtRorh3Q_"
      },
      "source": [
        "### Understanding FSDP Memory Benefits\n",
        "\n",
        "The diagram below shows how FSDP reduces memory usage compared to pure data parallelism:\n",
        "\n",
        "![FSDP Memory Comparison](https://github.com/jax-ml/scaling-book/blob/main/assets/img/fsdp-figure.png?raw=true)\n",
        "\n",
        "<sup> Image Source: [How To Scale Your Model](https://jax-ml.github.io/scaling-book) </sup>\n",
        "\n",
        "The rows show:\n",
        "1. **Pure Data Parallelism**: Parameters and optimizer states fully replicated\n",
        "2. **ZeRO-1**: Optimizer states sharded\n",
        "3. **ZeRO-2**: Optimizer states and gradients sharded  \n",
        "4. **ZeRO-3 (FSDP)**: Parameters, gradients, and optimizer states all sharded\n",
        "\n",
        "**Why FSDP matters**: Standard data parallelism involves significant duplicated work and memory. With FSDP:\n",
        "- Each device only stores 1/N of the parameters\n",
        "- Each device only updates 1/N of the optimizer state\n",
        "- AllGather parameters as needed for forward pass\n",
        "- ReduceScatter gradients for efficient updates\n",
        "\n",
        "This enables training much larger models that wouldn't fit with pure data parallelism.\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n0JFd67Dh3Q_"
      },
      "outputs": [],
      "source": [
        "# Create a 2D mesh for FSDP\n",
        "# We'll use 2 devices for data parallelism and 4 for model parallelism\n",
        "fsdp_mesh = jax.sharding.Mesh(\n",
        "    mesh_utils.create_device_mesh((2, 4)),\n",
        "    ('data', 'model')\n",
        ")\n",
        "print(f\"FSDP mesh shape: {fsdp_mesh.shape}\")\n",
        "print(f\"FSDP mesh axis names: {fsdp_mesh.axis_names}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YZ6MhzDeh3Q_"
      },
      "outputs": [],
      "source": [
        "# Define a custom MLP with explicit parameter sharding for FSDP\n",
        "import dataclasses\n",
        "from typing import Optional\n",
        "\n",
        "@dataclasses.dataclass(unsafe_hash=True)\n",
        "class MeshRules:\n",
        "    \"\"\"Rules for how to shard different parts of the model.\"\"\"\n",
        "    input_dim: Optional[str] = None\n",
        "    output_dim: Optional[str] = 'model'\n",
        "    bias: Optional[str] = 'model'\n",
        "\n",
        "mesh_rules = MeshRules()\n",
        "\n",
        "class FSDP_MLP(nnx.Module):\n",
        "    \"\"\"MLP with explicit parameter sharding for FSDP.\"\"\"\n",
        "\n",
        "    def __init__(self, layer_sizes, rngs: nnx.Rngs):\n",
        "        self.layer_sizes = layer_sizes\n",
        "\n",
        "        # Create layers with sharded parameters\n",
        "        self.weights = []\n",
        "        self.biases = []\n",
        "\n",
        "        for i in range(len(layer_sizes) - 1):\n",
        "            in_dim, out_dim = layer_sizes[i], layer_sizes[i + 1]\n",
        "\n",
        "            # Initialize weight with sharding spec\n",
        "            w = nnx.Param(\n",
        "                nnx.initializers.lecun_normal()(rngs.params(), (in_dim, out_dim)),\n",
        "                sharding=(mesh_rules.input_dim, mesh_rules.output_dim)\n",
        "            )\n",
        "\n",
        "            # Initialize bias with sharding spec\n",
        "            # For the last layer with output_dim=1, we don't shard\n",
        "            if out_dim == 1:\n",
        "                b = nnx.Param(\n",
        "                    jnp.zeros((out_dim,)),\n",
        "                    sharding=(None,)  # Don't shard single-element bias\n",
        "                )\n",
        "            else:\n",
        "                b = nnx.Param(\n",
        "                    jnp.zeros((out_dim,)),\n",
        "                    sharding=(mesh_rules.bias,)\n",
        "                )\n",
        "\n",
        "            self.weights.append(w)\n",
        "            self.biases.append(b)\n",
        "\n",
        "    def __call__(self, x):\n",
        "        activations = x\n",
        "\n",
        "        # Apply each layer\n",
        "        for i in range(len(self.weights) - 1):\n",
        "            activations = jnp.dot(activations, self.weights[i]) + self.biases[i]\n",
        "            activations = jax.nn.relu(activations)\n",
        "\n",
        "        # Last layer\n",
        "        return jnp.dot(activations, self.weights[-1]) + self.biases[-1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8OFI2_KRh3Q_"
      },
      "outputs": [],
      "source": [
        "# Custom SGD optimizer for FSDP (based on flax_nnx_dp_fsdp.py)\n",
        "class SGDState(nnx.Variable):\n",
        "    pass\n",
        "\n",
        "class SGD(nnx.Object):\n",
        "    def __init__(self, params: nnx.State, lr, decay=0.9):\n",
        "        def init_optimizer_state(variable: nnx.Variable):\n",
        "            return SGDState(\n",
        "                jnp.zeros_like(variable.value), **variable.get_metadata()\n",
        "            )\n",
        "\n",
        "        self.lr = lr\n",
        "        self.params = params\n",
        "        self.momentum: nnx.State = jax.tree.map(init_optimizer_state, self.params)\n",
        "        self.decay = decay\n",
        "\n",
        "    def update(self, grads: nnx.State):\n",
        "        def update_fn(params: nnx.Variable, momentum: SGDState, grad: nnx.VariableState):\n",
        "            # Momentum update\n",
        "            momentum.value = self.decay * momentum + (1 - self.decay) * grad.value\n",
        "            # Parameter update\n",
        "            params.value -= self.lr * momentum\n",
        "\n",
        "        jax.tree.map(update_fn, self.params, self.momentum, grads)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OsMxfnzDh3Q_"
      },
      "source": [
        "### Mixed FSDP and Tensor Parallelism\n",
        "\n",
        "For even larger models or smaller batch sizes, we can combine FSDP with tensor parallelism using a 2D mesh:\n",
        "\n",
        "![Mixed FSDP and Tensor Parallelism](https://github.com/jax-ml/scaling-book/blob/main/assets/img/mixed-fsdp-model-parallelism.png?raw=true)\n",
        "\n",
        "<sup> Image Source: [How To Scale Your Model](https://jax-ml.github.io/scaling-book) </sup>\n",
        "\n",
        "**Mathematical representation**:\n",
        "$$\\text{In}[B_X, D_Y] \\cdot_D W_{\\text{in}}[D_X, F_Y] \\cdot_F W_{\\text{out}}[F_Y, D_X] \\rightarrow \\text{Out}[B_X, D_Y]$$\n",
        "\n",
        "where:\n",
        "- $X$ = data/FSDP parallelism dimension\n",
        "- $Y$ = model/tensor parallelism dimension\n",
        "\n",
        "**Key insight**:\n",
        "- **FSDP moves weights** (communication scales with weight size)\n",
        "- **Tensor parallelism moves activations** (communication scales with activation size)\n",
        "- As batch size shrinks, activations get smaller → tensor parallelism becomes cheaper\n",
        "- As we add more tensor parallelism, weight gathers get smaller → FSDP becomes cheaper\n",
        "\n",
        "**Optimal sharding**: For $N = X \\times Y$ total chips:\n",
        "$$X_{\\text{opt}} = \\sqrt{\\frac{B}{F} \\frac{M_X}{M_Y} N}$$\n",
        "\n",
        "where $M_X$ and $M_Y$ are the number of mesh axes for FSDP and tensor parallelism respectively.\n",
        "\n",
        "**Communication bound condition**:\n",
        "$$\\frac{B}{N} > \\frac{4\\alpha^2}{M_X M_Y F}$$\n",
        "\n",
        "where $\\alpha = C/W_{\\text{ici}} = 2550$ for TPUv5p.\n",
        "\n",
        "This allows batch sizes as low as ~400 tokens per chip, roughly 2× better than pure FSDP.\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PIfGspLWh3RA"
      },
      "outputs": [],
      "source": [
        "# Create and initialize FSDP model\n",
        "@nnx.jit\n",
        "def create_fsdp_model():\n",
        "    # Use smaller layer sizes for FSDP demo to avoid memory issues\n",
        "    fsdp_layer_sizes = [input_dim, 1024, 512, 1]\n",
        "    model = FSDP_MLP(fsdp_layer_sizes, rngs=nnx.Rngs(0))\n",
        "    optimizer = SGD(nnx.variables(model, nnx.Param), learning_rate, decay=0.9)\n",
        "\n",
        "    # Get sharding specifications for the state\n",
        "    state = nnx.state(optimizer)\n",
        "\n",
        "    def get_named_shardings(path: tuple, value: nnx.VariableState):\n",
        "        if hasattr(value, 'sharding') and value.sharding is not None:\n",
        "            return value.replace(NamedSharding(fsdp_mesh, P(*value.sharding)))\n",
        "        return value\n",
        "\n",
        "    named_shardings = nnx.map_state(get_named_shardings, state)\n",
        "    sharded_state = jax.lax.with_sharding_constraint(state, named_shardings)\n",
        "    nnx.update(optimizer, sharded_state)\n",
        "\n",
        "    return model, optimizer\n",
        "\n",
        "print(\"Creating FSDP model...\")\n",
        "fsdp_model, fsdp_optimizer = create_fsdp_model()\n",
        "\n",
        "# Visualize parameter sharding\n",
        "print(\"\n",
        "Weight sharding (distributed across model axis):\")\n",
        "jax.debug.visualize_array_sharding(fsdp_model.weights[0].value)\n",
        "print(\"\n",
        "Momentum sharding:\")\n",
        "jax.debug.visualize_array_sharding(fsdp_optimizer.momentum['weights'][0].value)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kVmJ2hU6h3RA"
      },
      "outputs": [],
      "source": [
        "# FSDP training step\n",
        "@nnx.jit\n",
        "def fsdp_train_step(model: FSDP_MLP, optimizer: SGD, x_batch, y_batch):\n",
        "    \"\"\"FSDP training step.\"\"\"\n",
        "    def loss_fn(model):\n",
        "        predictions = model(x_batch)\n",
        "        return jnp.mean((predictions - y_batch) ** 2)\n",
        "\n",
        "    loss_value, grads = nnx.value_and_grad(loss_fn)(model)\n",
        "    optimizer.update(grads)\n",
        "    return loss_value\n",
        "\n",
        "# Training with FSDP\n",
        "print(\"\n",
        "Training with FSDP...\")\n",
        "losses_fsdp = []\n",
        "start_time = time.time()\n",
        "\n",
        "# Use smaller batch size for FSDP demo\n",
        "fsdp_batch_size = 256\n",
        "steps_per_epoch = num_samples // fsdp_batch_size\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    # Shuffle data\n",
        "    key, subkey = jax.random.split(key)\n",
        "    perm = jax.random.permutation(subkey, num_samples)\n",
        "    x_shuffled = x_data[perm]\n",
        "    y_shuffled = y_data[perm]\n",
        "\n",
        "    epoch_losses = []\n",
        "\n",
        "    for step in range(steps_per_epoch):\n",
        "        # Get batch\n",
        "        idx_start = step * fsdp_batch_size\n",
        "        idx_end = idx_start + fsdp_batch_size\n",
        "        x_batch = x_shuffled[idx_start:idx_end]\n",
        "        y_batch = y_shuffled[idx_start:idx_end]\n",
        "\n",
        "        # Shard data across the data axis only\n",
        "        data_sharding = NamedSharding(fsdp_mesh, P('data', None))\n",
        "        x_batch = jax.device_put(x_batch, data_sharding)\n",
        "        y_batch = jax.device_put(y_batch, data_sharding)\n",
        "\n",
        "        # Perform training step\n",
        "        loss = fsdp_train_step(fsdp_model, fsdp_optimizer, x_batch, y_batch)\n",
        "        epoch_losses.append(float(loss))\n",
        "\n",
        "    avg_loss = sum(epoch_losses) / len(epoch_losses)\n",
        "    losses_fsdp.append(avg_loss)\n",
        "    print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {avg_loss:.6f}\")\n",
        "\n",
        "end_time = time.time()\n",
        "time_fsdp = end_time - start_time\n",
        "print(f\"\n",
        "FSDP training time: {time_fsdp:.2f} seconds\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xiUFL6yoh3RA"
      },
      "source": [
        "## 4. Performance Comparison\n",
        "\n",
        "Let's compare the performance of all training approaches."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eCCdIGLph3RA"
      },
      "outputs": [],
      "source": [
        "# Plot training curves\n",
        "plt.figure(figsize=(12, 5))\n",
        "\n",
        "# Plot loss curves\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(losses_single, label='Single Device', linewidth=2)\n",
        "plt.plot(losses_dp, label='JAX Data Parallel (8-way)', linewidth=2)\n",
        "plt.plot(losses_nnx_dp, label='Flax NNX Data Parallel (8-way)', linewidth=2)\n",
        "plt.plot(losses_fsdp, label='Flax NNX FSDP', linewidth=2)\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Training Loss Comparison')\n",
        "plt.legend()\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "# Plot speedup comparison\n",
        "plt.subplot(1, 2, 2)\n",
        "methods = ['Single\n",
        "Device', 'JAX\n",
        "DP', 'Flax NNX\n",
        "DP', 'FSDP']\n",
        "times = [time_single, time_dp, time_nnx_dp, time_fsdp]\n",
        "speedups = [time_single / t for t in times]\n",
        "colors = ['gray', 'blue', 'green', 'red']\n",
        "\n",
        "bars = plt.bar(methods, speedups, color=colors, alpha=0.7)\n",
        "plt.axhline(y=1, color='black', linestyle='--', alpha=0.5)\n",
        "plt.ylabel('Speedup (relative to single device)')\n",
        "plt.title('Training Speedup Comparison')\n",
        "plt.grid(True, alpha=0.3, axis='y')\n",
        "\n",
        "# Add speedup values on bars\n",
        "for bar, speedup in zip(bars, speedups):\n",
        "    height = bar.get_height()\n",
        "    plt.text(bar.get_x() + bar.get_width()/2., height + 0.1,\n",
        "             f'{speedup:.2f}x', ha='center', va='bottom')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Print summary\n",
        "print(\"\n",
        "=== Performance Summary ===\")\n",
        "print(f\"Single Device: {time_single:.2f}s\")\n",
        "print(f\"JAX Data Parallel (8-way): {time_dp:.2f}s (speedup: {time_single/time_dp:.2f}x)\")\n",
        "print(f\"Flax NNX Data Parallel (8-way): {time_nnx_dp:.2f}s (speedup: {time_single/time_nnx_dp:.2f}x)\")\n",
        "print(f\"FSDP: {time_fsdp:.2f}s (speedup: {time_single/time_fsdp:.2f}x)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "djBz3BQjh3RA"
      },
      "source": [
        "### Communication vs Computation Analysis\n",
        "\n",
        "The plot below shows how different parallelism strategies perform as batch size changes:\n",
        "\n",
        "![Communication vs FLOPs Analysis](https://github.com/jax-ml/scaling-book/blob/main/assets/img/mixed-fsdp-comms-2.png?raw=true)\n",
        "\n",
        "<sup> Image Source: [How To Scale Your Model](https://jax-ml.github.io/scaling-book) </sup>\n",
        "\n",
        "Key observations:\n",
        "- **Pure Data Parallelism/FSDP** (green): Ratio scales linearly with batch size. Best for large batches.\n",
        "- **Tensor Parallelism** (blue): Fixed ratio independent of batch size. Limited by feed-forward dimension.\n",
        "- **Mixed FSDP + Tensor Parallelism** (red): Ratio scales with √B. Optimal for intermediate batch sizes.\n",
        "\n",
        "**The horizontal line at ratio = 1** marks the boundary between compute-bound (above) and communication-bound (below) regimes.\n",
        "\n",
        "**Practical implications**:\n",
        "- Large batch sizes (>850 per chip): Use pure FSDP\n",
        "- Intermediate batch sizes (400-850 per chip): Use mixed FSDP + tensor parallelism\n",
        "- Very small batch sizes: May become communication-bound regardless\n",
        "\n",
        "This analysis helps choose the right parallelism strategy based on your model size, batch size, and available hardware.\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JKv7On3ah3RA"
      },
      "source": [
        "### Summary: Parallelism Strategies Comparison\n",
        "\n",
        "Here's a comprehensive comparison of the parallelism strategies covered:\n",
        "\n",
        "| **Strategy** | **Mathematical Formula** | **Communication** | **Memory** | **Best Use Case** |\n",
        "|--------------|---------------------------|-------------------|------------|-------------------|\n",
        "| **Data Parallelism** | In[B<sub>X</sub>, D] ⋅ W[D, F] → Out[B<sub>X</sub>, D] | AllReduce gradients (backward only) | Parameters replicated | Model fits on single device |\n",
        "| **FSDP** | In[B<sub>X</sub>, D] ⋅ W[D<sub>X</sub>, F] → Out[B<sub>X</sub>, D] | AllGather weights + ReduceScatter grads | Parameters sharded | Large models, memory constraints |\n",
        "| **Tensor Parallelism** | In[B, D<sub>Y</sub>] ⋅ W[D, F<sub>Y</sub>] → Out[B, D<sub>Y</sub>] | AllGather activations + ReduceScatter | Weights sharded by FF dim | Small batch sizes |\n",
        "| **Mixed FSDP + TP** | In[B<sub>X</sub>, D<sub>Y</sub>] ⋅ W[D<sub>X</sub>, F<sub>Y</sub>] → Out[B<sub>X</sub>, D<sub>Y</sub>] | Both weight and activation movement | Both sharding strategies | Very large models, small batches |\n",
        "\n",
        "**Communication Bound Conditions** (for TPUv5p):\n",
        "- **Data Parallelism & FSDP**: Batch size per chip > 2,550\n",
        "- **Tensor Parallelism**: F > Y × 2,550 (typically 8-16 way max)\n",
        "- **Mixed FSDP + TP**: Batch size per chip > 400 (optimal combination)\n",
        "\n",
        "**Key Insight**: The choice of parallelism strategy depends on the interplay between:\n",
        "1. **Model size** (does it fit on one device?)\n",
        "2. **Batch size** (how much data per device?)\n",
        "3. **Hardware constraints** (memory, bandwidth, number of devices)\n",
        "\n",
        "For this tutorial's examples, we demonstrated the progression from simple data parallelism to more sophisticated FSDP approaches, showing how each strategy addresses different scaling challenges.\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e9ww5ulWh3RA"
      },
      "source": [
        "## 5. Key Takeaways\n",
        "\n",
        "In this tutorial, we've explored different approaches to parallel training:\n",
        "\n",
        "1. **8-way Data Parallel with Plain JAX**\n",
        "   - Simple to implement using `jax.jit` and sharding specifications\n",
        "   - Model is replicated across all devices\n",
        "   - Data is sharded along the batch dimension\n",
        "   - Good speedup for compute-intensive workloads\n",
        "\n",
        "2. **Data Parallel with Flax NNX**\n",
        "   - Higher-level API makes implementation cleaner\n",
        "   - Same performance as plain JAX implementation\n",
        "   - Better for complex models with many components\n",
        "\n",
        "3. **Fully Sharded Data Parallel (FSDP) with Flax NNX**\n",
        "   - Shards both model parameters and data\n",
        "   - Uses a 2D mesh (data × model axes)\n",
        "   - Essential for large models that don't fit on single device\n",
        "   - More complex but enables training of massive models\n",
        "\n",
        "### When to Use Each Approach:\n",
        "\n",
        "- **Data Parallel**: When your model fits on a single device and you want to scale training speed\n",
        "- **FSDP**: When your model is too large for a single device or you need maximum memory efficiency\n",
        "\n",
        "### References:\n",
        "- Example implementations: `jax_data_parallel.py` and `flax_nnx_dp_fsdp.py` in the root directory\n",
        "- JAX documentation on [distributed arrays and automatic parallelization](https://jax.readthedocs.io/en/latest/jax-101/08-pjit.html)\n",
        "- Flax NNX documentation on [distributed training](https://flax.readthedocs.io/en/latest/nnx/index.html)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.0"
    },
    "colab": {
      "provenance": [],
      "gpuType": "V28"
    },
    "accelerator": "TPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}