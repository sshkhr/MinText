{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "o5p_3ph2u3sv"
   },
   "source": [
    "# Tutorial 1: JAX Parallelization Basics\n",
    "\n",
    "[![Open in GitHub](https://img.shields.io/badge/Open%20in-GitHub-181717?style=flat-square&logo=github)](https://github.com/sshkhr/MinText/blob/main/docs/tutorials/1_Parallelization_Basics.ipynb)\n",
    "[![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/sshkhr/MinText/blob/main/docs/tutorials/1_Parallelization_Basics.ipynb)\n",
    "\n",
    "This tutorial covers the fundamental concepts of parallelization in JAX, including device meshes, sharded arrays, and collective operations. We'll build understanding step by step, starting with basic concepts and working towards practical implementations.\n",
    "\n",
    "**Learning objectives:**\n",
    "- Understand JAX's device mesh and sharding concepts\n",
    "- Learn about collective operations (AllGather, ReduceScatter, etc.)\n",
    "- Implement basic parallel computations\n",
    "- Visualize how data is distributed across devices\n",
    "\n",
    "**Prerequisites:**\n",
    "- Basic familiarity with JAX and NumPy\n",
    "- Understanding of matrix operations\n",
    "- No prior knowledge of distributed computing required\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qloP9r0Tu3sw"
   },
   "source": [
    "## 0. Why JAX?\n",
    "\n",
    "[JAX](https://docs.jax.dev/) is a high-performance numerical computing library that combines NumPy's familiar API with the power of automatic differentiation and hardware acceleration on GPUs and TPUs. Developed by Google Research, JAX enables writing high-performance code that can run efficiently on a single device or scale across multiple devices.\n",
    "\n",
    "\n",
    "We use JAX for this notebook (and in the MinText library) because of several reasons:\n",
    "\n",
    "1. Beginner-friendly automatic parallelization using `jax.jit`\n",
    "2. Ability to simulate multiple devices using `\"XLA_FLAGS\"`\n",
    "3. Google Colab provides an 8 device runtime, v2-8 TPU, for free. This consists of 8x8 GB TPU cores which adds up to a total of 64 GB VRAM compute. So you can actually run distributed operations over 8 devices.\n",
    "4. There are already several great pedagogical style libraries in Pytorch (such as [HuggingFace Nanotron](https://github.com/huggingface/nanotron)) which serve a similar purpose. The key concepts from these tutorials can often be directly translated to PyTorch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RgGURMiBu3sw"
   },
   "source": [
    "## 1. Multi-Device Computation\n",
    "\n",
    "Choose the v2-8 TPU runtime in Google Colab to run this notebook. Once you restart the  and exploring the available devices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2i4n1zkju3sw"
   },
   "outputs": [],
   "source": [
    "# Install JAX if needed\n",
    "# !pip install --upgrade jax jaxlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "bJytD437u3sw"
   },
   "outputs": [],
   "source": [
    "# Uncomment this to simulate running the code on 8 CPU devices (use for local runs)\n",
    "# import os\n",
    "# os.environ[\"XLA_FLAGS\"] = '--xla_force_host_platform_device_count=8' # Use 8 CPU devices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JvYD0GEWu3sx"
   },
   "outputs": [],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from jax.sharding import Mesh, NamedSharding, PartitionSpec as P\n",
    "from jax.experimental import mesh_utils\n",
    "\n",
    "from functools import partial\n",
    "import numpy as np\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import Optional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Dk-ci7Ozu3sx",
    "outputId": "29cefd99-c530-4e46-a70f-acf876f536b4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JAX version: 0.5.2\n",
      "Available devices: [TpuDevice(id=0, process_index=0, coords=(0,0,0), core_on_chip=0), TpuDevice(id=1, process_index=0, coords=(0,0,0), core_on_chip=1), TpuDevice(id=2, process_index=0, coords=(1,0,0), core_on_chip=0), TpuDevice(id=3, process_index=0, coords=(1,0,0), core_on_chip=1), TpuDevice(id=4, process_index=0, coords=(0,1,0), core_on_chip=0), TpuDevice(id=5, process_index=0, coords=(0,1,0), core_on_chip=1), TpuDevice(id=6, process_index=0, coords=(1,1,0), core_on_chip=0), TpuDevice(id=7, process_index=0, coords=(1,1,0), core_on_chip=1)]\n",
      "Device count: 8\n"
     ]
    }
   ],
   "source": [
    "# Check available devices\n",
    "print(f\"JAX version: {jax.__version__}\")\n",
    "print(f\"Available devices: {jax.devices()}\")\n",
    "print(f\"Device count: {jax.device_count()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_HjN6kuixEnI"
   },
   "source": [
    "If you cannot access the v2-8 TPU from Google Colab (you timed out or are running this locally) restart this notebook and un-comment the `os.environ[\"XLA_FLAGS\"] = '--xla_force_host_platform_device_count=8'` flag to simulate multiple devices on your CPU.\n",
    "\n",
    "The output of the cell above should then change to look something like this:\n",
    "\n",
    "```bash\n",
    "Available devices: [CpuDevice(id=0), CpuDevice(id=1), CpuDevice(id=2), CpuDevice(id=3), CpuDevice(id=4), CpuDevice(id=5), CpuDevice(id=6), CpuDevice(id=7)]\n",
    "Device count: 8\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_NVkU2c4u3sx"
   },
   "source": [
    "### Creating a Device Mesh\n",
    "\n",
    "JAX uses the concept of a device mesh to organize available devices. A device can be a CPU (or a CPU core), GPU, or TPU for JAX's purpose. A mesh is a multi-dimensional array of devices that can be addressed along different axes. This allows us to partition our data and computation along different dimensions of the mesh."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fnLKxdOyu3sx",
    "outputId": "aa9f67ec-d0b3-4124-c09b-66b40b696daf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2D Mesh: Mesh('x': 2, 'y': 2)\n",
      "[0, 1, 2, 3]\n"
     ]
    }
   ],
   "source": [
    "devices = jax.devices()\n",
    "\n",
    "# If you have multiple devices, you can create a 2D mesh\n",
    "if len(devices) >= 4:\n",
    "    mesh_2d = jax.make_mesh((2, 2), ('x', 'y'))\n",
    "    print(f\"2D Mesh: {mesh_2d}\")\n",
    "    print([d.id for d in mesh_2d.devices.flat])\n",
    "else:\n",
    "    print(\"Not enough devices for a 2D mesh demonstration\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fL_6yWgyu3sx"
   },
   "source": [
    "## 2. Sharded Matrices\n",
    "\n",
    "Sharded matrices are arrays that are split across multiple devices. JAX provides abstractions to create and operate on these distributed arrays efficiently.\n",
    "\n",
    "### Basic Concepts\n",
    "\n",
    "- **Mesh**: A logical arrangement of devices\n",
    "- **PartitionSpec (P)**: Specifies how to partition an array across mesh dimensions\n",
    "- **jax.device_put()**: Places an array on a specific device or according to a sharding\n",
    "\n",
    "Let's see how to create sharded arrays using JAX's sharding API:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qfGkerAz1Enu"
   },
   "source": [
    "![Sharding](https://github.com/jax-ml/scaling-book/blob/main/assets/img/sharding-example.png?raw=true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hfdTmAIu1R2b"
   },
   "source": [
    "<sup> Image Source: [How To Scale Your Model](https://jax-ml.github.io/scaling-book) </sup>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "biXk36aWu3sx",
    "outputId": "fa955f08-f46e-466c-9ad4-96597d4e9bef"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matrix shape: (8192, 8192), Size in memory: 256.00 MB\n"
     ]
    }
   ],
   "source": [
    "# Let's create a large matrix and shard it\n",
    "matrix_size = 8192  # Adjust based on your device memory\n",
    "matrix = jnp.ones((matrix_size, matrix_size))\n",
    "print(f\"Matrix shape: {matrix.shape}, Size in memory: {matrix.size * 4 / (1024**2):.2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 271
    },
    "id": "9CJJEff5u3sx",
    "outputId": "20bd8a95-a80a-4d56-85e3-e5276bcebb1f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sharded matrix type: <class 'jaxlib.xla_extension.ArrayImpl'>\n",
      "Sharding spec: NamedSharding(mesh=Mesh('x': 2, 'y': 2), spec=PartitionSpec('x', 'y'), memory_kind=device)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #393b79\">            </span><span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #de9ed6\">            </span>\n",
       "<span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #393b79\">            </span><span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #de9ed6\">            </span>\n",
       "<span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #393b79\">   TPU 0    </span><span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #de9ed6\">   TPU 1    </span>\n",
       "<span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #393b79\">            </span><span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #de9ed6\">            </span>\n",
       "<span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #393b79\">            </span><span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #de9ed6\">            </span>\n",
       "<span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #393b79\">            </span><span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #de9ed6\">            </span>\n",
       "<span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #ad494a\">            </span><span style=\"color: #000000; text-decoration-color: #000000; background-color: #b5cf6b\">            </span>\n",
       "<span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #ad494a\">            </span><span style=\"color: #000000; text-decoration-color: #000000; background-color: #b5cf6b\">            </span>\n",
       "<span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #ad494a\">   TPU 2    </span><span style=\"color: #000000; text-decoration-color: #000000; background-color: #b5cf6b\">   TPU 3    </span>\n",
       "<span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #ad494a\">            </span><span style=\"color: #000000; text-decoration-color: #000000; background-color: #b5cf6b\">            </span>\n",
       "<span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #ad494a\">            </span><span style=\"color: #000000; text-decoration-color: #000000; background-color: #b5cf6b\">            </span>\n",
       "<span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #ad494a\">            </span><span style=\"color: #000000; text-decoration-color: #000000; background-color: #b5cf6b\">            </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[38;2;255;255;255;48;2;57;59;121m            \u001b[0m\u001b[38;2;255;255;255;48;2;222;158;214m            \u001b[0m\n",
       "\u001b[38;2;255;255;255;48;2;57;59;121m            \u001b[0m\u001b[38;2;255;255;255;48;2;222;158;214m            \u001b[0m\n",
       "\u001b[38;2;255;255;255;48;2;57;59;121m   \u001b[0m\u001b[38;2;255;255;255;48;2;57;59;121mTPU 0\u001b[0m\u001b[38;2;255;255;255;48;2;57;59;121m    \u001b[0m\u001b[38;2;255;255;255;48;2;222;158;214m   \u001b[0m\u001b[38;2;255;255;255;48;2;222;158;214mTPU 1\u001b[0m\u001b[38;2;255;255;255;48;2;222;158;214m    \u001b[0m\n",
       "\u001b[38;2;255;255;255;48;2;57;59;121m            \u001b[0m\u001b[38;2;255;255;255;48;2;222;158;214m            \u001b[0m\n",
       "\u001b[38;2;255;255;255;48;2;57;59;121m            \u001b[0m\u001b[38;2;255;255;255;48;2;222;158;214m            \u001b[0m\n",
       "\u001b[38;2;255;255;255;48;2;57;59;121m            \u001b[0m\u001b[38;2;255;255;255;48;2;222;158;214m            \u001b[0m\n",
       "\u001b[38;2;255;255;255;48;2;173;73;74m            \u001b[0m\u001b[38;2;0;0;0;48;2;181;207;107m            \u001b[0m\n",
       "\u001b[38;2;255;255;255;48;2;173;73;74m            \u001b[0m\u001b[38;2;0;0;0;48;2;181;207;107m            \u001b[0m\n",
       "\u001b[38;2;255;255;255;48;2;173;73;74m   \u001b[0m\u001b[38;2;255;255;255;48;2;173;73;74mTPU 2\u001b[0m\u001b[38;2;255;255;255;48;2;173;73;74m    \u001b[0m\u001b[38;2;0;0;0;48;2;181;207;107m   \u001b[0m\u001b[38;2;0;0;0;48;2;181;207;107mTPU 3\u001b[0m\u001b[38;2;0;0;0;48;2;181;207;107m    \u001b[0m\n",
       "\u001b[38;2;255;255;255;48;2;173;73;74m            \u001b[0m\u001b[38;2;0;0;0;48;2;181;207;107m            \u001b[0m\n",
       "\u001b[38;2;255;255;255;48;2;173;73;74m            \u001b[0m\u001b[38;2;0;0;0;48;2;181;207;107m            \u001b[0m\n",
       "\u001b[38;2;255;255;255;48;2;173;73;74m            \u001b[0m\u001b[38;2;0;0;0;48;2;181;207;107m            \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Define the partition spec - shard along the first dimension\n",
    "partition_spec = P('x', 'y')\n",
    "\n",
    "# Create a shardings object\n",
    "from jax.sharding import NamedSharding\n",
    "shardings = NamedSharding(mesh_2d, partition_spec)\n",
    "\n",
    "# Create the sharded array\n",
    "sharded_matrix = jax.device_put(x=matrix, device=shardings)\n",
    "\n",
    "print(f\"Sharded matrix type: {type(sharded_matrix)}\")\n",
    "print(f\"Sharding spec: {shardings}\")\n",
    "\n",
    "jax.debug.visualize_array_sharding(sharded_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Aa9Xp22Pu3sx"
   },
   "source": [
    "### Inspecting Sharded Matrices\n",
    "\n",
    "We can inspect how the array is distributed across devices:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BPvuoHw96fHZ",
    "outputId": "2f0df8c8-e612-44dc-b063-a419082ab240"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Global matrix shape (8192, 8192)\n",
      "Shapes of Matrix Shards:\n",
      "TPU_0(process=0,(0,0,0,0)) (slice(0, 4096, None), slice(0, 4096, None)) (4096, 4096)\n",
      "TPU_1(process=0,(0,0,0,1)) (slice(0, 4096, None), slice(4096, 8192, None)) (4096, 4096)\n",
      "TPU_2(process=0,(1,0,0,0)) (slice(4096, 8192, None), slice(0, 4096, None)) (4096, 4096)\n",
      "TPU_3(process=0,(1,0,0,1)) (slice(4096, 8192, None), slice(4096, 8192, None)) (4096, 4096)\n"
     ]
    }
   ],
   "source": [
    "print(\"Global matrix shape\", sharded_matrix.shape)\n",
    "\n",
    "# Get the local arrays on each device\n",
    "print(\"Shapes of Matrix Shards:\")\n",
    "for shard in sharded_matrix.addressable_shards:\n",
    "  print(shard.device, shard.index, shard.data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ra5f9aOe9936"
   },
   "source": [
    "![Array Sharding Visualization](https://jax-ml.github.io/scaling-book/assets/img/sharding-colored4.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KUtX2m3S9937"
   },
   "source": [
    "<sup> Image Source: [How To Scale Your Model](https://jax-ml.github.io/scaling-book) </sup>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "v1aOZjpYu3sx"
   },
   "source": [
    "### Performance Benefits\n",
    "\n",
    "Let's compare the performance of element-wise operations on sharded vs. non-sharded matrices (we will matrix-level operations in a bit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "c3y0rJzMu3sx",
    "outputId": "d855e50f-b968-47e2-9e5b-6976eff817d4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.88 ms ± 36.4 µs per loop (mean ± std. dev. of 5 runs, 5 loops each)\n"
     ]
    }
   ],
   "source": [
    "# `matrix` is present on a single device\n",
    "%timeit -n 5 -r 5 jnp.sin(matrix).block_until_ready()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hZpzir6m_D6u",
    "outputId": "8cb3b340-04a8-419b-a57c-956fff6d0989"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.64 ms ± 55.9 µs per loop (mean ± std. dev. of 5 runs, 5 loops each)\n"
     ]
    }
   ],
   "source": [
    "# `sharded_matrix` is distributed across 4 devices\n",
    "%timeit -n 5 -r 5 jnp.sin(sharded_matrix).block_until_ready()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9PFgKkSJu3sy"
   },
   "source": [
    "### Different Sharding Strategies\n",
    "\n",
    "Each axis of the matrix can be sharded across each possible axis in the device mesh. This gives rise to a combinatorial number of possible shardings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "3UCpiympBJB6"
   },
   "outputs": [],
   "source": [
    "# A small helper function to define a sharding mesh\n",
    "default_mesh = jax.make_mesh((2, 2), ('a', 'b'))\n",
    "\n",
    "def mesh_sharding(\n",
    "    pspec: P, mesh: Optional[Mesh] = None,\n",
    "  ) -> NamedSharding:\n",
    "  if mesh is None:\n",
    "    mesh = default_mesh\n",
    "  return NamedSharding(mesh, pspec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 215
    },
    "id": "dc-8P5xVu3sy",
    "outputId": "cd549c0f-4eb5-4819-b7d3-95fcabf93a68"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #393b79\">            </span><span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #de9ed6\">            </span>\n",
       "<span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #393b79\">            </span><span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #de9ed6\">            </span>\n",
       "<span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #393b79\">   TPU 0    </span><span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #de9ed6\">   TPU 1    </span>\n",
       "<span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #393b79\">            </span><span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #de9ed6\">            </span>\n",
       "<span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #393b79\">            </span><span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #de9ed6\">            </span>\n",
       "<span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #393b79\">            </span><span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #de9ed6\">            </span>\n",
       "<span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #ad494a\">            </span><span style=\"color: #000000; text-decoration-color: #000000; background-color: #b5cf6b\">            </span>\n",
       "<span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #ad494a\">            </span><span style=\"color: #000000; text-decoration-color: #000000; background-color: #b5cf6b\">            </span>\n",
       "<span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #ad494a\">   TPU 2    </span><span style=\"color: #000000; text-decoration-color: #000000; background-color: #b5cf6b\">   TPU 3    </span>\n",
       "<span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #ad494a\">            </span><span style=\"color: #000000; text-decoration-color: #000000; background-color: #b5cf6b\">            </span>\n",
       "<span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #ad494a\">            </span><span style=\"color: #000000; text-decoration-color: #000000; background-color: #b5cf6b\">            </span>\n",
       "<span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #ad494a\">            </span><span style=\"color: #000000; text-decoration-color: #000000; background-color: #b5cf6b\">            </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[38;2;255;255;255;48;2;57;59;121m            \u001b[0m\u001b[38;2;255;255;255;48;2;222;158;214m            \u001b[0m\n",
       "\u001b[38;2;255;255;255;48;2;57;59;121m            \u001b[0m\u001b[38;2;255;255;255;48;2;222;158;214m            \u001b[0m\n",
       "\u001b[38;2;255;255;255;48;2;57;59;121m   \u001b[0m\u001b[38;2;255;255;255;48;2;57;59;121mTPU 0\u001b[0m\u001b[38;2;255;255;255;48;2;57;59;121m    \u001b[0m\u001b[38;2;255;255;255;48;2;222;158;214m   \u001b[0m\u001b[38;2;255;255;255;48;2;222;158;214mTPU 1\u001b[0m\u001b[38;2;255;255;255;48;2;222;158;214m    \u001b[0m\n",
       "\u001b[38;2;255;255;255;48;2;57;59;121m            \u001b[0m\u001b[38;2;255;255;255;48;2;222;158;214m            \u001b[0m\n",
       "\u001b[38;2;255;255;255;48;2;57;59;121m            \u001b[0m\u001b[38;2;255;255;255;48;2;222;158;214m            \u001b[0m\n",
       "\u001b[38;2;255;255;255;48;2;57;59;121m            \u001b[0m\u001b[38;2;255;255;255;48;2;222;158;214m            \u001b[0m\n",
       "\u001b[38;2;255;255;255;48;2;173;73;74m            \u001b[0m\u001b[38;2;0;0;0;48;2;181;207;107m            \u001b[0m\n",
       "\u001b[38;2;255;255;255;48;2;173;73;74m            \u001b[0m\u001b[38;2;0;0;0;48;2;181;207;107m            \u001b[0m\n",
       "\u001b[38;2;255;255;255;48;2;173;73;74m   \u001b[0m\u001b[38;2;255;255;255;48;2;173;73;74mTPU 2\u001b[0m\u001b[38;2;255;255;255;48;2;173;73;74m    \u001b[0m\u001b[38;2;0;0;0;48;2;181;207;107m   \u001b[0m\u001b[38;2;0;0;0;48;2;181;207;107mTPU 3\u001b[0m\u001b[38;2;0;0;0;48;2;181;207;107m    \u001b[0m\n",
       "\u001b[38;2;255;255;255;48;2;173;73;74m            \u001b[0m\u001b[38;2;0;0;0;48;2;181;207;107m            \u001b[0m\n",
       "\u001b[38;2;255;255;255;48;2;173;73;74m            \u001b[0m\u001b[38;2;0;0;0;48;2;181;207;107m            \u001b[0m\n",
       "\u001b[38;2;255;255;255;48;2;173;73;74m            \u001b[0m\u001b[38;2;0;0;0;48;2;181;207;107m            \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Shard first axis of matrix along mesh axis 'a', second axis of matrix along mesh axis 'b'\n",
    "ix_jy_sharding = jax.device_put(matrix, mesh_sharding(P('a', 'b')))\n",
    "jax.debug.visualize_array_sharding(ix_jy_sharding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 215
    },
    "id": "Pn0Ihnt4BdgE",
    "outputId": "075b05a2-d025-4b65-8629-cb313a2c1625"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #393b79\">            </span><span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #de9ed6\">            </span>\n",
       "<span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #393b79\">            </span><span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #de9ed6\">            </span>\n",
       "<span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #393b79\">   TPU 0    </span><span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #de9ed6\">   TPU 2    </span>\n",
       "<span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #393b79\">            </span><span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #de9ed6\">            </span>\n",
       "<span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #393b79\">            </span><span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #de9ed6\">            </span>\n",
       "<span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #393b79\">            </span><span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #de9ed6\">            </span>\n",
       "<span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #ad494a\">            </span><span style=\"color: #000000; text-decoration-color: #000000; background-color: #b5cf6b\">            </span>\n",
       "<span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #ad494a\">            </span><span style=\"color: #000000; text-decoration-color: #000000; background-color: #b5cf6b\">            </span>\n",
       "<span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #ad494a\">   TPU 1    </span><span style=\"color: #000000; text-decoration-color: #000000; background-color: #b5cf6b\">   TPU 3    </span>\n",
       "<span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #ad494a\">            </span><span style=\"color: #000000; text-decoration-color: #000000; background-color: #b5cf6b\">            </span>\n",
       "<span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #ad494a\">            </span><span style=\"color: #000000; text-decoration-color: #000000; background-color: #b5cf6b\">            </span>\n",
       "<span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #ad494a\">            </span><span style=\"color: #000000; text-decoration-color: #000000; background-color: #b5cf6b\">            </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[38;2;255;255;255;48;2;57;59;121m            \u001b[0m\u001b[38;2;255;255;255;48;2;222;158;214m            \u001b[0m\n",
       "\u001b[38;2;255;255;255;48;2;57;59;121m            \u001b[0m\u001b[38;2;255;255;255;48;2;222;158;214m            \u001b[0m\n",
       "\u001b[38;2;255;255;255;48;2;57;59;121m   \u001b[0m\u001b[38;2;255;255;255;48;2;57;59;121mTPU 0\u001b[0m\u001b[38;2;255;255;255;48;2;57;59;121m    \u001b[0m\u001b[38;2;255;255;255;48;2;222;158;214m   \u001b[0m\u001b[38;2;255;255;255;48;2;222;158;214mTPU 2\u001b[0m\u001b[38;2;255;255;255;48;2;222;158;214m    \u001b[0m\n",
       "\u001b[38;2;255;255;255;48;2;57;59;121m            \u001b[0m\u001b[38;2;255;255;255;48;2;222;158;214m            \u001b[0m\n",
       "\u001b[38;2;255;255;255;48;2;57;59;121m            \u001b[0m\u001b[38;2;255;255;255;48;2;222;158;214m            \u001b[0m\n",
       "\u001b[38;2;255;255;255;48;2;57;59;121m            \u001b[0m\u001b[38;2;255;255;255;48;2;222;158;214m            \u001b[0m\n",
       "\u001b[38;2;255;255;255;48;2;173;73;74m            \u001b[0m\u001b[38;2;0;0;0;48;2;181;207;107m            \u001b[0m\n",
       "\u001b[38;2;255;255;255;48;2;173;73;74m            \u001b[0m\u001b[38;2;0;0;0;48;2;181;207;107m            \u001b[0m\n",
       "\u001b[38;2;255;255;255;48;2;173;73;74m   \u001b[0m\u001b[38;2;255;255;255;48;2;173;73;74mTPU 1\u001b[0m\u001b[38;2;255;255;255;48;2;173;73;74m    \u001b[0m\u001b[38;2;0;0;0;48;2;181;207;107m   \u001b[0m\u001b[38;2;0;0;0;48;2;181;207;107mTPU 3\u001b[0m\u001b[38;2;0;0;0;48;2;181;207;107m    \u001b[0m\n",
       "\u001b[38;2;255;255;255;48;2;173;73;74m            \u001b[0m\u001b[38;2;0;0;0;48;2;181;207;107m            \u001b[0m\n",
       "\u001b[38;2;255;255;255;48;2;173;73;74m            \u001b[0m\u001b[38;2;0;0;0;48;2;181;207;107m            \u001b[0m\n",
       "\u001b[38;2;255;255;255;48;2;173;73;74m            \u001b[0m\u001b[38;2;0;0;0;48;2;181;207;107m            \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Shard first axis of matrix along mesh axis 'b', second axis of matrix along mesh axis 'a'\n",
    "iy_jx_sharding = jax.device_put(matrix, mesh_sharding(P('b', 'a')))\n",
    "jax.debug.visualize_array_sharding(iy_jx_sharding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 199
    },
    "id": "bk-iDwLkB1HB",
    "outputId": "680db174-d635-45bd-8319-a94d0a8406e8"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┌───────────────────────┐\n",
       "│                       │\n",
       "│        TPU <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>        │\n",
       "│                       │\n",
       "│                       │\n",
       "├───────────────────────┤\n",
       "│                       │\n",
       "│        TPU <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span>        │\n",
       "│                       │\n",
       "│                       │\n",
       "└───────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┌───────────────────────┐\n",
       "│                       │\n",
       "│        TPU \u001b[1;36m0\u001b[0m,\u001b[1;36m1\u001b[0m        │\n",
       "│                       │\n",
       "│                       │\n",
       "├───────────────────────┤\n",
       "│                       │\n",
       "│        TPU \u001b[1;36m2\u001b[0m,\u001b[1;36m3\u001b[0m        │\n",
       "│                       │\n",
       "│                       │\n",
       "└───────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Shard first axis of matrix along mesh axis 'a', replicate second axis of matrix along each shard\n",
    "ix_j_sharding = jax.device_put(matrix, mesh_sharding(P('a', None)))\n",
    "jax.debug.visualize_array_sharding(ix_j_sharding, use_color=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 199
    },
    "id": "qglM7sfHCFd6",
    "outputId": "f78716ea-7de7-4e8a-8cfa-2e8927b52be6"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┌──────────┬──────────┐\n",
       "│          │          │\n",
       "│          │          │\n",
       "│          │          │\n",
       "│          │          │\n",
       "│ TPU <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span>  │ TPU <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span>  │\n",
       "│          │          │\n",
       "│          │          │\n",
       "│          │          │\n",
       "│          │          │\n",
       "└──────────┴──────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┌──────────┬──────────┐\n",
       "│          │          │\n",
       "│          │          │\n",
       "│          │          │\n",
       "│          │          │\n",
       "│ TPU \u001b[1;36m0\u001b[0m,\u001b[1;36m2\u001b[0m  │ TPU \u001b[1;36m1\u001b[0m,\u001b[1;36m3\u001b[0m  │\n",
       "│          │          │\n",
       "│          │          │\n",
       "│          │          │\n",
       "│          │          │\n",
       "└──────────┴──────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Partition second axis of x over second mesh axis 'b', replicate first axis of matrix along each shard\n",
    "i_jy_shard = jax.device_put(matrix, mesh_sharding(P(None, 'b')))\n",
    "jax.debug.visualize_array_sharding(i_jy_shard, use_color=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EK_qrIQqIODG"
   },
   "source": [
    "For a 2D matrix being sharded along a 2D device mesh, here are all the possible sharding strategies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Sr3T40pqCdwX"
   },
   "source": [
    "![Possible Array Shardings](https://jax-ml.github.io/scaling-book/assets/img/sharding-colored5.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rJy0WY50CdwY"
   },
   "source": [
    "<sup> Image Source: [How To Scale Your Model](https://jax-ml.github.io/scaling-book) </sup>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UKtWvBtju3sy"
   },
   "source": [
    "### Choosing the Right Sharding Strategy\n",
    "\n",
    "As we discuss collectives (next section) and different parallelism strategies in (next tutorials), we will slowly do a deeper dive into how to chose sharding strategies based on your use case."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wvNt9kOLu3sy"
   },
   "source": [
    "## 3. Collective Operations\n",
    "\n",
    "Collective operations are essential for distributed algorithms where devices need to share or aggregate information. In deep learning, these are utilized in performing computations with sharded arrays."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eYxP1wOzEGu1"
   },
   "source": [
    "### Matrix Operations With Sharded Arrays\n",
    "\n",
    "If we want to perform matrix operations on sharded arrays, we need to think through some overheads involved in moving data between devices. \n",
    "\n",
    "In deep learning, these two operations are often used:\n",
    "\n",
    "- **Element-wise operations (e.g. ReLU)**: Operations that can be performed independently on each element of the array. These operations can be performed in parallel across aray shards without needing to communicate between them.\n",
    "- **Matrix-multiplication (e.g. Linear Layer, Attention etc)**: A more complex operation that requires communication between devices to compute the result. This is where collective operations come into play."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q4FlzP66EJLF"
   },
   "source": [
    "#### Block Matrix Multiplication\n",
    "\n",
    "We can think of a matrix as being composed of smaller blocks, which can be processed independently. \n",
    "\n",
    "\\begin{equation} \\begin{pmatrix} a_{00} & a_{01} & a_{02} & a_{03} \\\\ a_{10} & a_{11} & a_{12} & a_{13} \\\\ a_{20} & a_{21} & a_{22} & a_{23} \\\\ a_{30} & a_{31} & a_{32} & a_{33} \\end{pmatrix} = \\left( \\begin{matrix} \\begin{bmatrix} a_{00} & a_{01} \\\\ a_{10} & a_{11} \\end{bmatrix} \\\\ \\begin{bmatrix} a_{20} & a_{21} \\\\ a_{30} & a_{31} \\end{bmatrix} \\end{matrix} \\begin{matrix} \\begin{bmatrix} a_{02} & a_{03} \\\\ a_{12} & a_{13} \\end{bmatrix} \\\\ \\begin{bmatrix} a_{22} & a_{23} \\\\ a_{32} & a_{33} \\end{bmatrix} \\end{matrix} \\right) = \\begin{pmatrix} \\mathbf{A_{00}} & \\mathbf{A_{01}} \\\\ \\mathbf{A_{10}} & \\mathbf{A_{11}} \\end{pmatrix} \\end{equation}\n",
    "\n",
    "Matrix multiplication carries this nice property that the product of two matrices can be written in terms of block matmuls.\n",
    "\n",
    "\\begin{equation} \\begin{pmatrix} A_{00} & A_{01} \\\\ A_{10} & A_{11} \\end{pmatrix} \\cdot \\begin{pmatrix} B_{00} & B_{01} \\\\ B_{10} & B_{11} \\end{pmatrix} = \\begin{pmatrix} A_{00}B_{00} + A_{01}B_{10} & A_{00}B_{01} + A_{01}B_{11} \\\\ A_{10}B_{00} + A_{11}B_{10} & A_{10}B_{01} + A_{11}B_{11} \\end{pmatrix} \\end{equation}\n",
    "\n",
    "So we can compute distributed matrix multiplication by computing the block matmuls in parallel. The question is what communication is required to compute the final result, when to do it, and how expensive it is to perform."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "42gwCTSPERfY"
   },
   "source": [
    "#### Case 1: No Sharded Contracting Dimension"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider the matrix multiplication of two sharded matrices $\\mathbf{A}[I_X, J]$ and $\\mathbf{B}[J, K_Y]$. Note that the contracting dimension $J$ is not sharded. Thus we have:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ \\mathbf{A}[I_X, J] \\cdot \\mathbf{B}[J, K_Y] \\rightarrow \\mathbf{C}[I_X, K_Y] $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XwrnJc3OEMGS"
   },
   "source": [
    "We can multiply each local shard without any communication between devices. Each device computes its local shard of the result matrix $\\mathbf{C}$ independently. Each of the following possible sharded matrix multiplications can be performed without any communication:\n",
    "\n",
    "\\begin{align*} \\mathbf{A}[I, J] \\cdot \\mathbf{B}[J, K] \\rightarrow &\\ \\mathbf{C}[I, K] \\\\ \\mathbf{A}[I_X, J] \\cdot \\mathbf{B}[J, K] \\rightarrow &\\ \\mathbf{C}[I_X, K]\\\\ \\mathbf{A}[I, J] \\cdot \\mathbf{B}[J, K_Y] \\rightarrow &\\ \\mathbf{C}[I, K_Y]\\\\ \\mathbf{A}[I_X, J] \\cdot \\mathbf{B}[J, K_Y] \\rightarrow &\\ \\mathbf{C}[I_X, K_Y] \\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tbfYuMEau3sy"
   },
   "source": [
    "#### Case 2 (All-Gather): One matrix has a sharded contracting dimension\n",
    "\n",
    "Consider the case of a distributed matrix multiplication where one of the matrices has a sharded contracting dimension. For example, \n",
    "\n",
    "$$ \\mathbf{A}[I, J_X] \\cdot \\mathbf{B}[J, K] \\rightarrow \\mathbf{C}[I, K] $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we cannot directly multiply the local shards of $\\mathbf{A}$ and $\\mathbf{B}$ without communication. Each device needs to gather the shards of $\\mathbf{B}$ across all devices to compute its local shard of $\\mathbf{C}$. This is done using an all-gather operation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![All Gather](../_static/all-gather.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<sup> Image Source: [JAX documentation](https://docs.jax.dev/en/latest/notebooks/shard_map.html#all-gather) </sup>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@partial(jax.shard_map, mesh=mesh1d, in_specs=P('i'), out_specs=P('i'))\n",
    "def f4(x_block):\n",
    "  print('BEFORE:\n",
    "', x_block)\n",
    "  y_block = jax.lax.all_gather(x_block, 'i', tiled=True)\n",
    "  print('AFTER:\n",
    "', y_block)\n",
    "  return y_block\n",
    "\n",
    "x = jnp.array([3, 9, 5, 2])\n",
    "y = f4(x)\n",
    "print('FINAL RESULT:\n",
    "', y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To perform a matrix multiplication using the AllGather operation, we can follow these steps:\n",
    "1. **AllGather** the first matrix across all devices.\n",
    "$$\\textbf{AllGather}_X[I, J_X] \\rightarrow \\mathbf{A}[I, J]$$\n",
    "2. **Multiply** the gathered matrix with the second matrix.\n",
    "$$\\mathbf{A}[I, J] \\cdot \\mathbf{B}[J, K] \\rightarrow \\mathbf{C}[I, K]$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aEmIPGpRu3sy"
   },
   "outputs": [],
   "source": [
    "from jax.experimental import pjit\n",
    "\n",
    "# Create device-specific values\n",
    "def create_device_values(mesh):\n",
    "    # Create different values for each device\n",
    "    device_values = []\n",
    "    for i, device in enumerate(mesh.devices.flat):\n",
    "        value = jnp.ones((10, 10)) * (i + 1)  # Each device gets a different value\n",
    "        device_values.append(jax.device_put(value, device))\n",
    "    return device_values\n",
    "\n",
    "with mesh_1d:\n",
    "    # Example of all-reduce using pjit\n",
    "    @jax.jit\n",
    "    def all_reduce_sum(x):\n",
    "        # Explicit all-reduce\n",
    "        return jax.lax.psum(x, axis_name='devices')\n",
    "\n",
    "    # Create a sharded array where each device has a different value\n",
    "    n_devices = jax.device_count()\n",
    "    sharded_values = jnp.arange(1, n_devices + 1).reshape((n_devices, 1))\n",
    "    shardings = NamedSharding(mesh_1d, P('devices', None))\n",
    "    sharded_array = jax.device_put(sharded_values, shardings)\n",
    "\n",
    "    # Perform the all-reduce\n",
    "    result = all_reduce_sum(sharded_array)\n",
    "    print(f\"Original values:\n",
    "{sharded_values}\")\n",
    "    print(f\"After all-reduce sum:\n",
    "{result}\")\n",
    "\n",
    "    # Expected result: each device should have the sum of all values\n",
    "    expected_sum = jnp.sum(jnp.arange(1, n_devices + 1))\n",
    "    print(f\"Expected sum: {expected_sum}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**How is an all-gather performed?**\n",
    "\n",
    "![All Gather Operation](https://jax-ml.github.io/scaling-book/assets/img/all-gather.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<sup> Image Source: [How To Scale Your Model](https://jax-ml.github.io/scaling-book) </sup>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "57IU22SOu3sy"
   },
   "source": [
    "#### Case 3 (All-Reduce): Both Matrices have sharded contracting dimensions\n",
    "\n",
    "Consider the case where both matrices to be multiplied are sharded on their contracting dimensions, along the same mesh axes.\n",
    "\n",
    "$$\\textbf{A}[I, J_X] \\cdot \\textbf{B}[J_X, K] \\rightarrow C[I, K]$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case, we can multiply the local shards of the matrices, however each shard will only contain a partial result. \n",
    "\n",
    "$$\\textbf{A}[I, J_X] \\cdot_\\text{LOCAL} \\textbf{B}[J_X, K] \\rightarrow C[I, K] \\{\\ U_X \\}$$\n",
    "\n",
    "The notation $\\{\\ U_X \\}$ here refers to the fact that the matrix $C$ is unreduced along the mesh axis $X$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To obtain the final result, we need to perform an All-Reduce operation on the shards of $C$. Since the all-reduce sum operation is very common, jax provides the `jax.lax.psum` function to perform this operation efficiently.\n",
    "\n",
    "![All-Reduce](../_static/png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "q3AHaoIlu3sy"
   },
   "outputs": [],
   "source": [
    "with mesh_1d:\n",
    "    # Example of all-gather\n",
    "    @jax.jit\n",
    "    def all_gather(x):\n",
    "        # Explicit all-gather\n",
    "        return jax.lax.all_gather(x, axis_name='devices', axis=0)\n",
    "\n",
    "    # Use the same sharded array from before\n",
    "    result = all_gather(sharded_array)\n",
    "    print(f\"Original values:\n",
    "{sharded_values}\")\n",
    "    print(f\"After all-gather:\n",
    "{result}\")\n",
    "\n",
    "    # Expected result: each device should have all values\n",
    "    expected_gather = jnp.arange(1, n_devices + 1).reshape((n_devices, 1))\n",
    "    print(f\"Expected result:\n",
    "{expected_gather}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to perform the matrix multiplication $C = A \\cdot B$ using the AllReduce operation, we can break down the process into two main steps. \n",
    "\n",
    "1. **Local Matrix Multiplication** of input matrix shards on each device.\n",
    "$$A[I, J_X] \\cdot_\\text{LOCAL} B[J_X, K] \\rightarrow C[I, K] \\{ U_X \\}$$\n",
    "\n",
    "2. **AllReduce** the partial results across all devices.\n",
    "$$\\textbf{AllReduce}_X C[I, K] \\{ U_X \\} \\rightarrow C[I, K]$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Thinking of All-Reduce as ReduceScatter + AllGather"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Reduce Scatter in JAX](../_static/psum_scatter.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Case 4 (All-Gather): Both Matrices have non-contracting dimensions sharded along the same mesh axes\n",
    "\n",
    "**Whenever we shard a tensor, each mesh dimension can appear AT MOST ONCE.** Consider the case where both matrices to be multiplied are sharded on their non-contracting dimensions, along the same mesh axes.\n",
    "\n",
    "$$\\textbf{A}[I_X, J] \\cdot \\textbf{B}[J, K_X] \\rightarrow C[I_X, K_X]$$\n",
    "\n",
    "Such a sharding is **not allowed**, as there is not enough information along each shards to reconstruct the full matrix. In this case, we need to change the sharding of at least one of the matrices before multiplication.\n",
    "\n",
    "We have two options:\n",
    "\n",
    "1. **All-Gather the sharding dimension of matrix A** to have the non-contracting dimension unsharded.\n",
    "$$\\begin{align*} \\textbf{AllGather}_X A[I_X, J] \\rightarrow &\\ A[I, J] \\\\ A[I, J] \\cdot B[J, K_X] \\rightarrow &\\ C[I, K_X] \\end{align*}$$\n",
    "2. **All-Gather the sharding dimension of matrix B** to have the non-contracting dimension unsharded.\n",
    "$$\\begin{align*} \\textbf{AllGather}_X B[J, K_X] \\rightarrow &\\ B[J, K] \\\\ A[I_X, J] \\cdot B[J, K] \\rightarrow &\\ C[I_X, K] \\end{align*}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DF4DVnDiFH00"
   },
   "source": [
    "### Other Collectives"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aQGQ9-52FNl_"
   },
   "source": [
    "### Reduce-Scatter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SL5QXIWlu3sy"
   },
   "source": [
    "#### All-to-All\n",
    "\n",
    "All-to-all exchanges slices of data between all devices. This is useful for operations like matrix transposition or redistributing data with a different sharding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wLFnWWJ2u3sy"
   },
   "outputs": [],
   "source": [
    "if jax.device_count() >= 2:\n",
    "    with mesh_1d:\n",
    "        # Example of all-to-all\n",
    "        @jax.jit\n",
    "        def all_to_all(x):\n",
    "            # Explicit all-to-all - reshape to have a split dimension\n",
    "            return jax.lax.all_to_all(x, axis_name='devices', split_axis=0, concat_axis=1)\n",
    "\n",
    "        # Create a matrix where each device has part of the rows\n",
    "        n_devices = jax.device_count()\n",
    "        data = jnp.arange(n_devices * n_devices).reshape((n_devices, n_devices))\n",
    "        shardings = NamedSharding(mesh_1d, P('devices', None))\n",
    "        sharded_data = jax.device_put(data, shardings)\n",
    "\n",
    "        # Perform the all-to-all\n",
    "        result = all_to_all(sharded_data)\n",
    "        print(f\"Original data:\n",
    "{data}\")\n",
    "        print(f\"After all-to-all:\n",
    "{result}\")\n",
    "\n",
    "        # This effectively transposes the sharding from rows to columns\n",
    "        print(f\"Original sharding: {sharded_data.sharding}\")\n",
    "        print(f\"Result sharding: {result.sharding}\")\n",
    "else:\n",
    "    print(\"Need at least 2 devices for all-to-all demonstration\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0M2Sxz09u3sy"
   },
   "source": [
    "### Performance Considerations for Collective Operations\n",
    "\n",
    "When working with collective operations, consider:\n",
    "\n",
    "1. **Communication overhead**: Collective operations require device-to-device communication\n",
    "2. **Data size**: Larger transfers take more time\n",
    "3. **Network topology**: The physical connections between devices matter\n",
    "4. **Frequency**: Minimize the number of collective operations in your code\n",
    "\n",
    "Let's benchmark a simple all-reduce operation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jeZqfrxLu3sy"
   },
   "outputs": [],
   "source": [
    "if jax.device_count() > 1:\n",
    "    # Benchmark all-reduce with different data sizes\n",
    "    sizes = [10, 100, 1000, 10000]\n",
    "    times = []\n",
    "\n",
    "    with mesh_1d:\n",
    "        for size in sizes:\n",
    "            # Create data\n",
    "            data = jnp.ones((size, size))\n",
    "            shardings = NamedSharding(mesh_1d, P('devices', None))\n",
    "            sharded_data = jax.device_put(data, shardings)\n",
    "\n",
    "            # Define and compile the all-reduce\n",
    "            @jax.jit\n",
    "            def all_reduce(x):\n",
    "                return jax.lax.psum(x, axis_name='devices')\n",
    "\n",
    "            # Warm-up\n",
    "            all_reduce(sharded_data).block_until_ready()\n",
    "\n",
    "            # Benchmark\n",
    "            start = time.time()\n",
    "            all_reduce(sharded_data).block_until_ready()\n",
    "            elapsed = time.time() - start\n",
    "            times.append(elapsed)\n",
    "\n",
    "            print(f\"Size {size}x{size}, Time: {elapsed:.6f} seconds\")\n",
    "\n",
    "    # Plot the results\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(sizes, times, 'o-')\n",
    "    plt.xscale('log')\n",
    "    plt.xlabel('Matrix Size')\n",
    "    plt.ylabel('Time (seconds)')\n",
    "    plt.title('All-Reduce Performance by Data Size')\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"Multiple devices required for benchmark\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "739BfzUPu3sy"
   },
   "source": [
    "## 6. Advanced Topics\n",
    "\n",
    "### Automatic Sharding with SPMD (Single Program Multiple Data)\n",
    "\n",
    "JAX provides the `pjit` (Partitioned JIT) API for automatic sharding. With `pjit`, you specify the sharding of inputs and outputs, and JAX determines the optimal intermediate shardings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "76Jy_ICau3sy"
   },
   "outputs": [],
   "source": [
    "from jax.experimental import pjit\n",
    "\n",
    "def auto_sharded_matmul(a, b, mesh):\n",
    "    \"\"\"Matrix multiplication with automatic sharding.\"\"\"\n",
    "    with mesh:\n",
    "        # Define input and output specs\n",
    "        in_specs = (P('devices', None), P(None, 'devices'))  # Row and column sharding\n",
    "        out_spec = P(None, None)  # Output on all devices\n",
    "\n",
    "        # Define the pjit function\n",
    "        pjit_matmul = pjit.pjit(\n",
    "            lambda x, y: jnp.matmul(x, y),\n",
    "            in_shardings=in_specs,\n",
    "            out_shardings=out_spec\n",
    "        )\n",
    "\n",
    "        return pjit_matmul(a, b)\n",
    "\n",
    "if jax.device_count() > 1:\n",
    "    # Try the auto-sharded version\n",
    "    a, b = create_matrices(2000)\n",
    "\n",
    "    start = time.time()\n",
    "    c_auto = auto_sharded_matmul(a, b, mesh_1d).block_until_ready()\n",
    "    auto_time = time.time() - start\n",
    "    print(f\"Auto-sharded time: {auto_time:.4f} seconds\")\n",
    "\n",
    "    # Compare with our manual implementation\n",
    "    if 'distributed_time' in locals():\n",
    "        print(f\"Manual vs Auto ratio: {distributed_time / auto_time:.2f}x\")\n",
    "else:\n",
    "    print(\"Multiple devices required for demonstration\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zVjjyO9xu3sy"
   },
   "source": [
    "### Custom Sharding Rules\n",
    "\n",
    "JAX allows defining custom partitioning rules for operations. This is useful for operations where the default partitioning might not be optimal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "i84aKJ1ou3sy"
   },
   "outputs": [],
   "source": [
    "# Advanced topic: Custom partitioning rules\n",
    "from functools import partial\n",
    "\n",
    "def custom_matmul_rule(mesh):\n",
    "    # This is a simplified example - real custom rules would be more complex\n",
    "    def matmul_with_custom_rule(x, y):\n",
    "        # Custom implementation that's aware of sharding\n",
    "        return jnp.matmul(x, y)\n",
    "\n",
    "    return matmul_with_custom_rule\n",
    "\n",
    "# Example usage (conceptual)\n",
    "# my_custom_matmul = custom_matmul_rule(mesh_1d)\n",
    "# result = my_custom_matmul(sharded_a, sharded_b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "93CxXrqgu3sz"
   },
   "source": [
    "## Conclusion\n",
    "\n",
    "In this tutorial, we've explored JAX's powerful capabilities for distributed computation using sharded matrices and collective operations. We've covered:\n",
    "\n",
    "1. **Setting up a device mesh** for organizing available devices\n",
    "2. **Creating and using sharded matrices** to distribute data across devices\n",
    "3. **Different sharding strategies** and when to use them\n",
    "4. **Collective operations** for efficient device-to-device communication\n",
    "5. **Implementing distributed algorithms** using these primitives\n",
    "6. **Automatic sharding** with pjit for easier distributed programming\n",
    "7. **Application to distributed training** for machine learning models\n",
    "\n",
    "JAX's sharding capabilities enable efficient scaling of numerical computations across multiple devices, making it a powerful tool for large-scale machine learning and scientific computing.\n",
    "\n",
    "### Further Resources\n",
    "\n",
    "- [JAX Documentation](https://jax.readthedocs.io/)\n",
    "- [JAX Docs: Distributed arrays and automatic parallelization](https://docs.jax.dev/en/latest/notebooks/Distributed_arrays_and_automatic_parallelization.html)\n",
    "- [JAX Docs: Manual parallelism with `shard_map`](https://docs.jax.dev/en/latest/notebooks/shard_map.html)\n",
    "- [How to Scale Your Model: Sharded Matrices and How to Multiply Them](https://jax-ml.github.io/scaling-book/sharding/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hovJ0-9yu3sz"
   },
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "TPU",
  "colab": {
   "gpuType": "V28",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
