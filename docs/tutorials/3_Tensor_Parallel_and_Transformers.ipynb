{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "17be077e",
   "metadata": {},
   "source": [
    "# Tutorial 3: Tensor Parallel and Transformers Scaling\n",
    "\n",
    "[![Open in GitHub](https://img.shields.io/badge/Open%20in-GitHub-181717?style=flat-square&logo=github)](https://github.com/sshkhr/MinText/blob/main/docs/tutorials/3_Tensor_Parallel_and_Transformers.ipynb)\n",
    "[![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/sshkhr/MinText/blob/main/docs/tutorials/3_Tensor_Parallel_and_Transformers.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47d7d9d3",
   "metadata": {},
   "source": [
    "In the previous tutorial, we learned about data parallelism and how to use it to shard data batches across devices. We also learned about Fully Sharded Data Parallel (FSDP) and how it can be used to shard model parameters, gradients and optimizer states across devices. In this part, we will cover tensor parallelism and how it can be used to shard model layers across devices. We will also learn how to use the different parallelism techniques together to scale up training of large transformer models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "992ec8d3",
   "metadata": {},
   "source": [
    "### 0.1 Setup\n",
    "\n",
    "Let's start by importing the necessary libraries and initializing our environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd37fc51",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# Force JAX to see 8 devices for this tutorial (only use if not using TPU runtime)\n",
    "#os.environ['XLA_FLAGS'] = '--xla_force_host_platform_device_count=8'\n",
    "\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from jax.sharding import Mesh, PartitionSpec as P, NamedSharding\n",
    "from jax.experimental import mesh_utils\n",
    "from flax import nnx\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "from functools import partial\n",
    "import dataclasses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea947ba6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JAX version: 0.5.2\n",
      "Available devices: [TpuDevice(id=0, process_index=0, coords=(0,0,0), core_on_chip=0), TpuDevice(id=1, process_index=0, coords=(0,0,0), core_on_chip=1), TpuDevice(id=2, process_index=0, coords=(1,0,0), core_on_chip=0), TpuDevice(id=3, process_index=0, coords=(1,0,0), core_on_chip=1)]...\n",
      "Number of devices: 8\n"
     ]
    }
   ],
   "source": [
    "# Check available devices\n",
    "print(f\"JAX version: {jax.__version__}\")\n",
    "print(f\"Available devices: {jax.devices()[:4]}...\")\n",
    "print(f\"Number of devices: {jax.device_count()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "425d3061",
   "metadata": {},
   "source": [
    "## 1. Tensor Parallelism\n",
    "\n",
    "While fully-sharded data parallelism distributes model weights across different devices during the AllReduce operation, tensor parallelism takes a different approach. Also known as \"1D model parallelism\" or Megatron sharding, this technique shards the feedforward dimensions of individual model layers and distributes activations between devices during computation. This method enables smaller effective batch sizes per device, making it particularly useful for training very large models. The diagram below illustrates how a single matrix is partitioned across devices using this approach:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c76edab",
   "metadata": {},
   "source": [
    "### 1.1 Tensor Parallelism Theory\n",
    "\n",
    "**Sharding**: Model layer activations are sharded along tenso axes across devices, model parameters are replicated on each device.\n",
    "\n",
    "**Equation** (for our MLP example):\n",
    "$$\\text{In}[B, D_Y] \\cdot_D W_\\text{in}[D, F_Y] \\cdot_F W_\\text{out}[F_Y, D] \\rightarrow \\text{Out}[B, D_Y]$$\n",
    "\n",
    "where $F_Y$ indicates the activations are sharded across $Y$ devices."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01006675",
   "metadata": {},
   "source": [
    "![Tensor Parallel](https://jax-ml.github.io/scaling-book/assets/img/model-parallelism-1400.webp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f540e70",
   "metadata": {},
   "source": [
    "<sup> Image Source: [How To Scale Your Model](https://jax-ml.github.io/scaling-book) </sup>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a24d684",
   "metadata": {},
   "source": [
    "### 1.2 Tensor Parallelism Algorithm\n",
    "\n",
    "The computation pattern In[B, D_Y] * W_in[D, F_Y] * W_out[F_Y, D] → Out[B, D_Y] requires gathering activations prior to the initial matrix multiplication. This approach becomes more efficient than ZeRO sharding when activation sizes are smaller than weight sizes. However, this efficiency typically emerges only when combined with some degree of ZeRO sharding, which reduces the gather operation's overhead. This synergy explains why ZeRO sharding and model parallelism are commonly used together in practice.\n",
    "\n",
    "**Forward pass:** need to compute Loss[B]\n",
    "\n",
    "1.  In[B, D] = **AllGather**(In[B, D<sub>Y</sub>]) *(on critical path)*\n",
    "2.  Tmp[B, F<sub>Y</sub>] = In[B, D] \\*<sub>D</sub> W<sub>in</sub>[D, F<sub>Y</sub>] *(not sharded along contracting, so no comms)*\n",
    "3.  Out[B, D] {U<sub>Y</sub>} = Tmp[B, F<sub>Y</sub>] \\*<sub>F</sub> W<sub>out</sub>[F<sub>Y</sub>, D]\n",
    "4.  Out[B, D<sub>Y</sub>] = **ReduceScatter**(Out[B, D] {U<sub>Y</sub>}) *(on critical path)*\n",
    "5.  Loss[B] = ...\n",
    "\n",
    "**Backward pass:** need to compute dW<sub>out</sub>[F<sub>Y</sub>, D], dW<sub>in</sub>[D, F<sub>Y</sub>]\n",
    "\n",
    "1.  dOut[B, D<sub>Y</sub>] = ...\n",
    "2.  dOut[B, D] = **AllGather**(dOut[B, D<sub>Y</sub>]) *(on critical path)*\n",
    "3.  dW<sub>out</sub>[F<sub>Y</sub>, D] = Tmp[B, F<sub>Y</sub>] \\*<sub>B</sub> dOut[B, D]\n",
    "4.  dTmp[B, F<sub>Y</sub>] = dOut[B, D] \\*<sub>D</sub> W<sub>out</sub>[F<sub>Y</sub>, D] *(can throw away dOut[B, D] here)*\n",
    "5.  In[B, D] = **AllGather**(In[B, D<sub>Y</sub>]) *(this can be skipped by sharing with (1) from the forward pass)*\n",
    "6.  dW<sub>in</sub>[D, F<sub>Y</sub>] = dTmp[B, F<sub>Y</sub>] \\*<sub>B</sub> In[B, D]\n",
    "7.  dIn[B, D] {U.Y} = dTmp[B, F<sub>Y</sub>] \\*<sub>F</sub> W<sub>in</sub>[D, F<sub>Y</sub>] *(needed for previous layers)*\n",
    "8.  dIn[B, D<sub>Y</sub>] = **ReduceScatter**(dIn[B, D] {U.Y}) *(on critical path)*\n",
    "\n",
    "A key advantage of the two matrix operations in our MLP forward pass is that tensor parallelism integrates nicely with this setup. Without this optimization, we would need to perform an AllReduce operation after each matrix multiplication. However, the sequential computation In[B, D_Y] * W_in[D, F_Y] → Tmp[B, F_Y] followed by Tmp[B, F_Y] * W_out[F_Y, D] → Out[B, D_Y] allows us to perform a single AllGather on the input at the start and a single ReduceScatter on the output at the end, eliminating the need for intermediate AllReduce operations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5334a065",
   "metadata": {},
   "source": [
    "## 2. Combining Parallelism Techniques\n",
    "\n",
    "In this section, we will combine data parallelism, FSDP, and tensor parallelism to implemnt distributed training of a simple MLP model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60a857dc",
   "metadata": {},
   "source": [
    "### 2.1 Mesh Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45ed2560",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assign logical names 'data' and 'model' to the axes of this grid.\n",
    "# The first dimension (size 2) is named 'data'.\n",
    "# The second dimension (size 4) is named 'model'.\n",
    "mesh = jax.sharding.Mesh(\n",
    "  mesh_utils.create_device_mesh((2, 4)),\n",
    "  ('data', 'model'),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ce31aa1",
   "metadata": {},
   "source": [
    "### 2.2 Sharding Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d266bc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A dataclass to hold sharding rules for different parts of the model/data.\n",
    "# Makes it easy to manage and change sharding strategies.\n",
    "@dataclasses.dataclass(unsafe_hash=True)\n",
    "class MeshRules:\n",
    "  embed: str | None = None # Sharding rule for embedding-like dimensions\n",
    "  mlp: str | None = None   # Sharding rule for MLP layers dimensions\n",
    "  data: str | None = None  # Sharding rule for the data batch dimension\n",
    "\n",
    "  # Allows calling the instance like `mesh_rules('embed', 'mlp')`\n",
    "  # to get a tuple of the corresponding sharding rules.\n",
    "  def __call__(self, *keys: str) -> tuple[str, ...]:\n",
    "    return tuple(getattr(self, key) for key in keys)\n",
    "\n",
    "# Create an instance of MeshRules defining the specific strategy:\n",
    "# - 'embed' dimensions will be replicated (None).\n",
    "# - 'mlp' dimensions will be sharded along the 'model' mesh axis.\n",
    "# - 'data' dimensions will be sharded along the 'data' mesh axis.\n",
    "mesh_rules = MeshRules(\n",
    "  embed=None,\n",
    "  mlp='model',\n",
    "  data='data',\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f6e98b3",
   "metadata": {},
   "source": [
    "### 2.3 Define The Sharded Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31970b77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the MLP using Flax NNX API.\n",
    "class MLP(nnx.Module):\n",
    "  def __init__(self, din, dmid, dout, rngs: nnx.Rngs):\n",
    "    self.w1 = nnx.Param(\n",
    "      nnx.initializers.lecun_normal()(rngs.params(), (din, dmid)),\n",
    "      # ('embed', 'mlp') -> (None, 'model') -> Replicate dim 0, shard dim 1 along 'model' axis.\n",
    "      sharding=mesh_rules('embed', 'mlp'),\n",
    "    )\n",
    "    self.b1 = nnx.Param(\n",
    "      jnp.zeros((dmid,)),\n",
    "      # Sharding: ('mlp',) -> ('model',) -> Shard dim 0 along 'model' axis.\n",
    "      sharding=mesh_rules('mlp'),\n",
    "    )\n",
    "    self.w2 = nnx.Param(\n",
    "      nnx.initializers.lecun_normal()(rngs.params(), (dmid, dout)),\n",
    "       # Sharding: ('embed', 'mlp') -> (None, 'model') -> Replicate dim 0, shard dim 1 along 'model' axis.\n",
    "      sharding=mesh_rules('embed', 'mlp'),\n",
    "    )\n",
    "\n",
    "  def __call__(self, x: jax.Array):\n",
    "    return nnx.relu(x @ self.w1 + self.b1) @ self.w2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06d184b8",
   "metadata": {},
   "source": [
    "### 2.4 Handling Sharded Optimizer State"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d9f4258",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a custom type for SGD momentum state, inheriting from nnx.Variable.\n",
    "# This allows it to be tracked as part of the NNX state tree.\n",
    "class SGDState(nnx.Variable):\n",
    "  pass\n",
    "\n",
    "# Define the SGD optimizer using NNX API.\n",
    "class SGD(nnx.Object):\n",
    "  # Constructor takes the model parameters (as nnx.State), learning rate, and decay.\n",
    "  def __init__(self, params: nnx.State, lr, decay=0.9):\n",
    "    # Helper function to initialize momentum buffer for a given parameter.\n",
    "    def init_optimizer_state(variable: nnx.Variable):\n",
    "      # Create momentum state with zeros, same shape and metadata (incl. sharding)\n",
    "      # as the parameter it corresponds to.\n",
    "      return SGDState(\n",
    "        jnp.zeros_like(variable.value), **variable.get_metadata()\n",
    "      )\n",
    "\n",
    "    self.lr = lr\n",
    "    # Store a reference to the parameter State tree.\n",
    "    self.params = params\n",
    "    # Create the momentum state tree, mirroring the structure of 'params',\n",
    "    # using the helper function. Momentum will have the same sharding as params.\n",
    "    self.momentum = jax.tree.map(init_optimizer_state, self.params)\n",
    "    self.decay = decay\n",
    "\n",
    "  # Method to update parameters based on gradients.\n",
    "  def update(self, grads: nnx.State):\n",
    "    # Define the update logic for a single parameter/momentum/gradient triple.\n",
    "    def update_fn(\n",
    "      params: nnx.Variable, momentum: SGDState, grad: nnx.VariableState\n",
    "    ):\n",
    "      # Standard SGD with momentum update rule.\n",
    "      # v_t = β * v_{t-1} + (1 - β) * ∇J(θ_t)\n",
    "      momentum.value = self.decay * momentum.value + (1 - self.decay) * grad.value\n",
    "      # θ_{t+1} = θ_t - α * v_t\n",
    "      params.value -= self.lr * momentum.value # NOTE: Direct mutation of param value!\n",
    "\n",
    "    # Apply the update function across the parameter, momentum, and gradient trees.\n",
    "    # This performs the update in-place on the parameter values referenced by self.params.\n",
    "    jax.tree.map(update_fn, self.params, self.momentum, grads)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "850d7981",
   "metadata": {},
   "source": [
    "### 2.5 Applying Sharding to the Model and Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f54c41e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# JIT-compile the model and optimizer creation function.\n",
    "@nnx.jit\n",
    "def create_model():\n",
    "  # Instantiate the MLP model. rngs=nnx.Rngs(0) provides PRNG keys.\n",
    "  model = MLP(1, 32, 1, rngs=nnx.Rngs(0))\n",
    "  # Create the optimizer. nnx.variables(model, nnx.Param) extracts\n",
    "  # only the nnx.Param state variables from the model object.\n",
    "  optimizer = SGD(nnx.variables(model, nnx.Param), 0.01, decay=0.9)\n",
    "\n",
    "  # === Explicit Sharding Application ===\n",
    "  # 1. Extract ALL state (model params + optimizer momentum) into a flat State pytree.\n",
    "  state = nnx.state(optimizer)\n",
    "\n",
    "  # 2. Define the target sharding for the state pytree.\n",
    "  # This function maps state paths to NamedSharding objects based on stored metadata.\n",
    "  def get_named_shardings(path: tuple, value: nnx.VariableState):\n",
    "    # Assumes params and momentum use the sharding defined in their metadata.\n",
    "    if path[0] in ('params', 'momentum'):\n",
    "      # value.sharding contains the tuple like ('model',) or (None, 'model')\n",
    "      # stored during Param/SGDState creation.\n",
    "      return value.replace(NamedSharding(mesh, P(*value.sharding)))\n",
    "    else:\n",
    "      # Handle other state if necessary (e.g., learning rate if it were a Variable)\n",
    "      raise ValueError(f'Unknown path: {path}')\n",
    "  # Create the pytree of NamedSharding objects.\n",
    "  named_shardings = state.map(get_named_shardings)\n",
    "\n",
    "  # 3. Apply sharding constraint. This tells JAX how the 'state' pytree\n",
    "  # SHOULD be sharded when computations involving it are run under jit/pjit.\n",
    "  # It doesn't immediately move data but sets up the constraint for the compiler.\n",
    "  sharded_state = jax.lax.with_sharding_constraint(state, named_shardings)\n",
    "\n",
    "  # 4. Update the original objects (model params, optimizer momentum)\n",
    "  # with the constrained state values. This step makes the sharding\n",
    "  # \"stick\" to the objects themselves for subsequent use outside this function.\n",
    "  nnx.update(optimizer, sharded_state)\n",
    "\n",
    "  # Return the model and optimizer objects, now containing sharded state variables.\n",
    "  return model, optimizer\n",
    "\n",
    "# Call the function to create the sharded model and optimizer.\n",
    "model, optimizer = create_model()\n",
    "\n",
    "# Visualize the sharding of the first weight's parameter tensor.\n",
    "jax.debug.visualize_array_sharding(model.w1.value)\n",
    "# Visualize the sharding of the first weight's momentum tensor.\n",
    "jax.debug.visualize_array_sharding(optimizer.momentum.w1.value)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bcf251c",
   "metadata": {},
   "source": [
    "### 2.6 Distributed Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca7002f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# JIT-compile the training step function.\n",
    "@nnx.jit\n",
    "def train_step(model: MLP, optimizer: SGD, x, y):\n",
    "  # Define the loss function (Mean Squared Error).\n",
    "  # Takes the model object as input, consistent with nnx.value_and_grad.\n",
    "  def loss_fn(model):\n",
    "    y_pred = model(x) # Forward pass\n",
    "    loss = jnp.mean((y - y_pred) ** 2)\n",
    "    return loss\n",
    "\n",
    "  # Calculate loss and gradients w.r.t the model's state (its nnx.Param variables).\n",
    "  # 'grad' will be an nnx.State object mirroring model's Param structure.\n",
    "  loss, grad = nnx.value_and_grad(loss_fn)(model)\n",
    "\n",
    "  # Call the optimizer's update method to apply gradients.\n",
    "  # This updates the model parameters in-place.\n",
    "  optimizer.update(grad)\n",
    "\n",
    "  # Return the calculated loss.\n",
    "  return loss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7296dd13",
   "metadata": {},
   "source": [
    "### 2.7 Training Loop and Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beed9e47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate synthetic dataset: y = 0.8*x^2 + 0.1 + noise\n",
    "X = np.linspace(-2, 2, 100)[:, None] # Input features\n",
    "Y = 0.8 * X**2 + 0.1 + np.random.normal(0, 0.1, size=X.shape) # Target values\n",
    "\n",
    "# A generator function to yield batches of data for training.\n",
    "def dataset(batch_size, num_steps):\n",
    "  for _ in range(num_steps):\n",
    "    # Randomly sample indices for the batch.\n",
    "    idx = np.random.choice(len(X), size=batch_size)\n",
    "    # Yield the corresponding input and target pairs.\n",
    "    yield X[idx], Y[idx]\n",
    "\n",
    "# --- Training Loop ---\n",
    "losses = [] # To store loss values for plotting\n",
    "# Iterate through the dataset generator for 10,000 steps.\n",
    "for step, (x_batch, y_batch) in enumerate(\n",
    "  dataset(batch_size=32, num_steps=10_000)\n",
    "):\n",
    "  # CRITICAL: Place the NumPy data onto JAX devices AND apply sharding.\n",
    "  # named_sharding('data') -> Shard along the 'data' mesh axis (first dim, size 2).\n",
    "  # Each device along the 'data' axis gets a slice of the batch.\n",
    "  x_batch, y_batch = jax.device_put((x_batch, y_batch), named_sharding('data'))\n",
    "\n",
    "  # Execute the JIT-compiled training step with the sharded model, optimizer, and data.\n",
    "  loss = train_step(model, optimizer, x_batch, y_batch)\n",
    "\n",
    "  # Record the loss (move scalar loss back to host CPU).\n",
    "  losses.append(float(loss))\n",
    "  # Log progress periodically.\n",
    "  if step % 1000 == 0:\n",
    "    print(f'Step {step}: Loss = {loss}')\n",
    "\n",
    "# --- Plotting Results ---\n",
    "plt.figure()\n",
    "plt.title(\"Training Loss\")\n",
    "plt.plot(losses[20:]) # Plot loss, skipping initial noisy steps\n",
    "plt.xlabel(\"Step\")\n",
    "plt.ylabel(\"MSE Loss\")\n",
    "\n",
    "# Get model predictions on the full dataset (X is on host CPU).\n",
    "# Model applies function executes potentially on device, result brought back implicitly.\n",
    "y_pred = model(X)\n",
    "plt.figure()\n",
    "plt.title(\"Model Fit\")\n",
    "plt.scatter(X, Y, color='blue', label='Data') # Original data\n",
    "plt.plot(X, y_pred, color='black', label='Prediction') # Model's predictions\n",
    "plt.xlabel(\"X\")\n",
    "plt.ylabel(\"Y\")\n",
    "plt.legend()\n",
    "plt.show() # Display the plots\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22cf842d",
   "metadata": {},
   "source": [
    "## 3. Profiling Parallel Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa58e237",
   "metadata": {},
   "source": [
    "## 4. Transformer Scaling with Tensor Parallelism"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "373419be",
   "metadata": {},
   "source": [
    "![Megatraon-LM](https://docs.pytorch.org/tutorials/_images/megatron_lm.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a36c340b",
   "metadata": {},
   "source": [
    "<sup> Image Source: [Megatron-LM](https://arxiv.org/abs/1909.08053) </sup>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97fa5689",
   "metadata": {},
   "source": [
    "## 5. Scaling Transformers in Flax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24340e12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define our device mesh - same as in the original JAX implementation\n",
    "mesh = jax.make_mesh((4, 2), ('x', 'y'))\n",
    "\n",
    "# Define logical axis names and their mapping to device mesh dimensions\n",
    "# This maps conceptual dimensions to physical device axes\n",
    "sharding_rules = [\n",
    "    ('batch', 'x'),\n",
    "    ('sequence', None),\n",
    "    ('d_model', None),\n",
    "    ('query_heads', 'y'),\n",
    "    ('key_heads', 'y'),\n",
    "    ('key_dim', None),\n",
    "    ('d_ff', 'y'),\n",
    "    ('vocab', None)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5fb81c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration class\n",
    "class TransformerConfig:\n",
    "  def __init__(self):\n",
    "        # Significantly reduced model size\n",
    "        self.d_model = 512       \n",
    "        self.ffw_multiplier = 4\n",
    "        self.num_layers = 2      \n",
    "        self.query_heads = 8     \n",
    "        self.kv_heads = 8        \n",
    "        self.key_dim = 64        \n",
    "        self.vocab_size = 1024   \n",
    "        self.dtype = jnp.bfloat16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e28a003",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RMS Normalization layer\n",
    "class RMSNorm(nnx.Module):\n",
    "    def __init__(self, dim, rngs):\n",
    "        # Initialize gamma parameter with proper sharding annotation\n",
    "        self.gamma = nnx.Param(\n",
    "            nnx.with_metadata(\n",
    "                jax.nn.initializers.ones,\n",
    "                sharding=('d_model',),\n",
    "                sharding_rules=sharding_rules\n",
    "            )(rngs.params(), (dim,))\n",
    "        )\n",
    "    \n",
    "    def __call__(self, x):\n",
    "        rms = jax.lax.rsqrt(jnp.mean(jnp.square(x), axis=-1, keepdims=True) + 1e-6)\n",
    "        return self.gamma.value * x * rms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca2bbc61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformer layer implementation\n",
    "class TransformerLayer(nnx.Module):\n",
    "    def __init__(self, cfg, rngs):\n",
    "        # Initialize with he_normal (same as original)\n",
    "        init_fn = jax.nn.initializers.he_normal()\n",
    "        \n",
    "        # QKV projections with appropriate sharding annotations\n",
    "        self.q = nnx.Param(\n",
    "            nnx.with_metadata(\n",
    "                init_fn,\n",
    "                sharding=('d_model', 'query_heads', 'key_dim'),\n",
    "                sharding_rules=sharding_rules\n",
    "            )(rngs.params(), (cfg.d_model, cfg.query_heads, cfg.key_dim))\n",
    "        )\n",
    "        \n",
    "        self.k = nnx.Param(\n",
    "            nnx.with_metadata(\n",
    "                init_fn,\n",
    "                sharding=('d_model', 'key_heads', 'key_dim'),\n",
    "                sharding_rules=sharding_rules\n",
    "            )(rngs.params(), (cfg.d_model, cfg.kv_heads, cfg.key_dim))\n",
    "        )\n",
    "        \n",
    "        self.v = nnx.Param(\n",
    "            nnx.with_metadata(\n",
    "                init_fn,\n",
    "                sharding=('d_model', 'key_heads', 'key_dim'),\n",
    "                sharding_rules=sharding_rules\n",
    "            )(rngs.params(), (cfg.d_model, cfg.kv_heads, cfg.key_dim))\n",
    "        )\n",
    "        \n",
    "        # Output projection\n",
    "        self.proj = nnx.Param(\n",
    "            nnx.with_metadata(\n",
    "                init_fn,\n",
    "                sharding=('query_heads', 'key_dim', 'd_model'),\n",
    "                sharding_rules=sharding_rules\n",
    "            )(rngs.params(), (cfg.query_heads, cfg.key_dim, cfg.d_model))\n",
    "        )\n",
    "        \n",
    "        # FFN parameters\n",
    "        self.w1 = nnx.Param(\n",
    "            nnx.with_metadata(\n",
    "                init_fn,\n",
    "                sharding=('d_model', 'd_ff'),\n",
    "                sharding_rules=sharding_rules\n",
    "            )(rngs.params(), (cfg.d_model, cfg.d_model * cfg.ffw_multiplier))\n",
    "        )\n",
    "        \n",
    "        self.w2 = nnx.Param(\n",
    "            nnx.with_metadata(\n",
    "                init_fn,\n",
    "                sharding=('d_ff', 'd_model'),\n",
    "                sharding_rules=sharding_rules\n",
    "            )(rngs.params(), (cfg.d_model * cfg.ffw_multiplier, cfg.d_model))\n",
    "        )\n",
    "        \n",
    "        # Layer normalization parameters\n",
    "        self.norm1 = RMSNorm(cfg.d_model, rngs)\n",
    "        self.norm2 = RMSNorm(cfg.d_model, rngs)\n",
    "        \n",
    "        # Store config values needed during forward pass\n",
    "        self.query_heads = cfg.query_heads\n",
    "        self.kv_heads = cfg.kv_heads\n",
    "    \n",
    "    def __call__(self, x):\n",
    "        # Apply sharding constraint to input\n",
    "        x = jax.lax.with_sharding_constraint(x, PartitionSpec('x', None, 'x'))\n",
    "        \n",
    "        # First normalization and attention block\n",
    "        with jax.named_scope('pre_attn_norm'):\n",
    "            attn_in = self.norm1(x)\n",
    "        \n",
    "        # QKV projections\n",
    "        with jax.named_scope('qkv_matmul'):\n",
    "            q = jnp.einsum('btd,dhq->bhtq', attn_in, self.q.value)\n",
    "            k = jnp.einsum('btd,dhk->bhtk', attn_in, self.k.value)\n",
    "            v = jnp.einsum('btd,dhv->bhtv', attn_in, self.v.value)\n",
    "        \n",
    "        # Attention computation\n",
    "        with jax.named_scope('attn'):\n",
    "            scale = q.shape[-1] ** -0.5\n",
    "            num_query_heads, num_kv_heads = self.query_heads, self.kv_heads\n",
    "            \n",
    "            if num_query_heads == num_kv_heads or num_kv_heads == 1:\n",
    "                qk = jnp.einsum('bhtd,bhsd->bhts', q, k) * scale\n",
    "                logits = jax.nn.softmax(qk.astype(jnp.float32), axis=-1)\n",
    "                attn_vec = jnp.einsum('bhsd,bhts->bhtd', v, logits)\n",
    "            else:\n",
    "                assert num_query_heads % num_kv_heads == 0\n",
    "                q = q.reshape(q.shape[0:1] +\n",
    "                            (num_kv_heads, num_query_heads // num_kv_heads) +\n",
    "                            q.shape[2:])\n",
    "                qk = jnp.einsum('bqhtd,bhsd->bqhts', q, k) * scale\n",
    "                logits = jax.nn.softmax(qk.astype(jnp.float32), axis=-1)\n",
    "                attn_vec = jnp.einsum('bqsd,bqhts->bqhtd', v, logits)\n",
    "                attn_vec = attn_vec.reshape(attn_vec.shape[0:1] + (num_query_heads,) + attn_vec.shape[3:])\n",
    "        \n",
    "        # Attention projection\n",
    "        with jax.named_scope('attn_proj'):\n",
    "            attn_out = jnp.einsum('bhtv,hvd->btd', attn_vec, self.proj.value)\n",
    "        \n",
    "        # First residual connection\n",
    "        with jax.named_scope('residual'):\n",
    "            x = x + attn_out\n",
    "        \n",
    "        # Second normalization\n",
    "        with jax.named_scope('ffn_pre_norm'):\n",
    "            ffw_in = self.norm2(x)\n",
    "        \n",
    "        # Apply sharding constraint before FFN\n",
    "        ffw_in = jax.lax.with_sharding_constraint(ffw_in, PartitionSpec('x', None, 'x'))\n",
    "        \n",
    "        # FFN block\n",
    "        with jax.named_scope('ffw'):\n",
    "            # Add explicit sharding constraints on weights\n",
    "            w1 = jax.lax.with_sharding_constraint(\n",
    "                self.w1.value,\n",
    "                PartitionSpec(None, 'y')\n",
    "            )\n",
    "            w2 = jax.lax.with_sharding_constraint(\n",
    "                self.w2.value,\n",
    "                PartitionSpec('y', None)\n",
    "            )\n",
    "            \n",
    "            ffw_out = jnp.einsum('btd,df->btf', ffw_in, w1).astype(jnp.bfloat16)\n",
    "            \n",
    "            # Add sharding constraint on intermediate activations\n",
    "            ffw_out = jax.lax.with_sharding_constraint(\n",
    "                ffw_out, \n",
    "                PartitionSpec('x', None, 'y')\n",
    "            )\n",
    "            \n",
    "            ffw_out = jax.nn.gelu(ffw_out)\n",
    "            ffw_out = jnp.einsum('btf,fd->btd', ffw_out, w2).astype(jnp.bfloat16)\n",
    "        \n",
    "        # Second residual connection\n",
    "        with jax.named_scope('residual'):\n",
    "            x = x + ffw_out\n",
    "        \n",
    "        # Final sharding constraint\n",
    "        x = jax.lax.with_sharding_constraint(x, PartitionSpec('x', None, 'x'))\n",
    "        \n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18faa0e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Full transformer model\n",
    "class Transformer(nnx.Module):\n",
    "    def __init__(self, cfg, rngs):\n",
    "        # Token embedding with sharding annotation\n",
    "        self.embedding = nnx.Param(\n",
    "            nnx.with_metadata(\n",
    "                jax.nn.initializers.he_normal(),\n",
    "                sharding=('vocab', 'd_model'),\n",
    "                sharding_rules=sharding_rules\n",
    "            )(rngs.params(), (cfg.vocab_size, cfg.d_model))\n",
    "        )\n",
    "        \n",
    "        # Create all transformer layers\n",
    "        self.layers = [TransformerLayer(cfg, rngs) for _ in range(cfg.num_layers)]\n",
    "        self.vocab_size = cfg.vocab_size\n",
    "    \n",
    "    def __call__(self, x):\n",
    "        # Input embedding\n",
    "        one_hot = jax.nn.one_hot(x, self.vocab_size)\n",
    "        x = jnp.einsum('vd,btv->btd', self.embedding.value, one_hot)\n",
    "        \n",
    "        # Process through each layer\n",
    "        for idx, layer in enumerate(self.layers):\n",
    "            with jax.named_scope(f'layer_{idx}'):\n",
    "                x = layer(x)\n",
    "        \n",
    "        # Final projection to logits\n",
    "        logits = jnp.einsum('vd,btd->btv', self.embedding.value, x)\n",
    "        return jax.nn.log_softmax(logits, axis=-1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1951ee82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to create a sharded model\n",
    "@nnx.jit\n",
    "def create_sharded_model():\n",
    "    # Initialize configuration\n",
    "    cfg = TransformerConfig()\n",
    "    \n",
    "    # Create unsharded model\n",
    "    model = Transformer(cfg, rngs=nnx.Rngs(42))\n",
    "    \n",
    "    # Get model state and partition specs from annotations\n",
    "    state = nnx.state(model)\n",
    "    pspecs = nnx.get_partition_spec(state)\n",
    "    \n",
    "    # Apply sharding constraints to state\n",
    "    sharded_state = jax.lax.with_sharding_constraint(state, pspecs)\n",
    "    \n",
    "    # Update model with sharded state\n",
    "    nnx.update(model, sharded_state)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18184732",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create sample data with appropriate sharding\n",
    "def create_sample_data(batch_size, seq_len):\n",
    "    data_sharding = NamedSharding(mesh, PartitionSpec('x', None))\n",
    "    x = jnp.zeros((batch_size, seq_len), dtype=jnp.int32)\n",
    "    return jax.device_put(x, data_sharding)\n",
    "\n",
    "# Training step function\n",
    "@nnx.jit\n",
    "def train_step(model, optimizer, x, y):\n",
    "    def loss_fn(model):\n",
    "        logits = model(x)\n",
    "        \n",
    "        # Cross-entropy loss\n",
    "        labels_onehot = jax.nn.one_hot(y, model.vocab_size)\n",
    "        loss = -jnp.sum(labels_onehot * logits, axis=-1)\n",
    "        return jnp.mean(loss)\n",
    "    \n",
    "    loss, grads = nnx.value_and_grad(loss_fn)(model)\n",
    "    optimizer.update(grads)\n",
    "    \n",
    "    return loss\n",
    "\n",
    "# Profiling utility function as mentioned in the flax tutorial\n",
    "def block_all(xs):\n",
    "    jax.tree_util.tree_map(lambda x: x.block_until_ready(), xs)\n",
    "    return xs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fe999e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main execution block\n",
    "with mesh:\n",
    "    # Create sharded model\n",
    "    print(\"Creating sharded model...\")\n",
    "    sharded_model = create_sharded_model()\n",
    "    \n",
    "    # Create optimizer\n",
    "    print(\"Creating optimizer...\")\n",
    "    optimizer = nnx.Optimizer(sharded_model, optax.adam(1e-3))\n",
    "    \n",
    "    # Create sample data\n",
    "    print(\"Creating sample data...\")\n",
    "    batch_size, seq_len = 8, 1024\n",
    "    input_ids = create_sample_data(batch_size, seq_len)\n",
    "    target_ids = create_sample_data(batch_size, seq_len)\n",
    "    \n",
    "    # Train for a few steps\n",
    "    print(\"Starting training...\")\n",
    "    for i in range(5):\n",
    "        loss = block_all(train_step(sharded_model, optimizer, input_ids, target_ids))\n",
    "        print(f\"Step {i+1}, Loss: {loss}\")\n",
    "    \n",
    "    # Optionally profile the training step\n",
    "    print(\"\\nProfiling training step...\")\n",
    "    import time\n",
    "    \n",
    "    # Warmup\n",
    "    _ = block_all(train_step(sharded_model, optimizer, input_ids, target_ids))\n",
    "    \n",
    "    # Measure\n",
    "    start_time = time.time()\n",
    "    for _ in range(10):\n",
    "        _ = block_all(train_step(sharded_model, optimizer, input_ids, target_ids))\n",
    "    end_time = time.time()\n",
    "    \n",
    "    print(f\"Average step time: {(end_time - start_time) / 10:.4f} seconds\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
