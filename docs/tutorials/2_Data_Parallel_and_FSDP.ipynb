{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Parallel and Fully Sharded Data Parallel Training\n",
    "\n",
    "In the previous tutorial, we explored the basics of JAX parallelization, including device meshes, sharded matrices, and collective operations. In this tutorial, we'll build on those concepts to implement data parallel (DP) and fully sharded data parallel (FSDP) training for neural networks.\n",
    "\n",
    "We'll cover:\n",
    "1. 8-way data parallel training with plain JAX\n",
    "2. Re-implementing the same with Flax NNX\n",
    "3. Fully sharded data parallel (FSDP) training with Flax NNX\n",
    "\n",
    "By the end of this tutorial, you'll understand how to scale neural network training using parallelization techniques and how to leverage Flax's high-level abstractions for efficient distributed training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "Let's start by importing the necessary libraries and initializing our environment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Theory: Understanding Parallelism Strategies\n",
    "\n",
    "Before diving into the implementation, let's understand the theoretical foundations of different parallelism strategies. This section draws from \\\"How to Scale Your Model\\\" by Jacob Austin and others.\n",
    "\n",
    "### Model Representation\n",
    "\n",
    "For simplicity, we approximate a Transformer as a stack of MLP blocks, since attention is a comparatively small fraction of the FLOPs for larger models. Each layer consists of two main operations:\n",
    "\n",
    "![Simple Transformer](https://github.com/jax-ml/scaling-book/blob/main/assets/img/simple-transformer.png?raw=true)\n",
    "\n",
    "<sup> Image Source: [How To Scale Your Model](https://jax-ml.github.io/scaling-book) </sup>\n",
    "\n",
    "We treat each layer as:\n",
    "- **W<sub>in</sub>**: `bf16[D, F]` (up-projection) \n",
    "- **W<sub>out</sub>**: `bf16[F, D]` (down-projection)\n",
    "- **Input**: `bf16[B, D]`\n",
    "\n",
    "Where:\n",
    "- **D** = d<sub>model</sub> (hidden dimension)\n",
    "- **F** = d<sub>ff</sub> (feed-forward dimension) \n",
    "- **B** = batch size (total tokens)\n",
    "\n",
    "### Communication vs Computation Trade-offs\n",
    "\n",
    "The goal of scaling is to achieve **strong scaling**: linear increase in throughput with more chips. Performance depends on hiding inter-chip communication by overlapping it with useful FLOPs. \n",
    "\n",
    "We become **compute-bound** when:\n",
    "$$\\frac{T_{\\text{math}}}{T_{\\text{comms}}} > 1$$\n",
    "\n",
    "The key insight is that computation time scales with batch size, while communication time is often independent of batch size (since we transfer model weights).\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# Force JAX to see 8 devices for this tutorial\n",
    "os.environ['XLA_FLAGS'] = '--xla_force_host_platform_device_count=8'\n",
    "\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from jax.sharding import Mesh, PartitionSpec as P, NamedSharding\n",
    "from jax.experimental import mesh_utils\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "from functools import partial\n",
    "\n",
    "# Import our custom modules\n",
    "from jax_mlp import (\n",
    "    init_mlp_params, mlp_forward, mse_loss, train_step,\n",
    "    generate_synthetic_data, train_model\n",
    ")\n",
    "from nnx_mlp import MLP, mse_loss as nnx_mse_loss, train_step as nnx_train_step\n",
    "\n",
    "# Check available devices\n",
    "print(f\"JAX version: {jax.__version__}\")\n",
    "print(f\"Available devices: {jax.devices()[:4]}...\")\n",
    "print(f\"Number of devices: {jax.device_count()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 8-way Data Parallel Training with Plain JAX\n",
    "\n",
    "Data parallelism is a strategy where we replicate the model across multiple devices and shard the data batch. Each device processes a portion of the batch using its copy of the model, and then we aggregate the gradients across all devices.\n",
    "\n",
    "Let's implement 8-way data parallel training, following the approach in `jax_data_parallel.py`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Parallelism Theory\n",
    "\n",
    "**Definition**: Activations sharded along batch dimension, parameters replicated on each device. Communication only occurs during the backward pass.\n",
    "\n",
    "**Mathematical representation**:\n",
    "$$\\text{In}[B_X, D] \\cdot_D W_{\\text{in}}[D, F] \\cdot_F W_{\\text{out}}[F, D] \\rightarrow \\text{Out}[B_X, D]$$\n",
    "\n",
    "where $B_X$ indicates the batch is sharded across $X$ devices.\n",
    "\n",
    "![Data Parallelism](https://github.com/jax-ml/scaling-book/blob/main/assets/img/data-parallelism.png?raw=true)\n",
    "\n",
    "<sup> Image Source: [How To Scale Your Model](https://jax-ml.github.io/scaling-book) </sup>\n",
    "\n",
    "**Key properties**:\n",
    "- Forward pass requires **no communication** \n",
    "- Backward pass requires **AllReduce on gradients**\n",
    "- Model parameters and optimizer states are fully replicated\n",
    "- Memory usage scales with number of devices\n",
    "\n",
    "**Algorithm**:\n",
    "\n",
    "**Forward pass:**\n",
    "1. Tmp[B<sub>X</sub>, F] = In[B<sub>X</sub>, D] ×<sub>D</sub> W<sub>in</sub>[D, F]\n",
    "2. Out[B<sub>X</sub>, D] = Tmp[B<sub>X</sub>, F] ×<sub>F</sub> W<sub>out</sub>[F, D]\n",
    "\n",
    "**Backward pass:**\n",
    "1. dW<sub>out</sub>[F, D] = **AllReduce**(Tmp[B<sub>X</sub>, F] ×<sub>B</sub> dOut[B<sub>X</sub>, D])\n",
    "2. dW<sub>in</sub>[D, F] = **AllReduce**(In[B<sub>X</sub>, D] ×<sub>B</sub> dTmp[B<sub>X</sub>, F])\n",
    "\n",
    "**When do we become communication-bound?**\n",
    "\n",
    "For TPUv5p with $C = 4.6 \\times 10^{14}$ FLOPs/s and $W = 2 \\times 9 \\times 10^{10}$ bytes/s:\n",
    "\n",
    "$$\\frac{B}{X} > \\frac{C}{W_{\\text{ici}}} = 2550$$\n",
    "\n",
    "So our **batch size per chip must be at least 2,550** to avoid being communication-bound with 1D data parallelism.\n",
    "\n",
    "**Limitations**: Largest model we can train has approximately $\\text{HBM per device} / 10$ parameters (≈9B for TPUv5p with Adam optimizer).\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Generate Training Data\n",
    "\n",
    "First, let's generate our synthetic dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate training data\n",
    "seed = 42\n",
    "key = jax.random.PRNGKey(seed)\n",
    "num_samples = 10000\n",
    "input_dim = 784  # Similar to MNIST input dimension\n",
    "\n",
    "x_data, y_data = generate_synthetic_data(num_samples, input_dim, key)\n",
    "\n",
    "print(f\"Data shape: x={x_data.shape}, y={y_data.shape}\")\n",
    "print(f\"Example: x[0] sum = {jnp.sum(x_data[0]**2):.4f}, y[0] = {y_data[0][0]:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Single-Device Baseline\n",
    "\n",
    "Let's first establish a baseline by training on a single device."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model architecture - larger model to demonstrate parallelization benefits\n",
    "layer_sizes = [input_dim, 8192, 8192, 8192, 10]\n",
    "batch_size = 8192  # Large batch size as in the reference\n",
    "learning_rate = 1e-5\n",
    "num_epochs = 5\n",
    "\n",
    "# Initialize model parameters\n",
    "init_key = jax.random.PRNGKey(123)\n",
    "params = init_mlp_params(layer_sizes, init_key)\n",
    "\n",
    "# Place everything on a single device\n",
    "single_device = jax.devices()[0]\n",
    "params_single = jax.device_put(params, single_device)\n",
    "x_data_single = jax.device_put(x_data, single_device)\n",
    "y_data_single = jax.device_put(y_data, single_device)\n",
    "\n",
    "# Train on single device\n",
    "print(\"Training on single device...\")\n",
    "params_single, losses_single, time_single = train_model(\n",
    "    params_single, x_data_single, y_data_single,\n",
    "    learning_rate, batch_size, num_epochs, key\n",
    ")\n",
    "print(f\"Single-device training time: {time_single:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 8-way Data Parallel Training\n",
    "\n",
    "Now let's implement 8-way data parallel training where we'll shard the batch across 8 devices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an 8-device mesh for data parallelism\n",
    "devices = jax.devices()\n",
    "mesh = jax.make_mesh((8,), ('batch',))\n",
    "print(f\"Mesh shape: {mesh.shape}\")\n",
    "print(f\"Mesh axis names: {mesh.axis_names}\")\n",
    "\n",
    "# Create sharding specifications\n",
    "# Replicate parameters across all devices\n",
    "replicated_sharding = NamedSharding(mesh, P())\n",
    "# Shard data along the batch dimension\n",
    "batch_sharding = NamedSharding(mesh, P('batch'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define data-parallel training step\n",
    "@jax.jit\n",
    "def dp_train_step(params, x_batch, y_batch, learning_rate):\n",
    "    \"\"\"Data-parallel training step.\n",
    "    \n",
    "    This function will be executed on each device with its shard of the data.\n",
    "    JAX automatically handles gradient aggregation across devices.\n",
    "    \"\"\"\n",
    "    # Compute loss and gradients\n",
    "    loss_value, grads = jax.value_and_grad(mse_loss)(params, x_batch, y_batch)\n",
    "    \n",
    "    # Update parameters using gradient descent\n",
    "    new_params = [(w - learning_rate * dw, b - learning_rate * db)\n",
    "                 for (w, b), (dw, db) in zip(params, grads)]\n",
    "    \n",
    "    return new_params, loss_value\n",
    "\n",
    "# Place parameters and data with appropriate sharding\n",
    "params_dp = jax.device_put(params, replicated_sharding)\n",
    "\n",
    "# Ensure batch size is divisible by number of devices\n",
    "assert batch_size % 8 == 0, \"Batch size must be divisible by number of devices\"\n",
    "\n",
    "# Training loop\n",
    "print(\"\n",
    "Training with 8-way data parallelism...\")\n",
    "losses_dp = []\n",
    "start_time = time.time()\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # Shuffle data\n",
    "    key, subkey = jax.random.split(key)\n",
    "    perm = jax.random.permutation(subkey, num_samples)\n",
    "    x_shuffled = x_data[perm]\n",
    "    y_shuffled = y_data[perm]\n",
    "    \n",
    "    epoch_losses = []\n",
    "    steps_per_epoch = num_samples // batch_size\n",
    "    \n",
    "    for step in range(steps_per_epoch):\n",
    "        # Get batch\n",
    "        idx_start = step * batch_size\n",
    "        idx_end = idx_start + batch_size\n",
    "        x_batch = x_shuffled[idx_start:idx_end]\n",
    "        y_batch = y_shuffled[idx_start:idx_end]\n",
    "        \n",
    "        # Shard the batch across devices\n",
    "        x_batch = jax.device_put(x_batch, batch_sharding)\n",
    "        y_batch = jax.device_put(y_batch, batch_sharding)\n",
    "        \n",
    "        # Perform training step\n",
    "        params_dp, loss = dp_train_step(params_dp, x_batch, y_batch, learning_rate)\n",
    "        epoch_losses.append(float(loss))\n",
    "    \n",
    "    avg_loss = sum(epoch_losses) / len(epoch_losses)\n",
    "    losses_dp.append(avg_loss)\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {avg_loss:.6f}\")\n",
    "\n",
    "end_time = time.time()\n",
    "time_dp = end_time - start_time\n",
    "print(f\"\n",
    "Data-parallel training time: {time_dp:.2f} seconds\")\n",
    "print(f\"Speedup: {time_single / time_dp:.2f}x\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 Visualizing Data Sharding\n",
    "\n",
    "Let's visualize how the batch is sharded across devices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a small example batch to visualize sharding\n",
    "example_batch = jnp.arange(32).reshape(8, 4)  # 8 samples, 4 features\n",
    "sharded_batch = jax.device_put(example_batch, batch_sharding)\n",
    "\n",
    "print(\"Visualizing batch sharding across 8 devices:\")\n",
    "print(\"Original batch shape:\", example_batch.shape)\n",
    "print(\"\n",
    "Each device gets 1 sample:\")\n",
    "jax.debug.visualize_array_sharding(sharded_batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fully Sharded Data Parallelism (FSDP) Theory\n",
    "\n",
    "**Definition**: Activations, weights, and optimizer states are sharded along batch dimension. Weights are gathered just-in-time before use.\n",
    "\n",
    "**Mathematical representation**:\n",
    "$$\\text{In}[B_X, D] \\cdot_D W_{\\text{in}}[D_X, F] \\cdot_F W_{\\text{out}}[F, D_X] \\rightarrow \\text{Out}[B_X, D]$$\n",
    "\n",
    "where both batch and weight dimensions are sharded across $X$ devices.\n",
    "\n",
    "![FSDP](https://github.com/jax-ml/scaling-book/blob/main/assets/img/fsdp.png?raw=true)\n",
    "\n",
    "<sup> Image Source: [How To Scale Your Model](https://jax-ml.github.io/scaling-book) </sup>\n",
    "\n",
    "**Key properties**:\n",
    "- **Drastically reduces per-device memory usage**\n",
    "- Saves on backward pass FLOPs\n",
    "- Decomposes AllReduce into AllGather + ReduceScatter\n",
    "- Same communication cost as pure data parallelism\n",
    "- Also called \\\"ZeRO sharding\\\" (ZeRO-3 shards parameters, gradients, and optimizer states)\n",
    "\n",
    "**Algorithm**:\n",
    "\n",
    "**Forward pass:**\n",
    "1. W<sub>in</sub>[D, F] = **AllGather**(W<sub>in</sub>[D<sub>X</sub>, F])\n",
    "2. Tmp[B<sub>X</sub>, F] = In[B<sub>X</sub>, D] ×<sub>D</sub> W<sub>in</sub>[D, F]\n",
    "3. W<sub>out</sub>[F, D] = **AllGather**(W<sub>out</sub>[F, D<sub>X</sub>])\n",
    "4. Out[B<sub>X</sub>, D] = Tmp[B<sub>X</sub>, F] ×<sub>F</sub> W<sub>out</sub>[F, D]\n",
    "\n",
    "**Backward pass:**\n",
    "1. dW<sub>out</sub>[F, D<sub>X</sub>] = **ReduceScatter**(Tmp[B<sub>X</sub>, F] ×<sub>B</sub> dOut[B<sub>X</sub>, D])\n",
    "2. dW<sub>in</sub>[D<sub>X</sub>, F] = **ReduceScatter**(dTmp[B<sub>X</sub>, F] ×<sub>B</sub> In[B<sub>X</sub>, D])\n",
    "\n",
    "**Communication Analysis**:\n",
    "\n",
    "FSDP has the **same roofline as pure data parallelism** because:\n",
    "- AllReduce = AllGather + ReduceScatter\n",
    "- Total communication volume is identical\n",
    "- Same condition: $\\frac{B}{X} > \\frac{C}{W_{\\text{ici}}} = 2550$\n",
    "\n",
    "**Benefits**:\n",
    "- Memory reduction: Parameters and optimizer states sharded across devices\n",
    "- Zero overhead: Same FLOPs-to-communication ratio\n",
    "- Can upgrade from data parallelism without performance loss\n",
    "- Essential for models > 9B parameters\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Parallel Training with Flax NNX\n",
    "\n",
    "Now let's implement the same 8-way data parallel training using Flax NNX, which provides higher-level abstractions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Flax NNX\n",
    "try:\n",
    "    import flax.nnx as nnx\n",
    "    import optax\n",
    "except ImportError:\n",
    "    !pip install -q flax optax\n",
    "    import flax.nnx as nnx\n",
    "    import optax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create model and optimizer\n",
    "rngs = nnx.Rngs(0)\n",
    "model = MLP(layer_sizes, rngs=rngs)\n",
    "optimizer = nnx.Optimizer(model, optax.sgd(learning_rate))\n",
    "\n",
    "# Replicate model and optimizer state across devices\n",
    "state = nnx.state((model, optimizer))\n",
    "state = jax.device_put(state, replicated_sharding)\n",
    "nnx.update((model, optimizer), state)\n",
    "\n",
    "# Visualize model parameter sharding (should be replicated)\n",
    "print(\"Model parameter sharding (replicated across all devices):\")\n",
    "jax.debug.visualize_array_sharding(model.layers[0].kernel.value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the training step for Flax NNX\n",
    "@nnx.jit\n",
    "def nnx_dp_train_step(model: MLP, optimizer: nnx.Optimizer, x_batch, y_batch):\n",
    "    \"\"\"Data-parallel training step using Flax NNX.\"\"\"\n",
    "    def loss_fn(model):\n",
    "        predictions = model(x_batch)\n",
    "        return jnp.mean((predictions - y_batch) ** 2)\n",
    "    \n",
    "    loss_value, grads = nnx.value_and_grad(loss_fn)(model)\n",
    "    optimizer.update(grads)\n",
    "    return loss_value\n",
    "\n",
    "# Training loop with Flax NNX\n",
    "print(\"\n",
    "Training with Flax NNX 8-way data parallelism...\")\n",
    "losses_nnx_dp = []\n",
    "start_time = time.time()\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # Shuffle data\n",
    "    key, subkey = jax.random.split(key)\n",
    "    perm = jax.random.permutation(subkey, num_samples)\n",
    "    x_shuffled = x_data[perm]\n",
    "    y_shuffled = y_data[perm]\n",
    "    \n",
    "    epoch_losses = []\n",
    "    steps_per_epoch = num_samples // batch_size\n",
    "    \n",
    "    for step in range(steps_per_epoch):\n",
    "        # Get batch\n",
    "        idx_start = step * batch_size\n",
    "        idx_end = idx_start + batch_size\n",
    "        x_batch = x_shuffled[idx_start:idx_end]\n",
    "        y_batch = y_shuffled[idx_start:idx_end]\n",
    "        \n",
    "        # Shard the batch across devices\n",
    "        x_batch = jax.device_put(x_batch, batch_sharding)\n",
    "        y_batch = jax.device_put(y_batch, batch_sharding)\n",
    "        \n",
    "        # Visualize data sharding for first step\n",
    "        if epoch == 0 and step == 0:\n",
    "            print(\"\n",
    "Data sharding (batch distributed across devices):\")\n",
    "            jax.debug.visualize_array_sharding(x_batch)\n",
    "        \n",
    "        # Perform training step\n",
    "        loss = nnx_dp_train_step(model, optimizer, x_batch, y_batch)\n",
    "        epoch_losses.append(float(loss))\n",
    "    \n",
    "    avg_loss = sum(epoch_losses) / len(epoch_losses)\n",
    "    losses_nnx_dp.append(avg_loss)\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {avg_loss:.6f}\")\n",
    "\n",
    "end_time = time.time()\n",
    "time_nnx_dp = end_time - start_time\n",
    "print(f\"\n",
    "Flax NNX data-parallel training time: {time_nnx_dp:.2f} seconds\")\n",
    "print(f\"Speedup over single device: {time_single / time_nnx_dp:.2f}x\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Fully Sharded Data Parallel (FSDP) Training with Flax NNX\n",
    "\n",
    "Now let's implement FSDP where we shard both the data and model parameters across devices. This is especially useful for large models that don't fit on a single device."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Understanding FSDP Memory Benefits\n",
    "\n",
    "The diagram below shows how FSDP reduces memory usage compared to pure data parallelism:\n",
    "\n",
    "![FSDP Memory Comparison](https://github.com/jax-ml/scaling-book/blob/main/assets/img/fsdp-figure.png?raw=true)\n",
    "\n",
    "<sup> Image Source: [How To Scale Your Model](https://jax-ml.github.io/scaling-book) </sup>\n",
    "\n",
    "The rows show:\n",
    "1. **Pure Data Parallelism**: Parameters and optimizer states fully replicated\n",
    "2. **ZeRO-1**: Optimizer states sharded\n",
    "3. **ZeRO-2**: Optimizer states and gradients sharded  \n",
    "4. **ZeRO-3 (FSDP)**: Parameters, gradients, and optimizer states all sharded\n",
    "\n",
    "**Why FSDP matters**: Standard data parallelism involves significant duplicated work and memory. With FSDP:\n",
    "- Each device only stores 1/N of the parameters\n",
    "- Each device only updates 1/N of the optimizer state\n",
    "- AllGather parameters as needed for forward pass\n",
    "- ReduceScatter gradients for efficient updates\n",
    "\n",
    "This enables training much larger models that wouldn't fit with pure data parallelism.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a 2D mesh for FSDP\n",
    "# We'll use 2 devices for data parallelism and 4 for model parallelism\n",
    "fsdp_mesh = jax.sharding.Mesh(\n",
    "    mesh_utils.create_device_mesh((2, 4)),\n",
    "    ('data', 'model')\n",
    ")\n",
    "print(f\"FSDP mesh shape: {fsdp_mesh.shape}\")\n",
    "print(f\"FSDP mesh axis names: {fsdp_mesh.axis_names}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a custom MLP with explicit parameter sharding for FSDP\n",
    "import dataclasses\n",
    "from typing import Optional\n",
    "\n",
    "@dataclasses.dataclass(unsafe_hash=True)\n",
    "class MeshRules:\n",
    "    \"\"\"Rules for how to shard different parts of the model.\"\"\"\n",
    "    input_dim: Optional[str] = None\n",
    "    output_dim: Optional[str] = 'model'\n",
    "    bias: Optional[str] = 'model'\n",
    "    \n",
    "mesh_rules = MeshRules()\n",
    "\n",
    "class FSDP_MLP(nnx.Module):\n",
    "    \"\"\"MLP with explicit parameter sharding for FSDP.\"\"\"\n",
    "    \n",
    "    def __init__(self, layer_sizes, rngs: nnx.Rngs):\n",
    "        self.layer_sizes = layer_sizes\n",
    "        \n",
    "        # Create layers with sharded parameters\n",
    "        self.weights = []\n",
    "        self.biases = []\n",
    "        \n",
    "        for i in range(len(layer_sizes) - 1):\n",
    "            in_dim, out_dim = layer_sizes[i], layer_sizes[i + 1]\n",
    "            \n",
    "            # Initialize weight with sharding spec\n",
    "            w = nnx.Param(\n",
    "                nnx.initializers.lecun_normal()(rngs.params(), (in_dim, out_dim)),\n",
    "                sharding=(mesh_rules.input_dim, mesh_rules.output_dim)\n",
    "            )\n",
    "            \n",
    "            # Initialize bias with sharding spec\n",
    "            # For the last layer with output_dim=1, we don't shard\n",
    "            if out_dim == 1:\n",
    "                b = nnx.Param(\n",
    "                    jnp.zeros((out_dim,)),\n",
    "                    sharding=(None,)  # Don't shard single-element bias\n",
    "                )\n",
    "            else:\n",
    "                b = nnx.Param(\n",
    "                    jnp.zeros((out_dim,)),\n",
    "                    sharding=(mesh_rules.bias,)\n",
    "                )\n",
    "            \n",
    "            self.weights.append(w)\n",
    "            self.biases.append(b)\n",
    "    \n",
    "    def __call__(self, x):\n",
    "        activations = x\n",
    "        \n",
    "        # Apply each layer\n",
    "        for i in range(len(self.weights) - 1):\n",
    "            activations = jnp.dot(activations, self.weights[i]) + self.biases[i]\n",
    "            activations = jax.nn.relu(activations)\n",
    "        \n",
    "        # Last layer\n",
    "        return jnp.dot(activations, self.weights[-1]) + self.biases[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom SGD optimizer for FSDP (based on flax_nnx_dp_fsdp.py)\n",
    "class SGDState(nnx.Variable):\n",
    "    pass\n",
    "\n",
    "class SGD(nnx.Object):\n",
    "    def __init__(self, params: nnx.State, lr, decay=0.9):\n",
    "        def init_optimizer_state(variable: nnx.Variable):\n",
    "            return SGDState(\n",
    "                jnp.zeros_like(variable.value), **variable.get_metadata()\n",
    "            )\n",
    "        \n",
    "        self.lr = lr\n",
    "        self.params = params\n",
    "        self.momentum: nnx.State = jax.tree.map(init_optimizer_state, self.params)\n",
    "        self.decay = decay\n",
    "    \n",
    "    def update(self, grads: nnx.State):\n",
    "        def update_fn(params: nnx.Variable, momentum: SGDState, grad: nnx.VariableState):\n",
    "            # Momentum update\n",
    "            momentum.value = self.decay * momentum + (1 - self.decay) * grad.value\n",
    "            # Parameter update\n",
    "            params.value -= self.lr * momentum\n",
    "        \n",
    "        jax.tree.map(update_fn, self.params, self.momentum, grads)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mixed FSDP and Tensor Parallelism\n",
    "\n",
    "For even larger models or smaller batch sizes, we can combine FSDP with tensor parallelism using a 2D mesh:\n",
    "\n",
    "![Mixed FSDP and Tensor Parallelism](https://github.com/jax-ml/scaling-book/blob/main/assets/img/mixed-fsdp-model-parallelism.png?raw=true)\n",
    "\n",
    "<sup> Image Source: [How To Scale Your Model](https://jax-ml.github.io/scaling-book) </sup>\n",
    "\n",
    "**Mathematical representation**:\n",
    "$$\\text{In}[B_X, D_Y] \\cdot_D W_{\\text{in}}[D_X, F_Y] \\cdot_F W_{\\text{out}}[F_Y, D_X] \\rightarrow \\text{Out}[B_X, D_Y]$$\n",
    "\n",
    "where:\n",
    "- $X$ = data/FSDP parallelism dimension\n",
    "- $Y$ = model/tensor parallelism dimension\n",
    "\n",
    "**Key insight**: \n",
    "- **FSDP moves weights** (communication scales with weight size)\n",
    "- **Tensor parallelism moves activations** (communication scales with activation size)\n",
    "- As batch size shrinks, activations get smaller → tensor parallelism becomes cheaper\n",
    "- As we add more tensor parallelism, weight gathers get smaller → FSDP becomes cheaper\n",
    "\n",
    "**Optimal sharding**: For $N = X \\times Y$ total chips:\n",
    "$$X_{\\text{opt}} = \\sqrt{\\frac{B}{F} \\frac{M_X}{M_Y} N}$$\n",
    "\n",
    "where $M_X$ and $M_Y$ are the number of mesh axes for FSDP and tensor parallelism respectively.\n",
    "\n",
    "**Communication bound condition**:\n",
    "$$\\frac{B}{N} > \\frac{4\\alpha^2}{M_X M_Y F}$$\n",
    "\n",
    "where $\\alpha = C/W_{\\text{ici}} = 2550$ for TPUv5p.\n",
    "\n",
    "This allows batch sizes as low as ~400 tokens per chip, roughly 2× better than pure FSDP.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and initialize FSDP model\n",
    "@nnx.jit\n",
    "def create_fsdp_model():\n",
    "    # Use smaller layer sizes for FSDP demo to avoid memory issues\n",
    "    fsdp_layer_sizes = [input_dim, 1024, 512, 1]\n",
    "    model = FSDP_MLP(fsdp_layer_sizes, rngs=nnx.Rngs(0))\n",
    "    optimizer = SGD(nnx.variables(model, nnx.Param), learning_rate, decay=0.9)\n",
    "    \n",
    "    # Get sharding specifications for the state\n",
    "    state = nnx.state(optimizer)\n",
    "    \n",
    "    def get_named_shardings(path: tuple, value: nnx.VariableState):\n",
    "        if hasattr(value, 'sharding') and value.sharding is not None:\n",
    "            return value.replace(NamedSharding(fsdp_mesh, P(*value.sharding)))\n",
    "        return value\n",
    "    \n",
    "    named_shardings = nnx.map_state(get_named_shardings, state)\n",
    "    sharded_state = jax.lax.with_sharding_constraint(state, named_shardings)\n",
    "    nnx.update(optimizer, sharded_state)\n",
    "    \n",
    "    return model, optimizer\n",
    "\n",
    "print(\"Creating FSDP model...\")\n",
    "fsdp_model, fsdp_optimizer = create_fsdp_model()\n",
    "\n",
    "# Visualize parameter sharding\n",
    "print(\"\n",
    "Weight sharding (distributed across model axis):\")\n",
    "jax.debug.visualize_array_sharding(fsdp_model.weights[0].value)\n",
    "print(\"\n",
    "Momentum sharding:\")\n",
    "jax.debug.visualize_array_sharding(fsdp_optimizer.momentum['weights'][0].value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FSDP training step\n",
    "@nnx.jit\n",
    "def fsdp_train_step(model: FSDP_MLP, optimizer: SGD, x_batch, y_batch):\n",
    "    \"\"\"FSDP training step.\"\"\"\n",
    "    def loss_fn(model):\n",
    "        predictions = model(x_batch)\n",
    "        return jnp.mean((predictions - y_batch) ** 2)\n",
    "    \n",
    "    loss_value, grads = nnx.value_and_grad(loss_fn)(model)\n",
    "    optimizer.update(grads)\n",
    "    return loss_value\n",
    "\n",
    "# Training with FSDP\n",
    "print(\"\n",
    "Training with FSDP...\")\n",
    "losses_fsdp = []\n",
    "start_time = time.time()\n",
    "\n",
    "# Use smaller batch size for FSDP demo\n",
    "fsdp_batch_size = 256\n",
    "steps_per_epoch = num_samples // fsdp_batch_size\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # Shuffle data\n",
    "    key, subkey = jax.random.split(key)\n",
    "    perm = jax.random.permutation(subkey, num_samples)\n",
    "    x_shuffled = x_data[perm]\n",
    "    y_shuffled = y_data[perm]\n",
    "    \n",
    "    epoch_losses = []\n",
    "    \n",
    "    for step in range(steps_per_epoch):\n",
    "        # Get batch\n",
    "        idx_start = step * fsdp_batch_size\n",
    "        idx_end = idx_start + fsdp_batch_size\n",
    "        x_batch = x_shuffled[idx_start:idx_end]\n",
    "        y_batch = y_shuffled[idx_start:idx_end]\n",
    "        \n",
    "        # Shard data across the data axis only\n",
    "        data_sharding = NamedSharding(fsdp_mesh, P('data', None))\n",
    "        x_batch = jax.device_put(x_batch, data_sharding)\n",
    "        y_batch = jax.device_put(y_batch, data_sharding)\n",
    "        \n",
    "        # Perform training step\n",
    "        loss = fsdp_train_step(fsdp_model, fsdp_optimizer, x_batch, y_batch)\n",
    "        epoch_losses.append(float(loss))\n",
    "    \n",
    "    avg_loss = sum(epoch_losses) / len(epoch_losses)\n",
    "    losses_fsdp.append(avg_loss)\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {avg_loss:.6f}\")\n",
    "\n",
    "end_time = time.time()\n",
    "time_fsdp = end_time - start_time\n",
    "print(f\"\n",
    "FSDP training time: {time_fsdp:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Performance Comparison\n",
    "\n",
    "Let's compare the performance of all training approaches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training curves\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "# Plot loss curves\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(losses_single, label='Single Device', linewidth=2)\n",
    "plt.plot(losses_dp, label='JAX Data Parallel (8-way)', linewidth=2)\n",
    "plt.plot(losses_nnx_dp, label='Flax NNX Data Parallel (8-way)', linewidth=2)\n",
    "plt.plot(losses_fsdp, label='Flax NNX FSDP', linewidth=2)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training Loss Comparison')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot speedup comparison\n",
    "plt.subplot(1, 2, 2)\n",
    "methods = ['Single\n",
    "Device', 'JAX\n",
    "DP', 'Flax NNX\n",
    "DP', 'FSDP']\n",
    "times = [time_single, time_dp, time_nnx_dp, time_fsdp]\n",
    "speedups = [time_single / t for t in times]\n",
    "colors = ['gray', 'blue', 'green', 'red']\n",
    "\n",
    "bars = plt.bar(methods, speedups, color=colors, alpha=0.7)\n",
    "plt.axhline(y=1, color='black', linestyle='--', alpha=0.5)\n",
    "plt.ylabel('Speedup (relative to single device)')\n",
    "plt.title('Training Speedup Comparison')\n",
    "plt.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# Add speedup values on bars\n",
    "for bar, speedup in zip(bars, speedups):\n",
    "    height = bar.get_height()\n",
    "    plt.text(bar.get_x() + bar.get_width()/2., height + 0.1,\n",
    "             f'{speedup:.2f}x', ha='center', va='bottom')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print summary\n",
    "print(\"\n",
    "=== Performance Summary ===\")\n",
    "print(f\"Single Device: {time_single:.2f}s\")\n",
    "print(f\"JAX Data Parallel (8-way): {time_dp:.2f}s (speedup: {time_single/time_dp:.2f}x)\")\n",
    "print(f\"Flax NNX Data Parallel (8-way): {time_nnx_dp:.2f}s (speedup: {time_single/time_nnx_dp:.2f}x)\")\n",
    "print(f\"FSDP: {time_fsdp:.2f}s (speedup: {time_single/time_fsdp:.2f}x)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Communication vs Computation Analysis\n",
    "\n",
    "The plot below shows how different parallelism strategies perform as batch size changes:\n",
    "\n",
    "![Communication vs FLOPs Analysis](https://github.com/jax-ml/scaling-book/blob/main/assets/img/mixed-fsdp-comms-2.png?raw=true)\n",
    "\n",
    "<sup> Image Source: [How To Scale Your Model](https://jax-ml.github.io/scaling-book) </sup>\n",
    "\n",
    "Key observations:\n",
    "- **Pure Data Parallelism/FSDP** (green): Ratio scales linearly with batch size. Best for large batches.\n",
    "- **Tensor Parallelism** (blue): Fixed ratio independent of batch size. Limited by feed-forward dimension.\n",
    "- **Mixed FSDP + Tensor Parallelism** (red): Ratio scales with √B. Optimal for intermediate batch sizes.\n",
    "\n",
    "**The horizontal line at ratio = 1** marks the boundary between compute-bound (above) and communication-bound (below) regimes.\n",
    "\n",
    "**Practical implications**:\n",
    "- Large batch sizes (>850 per chip): Use pure FSDP\n",
    "- Intermediate batch sizes (400-850 per chip): Use mixed FSDP + tensor parallelism\n",
    "- Very small batch sizes: May become communication-bound regardless\n",
    "\n",
    "This analysis helps choose the right parallelism strategy based on your model size, batch size, and available hardware.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary: Parallelism Strategies Comparison\n",
    "\n",
    "Here's a comprehensive comparison of the parallelism strategies covered:\n",
    "\n",
    "| **Strategy** | **Mathematical Formula** | **Communication** | **Memory** | **Best Use Case** |\n",
    "|--------------|---------------------------|-------------------|------------|-------------------|\n",
    "| **Data Parallelism** | In[B<sub>X</sub>, D] ⋅ W[D, F] → Out[B<sub>X</sub>, D] | AllReduce gradients (backward only) | Parameters replicated | Model fits on single device |\n",
    "| **FSDP** | In[B<sub>X</sub>, D] ⋅ W[D<sub>X</sub>, F] → Out[B<sub>X</sub>, D] | AllGather weights + ReduceScatter grads | Parameters sharded | Large models, memory constraints |\n",
    "| **Tensor Parallelism** | In[B, D<sub>Y</sub>] ⋅ W[D, F<sub>Y</sub>] → Out[B, D<sub>Y</sub>] | AllGather activations + ReduceScatter | Weights sharded by FF dim | Small batch sizes |\n",
    "| **Mixed FSDP + TP** | In[B<sub>X</sub>, D<sub>Y</sub>] ⋅ W[D<sub>X</sub>, F<sub>Y</sub>] → Out[B<sub>X</sub>, D<sub>Y</sub>] | Both weight and activation movement | Both sharding strategies | Very large models, small batches |\n",
    "\n",
    "**Communication Bound Conditions** (for TPUv5p):\n",
    "- **Data Parallelism & FSDP**: Batch size per chip > 2,550\n",
    "- **Tensor Parallelism**: F > Y × 2,550 (typically 8-16 way max)\n",
    "- **Mixed FSDP + TP**: Batch size per chip > 400 (optimal combination)\n",
    "\n",
    "**Key Insight**: The choice of parallelism strategy depends on the interplay between:\n",
    "1. **Model size** (does it fit on one device?)\n",
    "2. **Batch size** (how much data per device?)\n",
    "3. **Hardware constraints** (memory, bandwidth, number of devices)\n",
    "\n",
    "For this tutorial's examples, we demonstrated the progression from simple data parallelism to more sophisticated FSDP approaches, showing how each strategy addresses different scaling challenges.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Key Takeaways\n",
    "\n",
    "In this tutorial, we've explored different approaches to parallel training:\n",
    "\n",
    "1. **8-way Data Parallel with Plain JAX**\n",
    "   - Simple to implement using `jax.jit` and sharding specifications\n",
    "   - Model is replicated across all devices\n",
    "   - Data is sharded along the batch dimension\n",
    "   - Good speedup for compute-intensive workloads\n",
    "\n",
    "2. **Data Parallel with Flax NNX**\n",
    "   - Higher-level API makes implementation cleaner\n",
    "   - Same performance as plain JAX implementation\n",
    "   - Better for complex models with many components\n",
    "\n",
    "3. **Fully Sharded Data Parallel (FSDP) with Flax NNX**\n",
    "   - Shards both model parameters and data\n",
    "   - Uses a 2D mesh (data × model axes)\n",
    "   - Essential for large models that don't fit on single device\n",
    "   - More complex but enables training of massive models\n",
    "\n",
    "### When to Use Each Approach:\n",
    "\n",
    "- **Data Parallel**: When your model fits on a single device and you want to scale training speed\n",
    "- **FSDP**: When your model is too large for a single device or you need maximum memory efficiency\n",
    "\n",
    "### References:\n",
    "- Example implementations: `jax_data_parallel.py` and `flax_nnx_dp_fsdp.py` in the root directory\n",
    "- JAX documentation on [distributed arrays and automatic parallelization](https://jax.readthedocs.io/en/latest/jax-101/08-pjit.html)\n",
    "- Flax NNX documentation on [distributed training](https://flax.readthedocs.io/en/latest/nnx/index.html)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
