{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Parallel and Fully Sharded Data Parallel Training\n",
    "\n",
    "In the previous tutorial, we explored the basics of JAX parallelization, including device meshes, sharded matrices, and collective operations. In this tutorial, we'll build on those concepts to implement data parallel (DP) and fully sharded data parallel (FSDP) training for neural networks.\n",
    "\n",
    "We'll cover:\n",
    "1. Data parallel training with plain JAX\n",
    "2. Fully sharded data parallel training with plain JAX\n",
    "3. Using Flax NNX API for data parallel training\n",
    "4. Using Flax NNX API for FSDP training\n",
    "\n",
    "By the end of this tutorial, you'll understand how to scale neural network training using parallelization techniques and how to leverage Flax's high-level abstractions for efficient distributed training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "Let's start by importing the necessary libraries and initializing our environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from jax.sharding import Mesh, PartitionSpec as P\n",
    "from jax.experimental import mesh_utils\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "from functools import partial\n",
    "from typing import Dict, Tuple, Any, List, Callable\n",
    "\n",
    "# For visualizing sharding\n",
    "from jax.experimental.shard_map import shard_map\n",
    "from jax.experimental.pjit import pjit\n",
    "\n",
    "# Check available devices\n",
    "print(f\"JAX version: {jax.__version__}\")\n",
    "print(f\"Available devices: {jax.devices()}\")\n",
    "print(f\"Number of devices: {jax.device_count()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data Parallel Training with Plain JAX\n",
    "\n",
    "Data parallelism is a strategy where we replicate the model across multiple devices and shard the data batch. Each device processes a portion of the batch using its copy of the model, and then we aggregate the gradients across all devices.\n",
    "\n",
    "Let's implement a simple neural network and train it using data parallelism in JAX."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Define a Simple Neural Network\n",
    "\n",
    "First, we'll define a simple Multi-Layer Perceptron (MLP) with two hidden layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_mlp_params(layer_sizes, key):\n",
    "    \"\"\"Initialize parameters for a simple MLP.\"\"\"\n",
    "    keys = jax.random.split(key, len(layer_sizes))\n",
    "    params = []\n",
    "    \n",
    "    for i in range(len(layer_sizes) - 1):\n",
    "        in_dim, out_dim = layer_sizes[i], layer_sizes[i + 1]\n",
    "        w_key, b_key = jax.random.split(keys[i])\n",
    "        \n",
    "        # Initialize weights with scaled normal distribution\n",
    "        w = jax.random.normal(w_key, (in_dim, out_dim)) * jnp.sqrt(2.0 / in_dim)\n",
    "        b = jnp.zeros((out_dim,))\n",
    "        \n",
    "        params.append((w, b))\n",
    "    \n",
    "    return params\n",
    "\n",
    "def mlp_forward(params, x):\n",
    "    \"\"\"Forward pass through the MLP.\"\"\"\n",
    "    activations = x\n",
    "    \n",
    "    # Apply each layer with ReLU activation except the last layer\n",
    "    for i, (w, b) in enumerate(params[:-1]):\n",
    "        activations = jnp.dot(activations, w) + b\n",
    "        activations = jax.nn.relu(activations)\n",
    "    \n",
    "    # Last layer without activation (for regression)\n",
    "    w, b = params[-1]\n",
    "    output = jnp.dot(activations, w) + b\n",
    "    \n",
    "    return output\n",
    "\n",
    "def mse_loss(params, x_batch, y_batch):\n",
    "    \"\"\"Mean squared error loss function.\"\"\"\n",
    "    predictions = mlp_forward(params, x_batch)\n",
    "    return jnp.mean((predictions - y_batch) ** 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Generate Synthetic Data\n",
    "\n",
    "Let's create a synthetic dataset to train our model. We'll use a simple quadratic function with some noise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_data(num_samples, input_dim, key):\n",
    "    \"\"\"Generate synthetic data from a quadratic function with noise.\"\"\"\n",
    "    x_key, noise_key = jax.random.split(key)\n",
    "    \n",
    "    # Generate random input features\n",
    "    x = jax.random.normal(x_key, (num_samples, input_dim))\n",
    "    \n",
    "    # Generate target values: sum of squares with noise\n",
    "    y_clean = jnp.sum(x ** 2, axis=1, keepdims=True)\n",
    "    noise = 0.1 * jax.random.normal(noise_key, (num_samples, 1))\n",
    "    y = y_clean + noise\n",
    "    \n",
    "    return x, y\n",
    "\n",
    "# Generate training data\n",
    "seed = 42\n",
    "key = jax.random.PRNGKey(seed)\n",
    "num_samples = 10000\n",
    "input_dim = 10\n",
    "x_data, y_data = generate_data(num_samples, input_dim, key)\n",
    "\n",
    "# Display a few examples\n",
    "for i in range(5):\n",
    "    print(f\"x[{i}] = {x_data[i][:3]}...  =>  y[{i}] = {y_data[i][0]:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Single-Device Training\n",
    "\n",
    "First, let's implement and benchmark training on a single device to establish a baseline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step(params, x_batch, y_batch, learning_rate):\n",
    "    \"\"\"Perform a single training step using gradient descent.\"\"\"\n",
    "    # Compute loss and gradients\n",
    "    loss_value, grads = jax.value_and_grad(mse_loss)(params, x_batch, y_batch)\n",
    "    \n",
    "    # Update parameters using gradient descent\n",
    "    new_params = [(w - learning_rate * dw, b - learning_rate * db)\n",
    "                 for (w, b), (dw, db) in zip(params, grads)]\n",
    "    \n",
    "    return new_params, loss_value\n",
    "\n",
    "# JIT-compile the training step for better performance\n",
    "train_step_jit = jax.jit(train_step)\n",
    "\n",
    "# Initialize model parameters\n",
    "layer_sizes = [input_dim, 32, 16, 1]  # [input_dim, hidden1, hidden2, output_dim]\n",
    "init_key = jax.random.PRNGKey(123)\n",
    "params = init_mlp_params(layer_sizes, init_key)\n",
    "\n",
    "# Training hyperparameters\n",
    "learning_rate = 0.01\n",
    "batch_size = 128\n",
    "num_epochs = 5\n",
    "steps_per_epoch = num_samples // batch_size\n",
    "\n",
    "# Training loop\n",
    "losses = []\n",
    "start_time = time.time()\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # Shuffle data at the beginning of each epoch\n",
    "    key, subkey = jax.random.split(key)\n",
    "    perm = jax.random.permutation(subkey, num_samples)\n",
    "    x_shuffled = x_data[perm]\n",
    "    y_shuffled = y_data[perm]\n",
    "    \n",
    "    epoch_losses = []\n",
    "    \n",
    "    for step in range(steps_per_epoch):\n",
    "        # Get batch\n",
    "        idx_start = step * batch_size\n",
    "        idx_end = idx_start + batch_size\n",
    "        x_batch = x_shuffled[idx_start:idx_end]\n",
    "        y_batch = y_shuffled[idx_start:idx_end]\n",
    "        \n",
    "        # Perform training step\n",
    "        params, loss = train_step_jit(params, x_batch, y_batch, learning_rate)\n",
    "        epoch_losses.append(loss)\n",
    "    \n",
    "    # Compute average loss for this epoch\n",
    "    avg_loss = sum(epoch_losses) / len(epoch_losses)\n",
    "    losses.append(avg_loss)\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {avg_loss:.6f}\")\n",
    "\n",
    "end_time = time.time()\n",
    "single_device_time = end_time - start_time\n",
    "print(f\"Single-device training time: {single_device_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 Data Parallel Training\n",
    "\n",
    "Now let's implement data parallel training where we'll shard the batch across multiple devices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a device mesh for data parallelism\n",
    "devices = jax.devices()\n",
    "num_devices = len(devices)\n",
    "device_mesh = mesh_utils.create_device_mesh((num_devices,))\n",
    "print(f\"Device mesh shape: {device_mesh.shape}\")\n",
    "\n",
    "# Define partition specs for data-parallel training\n",
    "# We'll shard the batch dimension (first dimension) across devices\n",
    "batch_dim = 'batch'\n",
    "\n",
    "def data_parallel_train_step(params, x_batch, y_batch, learning_rate):\n",
    "    \"\"\"Training step function for data parallel training.\"\"\"\n",
    "    # Compute per-device gradients\n",
    "    def loss_fn(p):\n",
    "        return mse_loss(p, x_batch, y_batch)\n",
    "    \n",
    "    loss_value, grads = jax.value_and_grad(loss_fn)(params)\n",
    "    \n",
    "    # Update parameters using gradient descent\n",
    "    new_params = [(w - learning_rate * dw, b - learning_rate * db)\n",
    "                 for (w, b), (dw, db) in zip(params, grads)]\n",
    "    \n",
    "    return new_params, loss_value\n",
    "\n",
    "# Set up pjit for data-parallel training\n",
    "# We'll shard the batch dimension of inputs and replicate the parameters\n",
    "dp_train_step = pjit(\n",
    "    data_parallel_train_step,\n",
    "    in_shardings=(None, P(batch_dim), P(batch_dim), None),  # Replicate params, shard data\n",
    "    out_shardings=(None, None)  # Replicate outputs\n",
    ")\n",
    "\n",
    "# Adjust batch size to be a multiple of the number of devices\n",
    "global_batch_size = batch_size * num_devices\n",
    "dp_steps_per_epoch = num_samples // global_batch_size\n",
    "\n",
    "# Initialize parameters (same as before)\n",
    "dp_params = init_mlp_params(layer_sizes, init_key)\n",
    "\n",
    "# Training loop with data parallelism\n",
    "dp_losses = []\n",
    "start_time = time.time()\n",
    "\n",
    "with Mesh(device_mesh, (batch_dim,)):\n",
    "    for epoch in range(num_epochs):\n",
    "        # Shuffle data at the beginning of each epoch\n",
    "        key, subkey = jax.random.split(key)\n",
    "        perm = jax.random.permutation(subkey, num_samples)\n",
    "        x_shuffled = x_data[perm]\n",
    "        y_shuffled = y_data[perm]\n",
    "        \n",
    "        epoch_losses = []\n",
    "        \n",
    "        for step in range(dp_steps_per_epoch):\n",
    "            # Get global batch\n",
    "            idx_start = step * global_batch_size\n",
    "            idx_end = idx_start + global_batch_size\n",
    "            x_batch = x_shuffled[idx_start:idx_end]\n",
    "            y_batch = y_shuffled[idx_start:idx_end]\n",
    "            \n",
    "            # Perform data-parallel training step\n",
    "            dp_params, loss = dp_train_step(dp_params, x_batch, y_batch, learning_rate)\n",
    "            epoch_losses.append(loss)\n",
    "        \n",
    "        # Compute average loss for this epoch\n",
    "        avg_loss = sum(epoch_losses) / len(epoch_losses)\n",
    "        dp_losses.append(avg_loss)\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {avg_loss:.6f}\")\n",
    "\n",
    "end_time = time.time()\n",
    "dp_time = end_time - start_time\n",
    "print(f\"Data-parallel training time: {dp_time:.2f} seconds\")\n",
    "print(f\"Speedup: {single_device_time / dp_time:.2f}x\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.5 Visualizing the Data Parallelism Strategy\n",
    "\n",
    "Let's visualize how the data is sharded across devices in the data parallel training setup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_device_placement(mesh_shape, batch_shape):\n",
    "    \"\"\"Visualize how a batch is distributed across devices in data parallelism.\"\"\"\n",
    "    num_devices = np.prod(mesh_shape)\n",
    "    batch_size = batch_shape[0]\n",
    "    samples_per_device = batch_size // num_devices\n",
    "    \n",
    "    # Create a grid to represent the devices\n",
    "    fig, ax = plt.subplots(figsize=(10, 6))\n",
    "    \n",
    "    # Draw the global batch\n",
    "    ax.add_patch(plt.Rectangle((0, 0), 10, 2, fill=False, edgecolor='black', linewidth=2))\n",
    "    ax.text(5, 1, f\"Global Batch (size={batch_size})\", ha='center', va='center', fontsize=12)\n",
    "    \n",
    "    # Draw device allocations\n",
    "    for i in range(num_devices):\n",
    "        ax.add_patch(plt.Rectangle((i * (10/num_devices), 3), 10/num_devices, 1.5, \n",
    "                                   fill=True, edgecolor='black', \n",
    "                                   facecolor=plt.cm.tab10(i % 10),\n",
    "                                   alpha=0.7, linewidth=1))\n",
    "        ax.text(i * (10/num_devices) + (5/num_devices), 3.75, \n",
    "                f\"Device {i}\\n{samples_per_device} samples\", \n",
    "                ha='center', va='center', fontsize=10)\n",
    "        \n",
    "        # Connect with arrows\n",
    "        ax.arrow(i * (10/num_devices) + (5/num_devices), 3, 0, -0.5, \n",
    "                 head_width=0.2, head_length=0.2, fc='black', ec='black')\n",
    "    \n",
    "    # Add model replication indicators\n",
    "    for i in range(num_devices):\n",
    "        ax.add_patch(plt.Rectangle((i * (10/num_devices), 5), 10/num_devices, 1.5, \n",
    "                                   fill=True, edgecolor='black', \n",
    "                                   facecolor='lightgreen',\n",
    "                                   alpha=0.7, linewidth=1))\n",
    "        ax.text(i * (10/num_devices) + (5/num_devices), 5.75, \n",
    "                f\"Model Copy\\non Device {i}\", \n",
    "                ha='center', va='center', fontsize=10)\n",
    "    \n",
    "    # Add title and labels\n",
    "    ax.set_title(\"Data Parallel Training: Batch Sharding with Model Replication\", fontsize=14)\n",
    "    ax.set_xlim(-0.5, 10.5)\n",
    "    ax.set_ylim(-0.5, 7)\n",
    "    ax.set_axis_off()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Visualize data parallelism for our setup\n",
    "visualize_device_placement((num_devices,), (global_batch_size, input_dim))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Fully Sharded Data Parallel (FSDP) Training with Plain JAX\n",
    "\n",
    "Now, let's implement Fully Sharded Data Parallel (FSDP) training. In FSDP, we shard both the data and the model parameters across devices. This approach helps scale to larger models that might not fit in a single device's memory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Setting Up a 2D Mesh for FSDP\n",
    "\n",
    "For FSDP, we'll use a 2D mesh to shard data along one dimension and model parameters along another."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try to create a 2D mesh if possible, otherwise use a 1D mesh\n",
    "if num_devices >= 4:\n",
    "    # Create a 2D mesh (e.g., 2x4 for 8 devices)\n",
    "    mesh_shape = (2, num_devices // 2)\n",
    "else:\n",
    "    # Fall back to a 1D mesh for fewer devices\n",
    "    mesh_shape = (1, num_devices)\n",
    "\n",
    "fsdp_device_mesh = mesh_utils.create_device_mesh(mesh_shape)\n",
    "print(f\"FSDP device mesh shape: {fsdp_device_mesh.shape}\")\n",
    "\n",
    "# Define mesh axes names\n",
    "data_axis = 'data'  # For sharding data\n",
    "model_axis = 'model'  # For sharding model parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Implementing FSDP Training\n",
    "\n",
    "We'll modify our model representation to support parameter sharding and implement the FSDP training loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fsdp_train_step(params, x_batch, y_batch, learning_rate):\n",
    "    \"\"\"Training step function for FSDP training.\"\"\"\n",
    "    # Compute loss and gradients\n",
    "    loss_value, grads = jax.value_and_grad(mse_loss)(params, x_batch, y_batch)\n",
    "    \n",
    "    # Update parameters using gradient descent\n",
    "    new_params = [(w - learning_rate * dw, b - learning_rate * db)\n",
    "                 for (w, b), (dw, db) in zip(params, grads)]\n",
    "    \n",
    "    return new_params, loss_value\n",
    "\n",
    "# Define parameter sharding strategy for each layer\n",
    "def get_param_sharding_specs(layer_sizes):\n",
    "    \"\"\"Create parameter partition specs for FSDP.\"\"\"\n",
    "    param_specs = []\n",
    "    \n",
    "    for i in range(len(layer_sizes) - 1):\n",
    "        # Weights are sharded along the model axis (second dimension)\n",
    "        # For larger models, you could shard both dimensions\n",
    "        w_spec = P(None, model_axis)  # Shard columns (output dim)\n",
    "        b_spec = P(model_axis)  # Shard bias\n",
    "        \n",
    "        param_specs.append((w_spec, b_spec))\n",
    "    \n",
    "    return param_specs\n",
    "\n",
    "# Get parameter sharding specs\n",
    "param_specs = get_param_sharding_specs(layer_sizes)\n",
    "\n",
    "# Set up pjit for FSDP training\n",
    "# Shard batch across data axis and parameters according to param_specs\n",
    "fsdp_train_step_pjit = pjit(\n",
    "    fsdp_train_step,\n",
    "    in_shardings=(param_specs, P(data_axis, None), P(data_axis, None), None),\n",
    "    out_shardings=(param_specs, None)\n",
    ")\n",
    "\n",
    "# Initialize parameters for FSDP\n",
    "fsdp_params = init_mlp_params(layer_sizes, init_key)\n",
    "\n",
    "# Training loop with FSDP\n",
    "fsdp_losses = []\n",
    "start_time = time.time()\n",
    "\n",
    "with Mesh(fsdp_device_mesh, (data_axis, model_axis)):\n",
    "    for epoch in range(num_epochs):\n",
    "        # Shuffle data at the beginning of each epoch\n",
    "        key, subkey = jax.random.split(key)\n",
    "        perm = jax.random.permutation(subkey, num_samples)\n",
    "        x_shuffled = x_data[perm]\n",
    "        y_shuffled = y_data[perm]\n",
    "        \n",
    "        epoch_losses = []\n",
    "        \n",
    "        for step in range(dp_steps_per_epoch):  # Use same steps as DP for fair comparison\n",
    "            # Get batch\n",
    "            idx_start = step * global_batch_size\n",
    "            idx_end = idx_start + global_batch_size\n",
    "            x_batch = x_shuffled[idx_start:idx_end]\n",
    "            y_batch = y_shuffled[idx_start:idx_end]\n",
    "            \n",
    "            # Perform FSDP training step\n",
    "            fsdp_params, loss = fsdp_train_step_pjit(fsdp_params, x_batch, y_batch, learning_rate)\n",
    "            epoch_losses.append(loss)\n",
    "        \n",
    "        # Compute average loss for this epoch\n",
    "        avg_loss = sum(epoch_losses) / len(epoch_losses)\n",
    "        fsdp_losses.append(avg_loss)\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {avg_loss:.6f}\")\n",
    "\n",
    "end_time = time.time()\n",
    "fsdp_time = end_time - start_time\n",
    "print(f\"FSDP training time: {fsdp_time:.2f} seconds\")\n",
    "print(f\"Speedup over single device: {single_device_time / fsdp_time:.2f}x\")\n",
    "print(f\"Speedup over data parallelism: {dp_time / fsdp_time:.2f}x\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Visualizing FSDP Strategy\n",
    "\n",
    "Let's visualize how both data and model parameters are sharded in the FSDP approach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_fsdp(mesh_shape):\n",
    "    \"\"\"Visualize both data and model sharding in FSDP.\"\"\"\n",
    "    data_dim, model_dim = mesh_shape\n",
    "    num_devices = data_dim * model_dim\n",
    "    \n",
    "    # Create a grid to represent the devices\n",
    "    fig, ax = plt.subplots(figsize=(12, 8))\n",
    "    \n",
    "    # Draw the 2D mesh grid\n",
    "    for i in range(data_dim + 1):\n",
    "        ax.axhline(y=i, color='black', linestyle='-', linewidth=1)\n",
    "    for j in range(model_dim + 1):\n",
    "        ax.axvline(x=j, color='black', linestyle='-', linewidth=1)\n",
    "    \n",
    "    # Label each device in the mesh\n",
    "    for i in range(data_dim):\n",
    "        for j in range(model_dim):\n",
    "            device_id = i * model_dim + j\n",
    "            ax.text(j + 0.5, i + 0.5, f\"Device {device_id}\", \n",
    "                    ha='center', va='center', fontsize=12)\n",
    "    \n",
    "    # Add axis labels\n",
    "    ax.text(model_dim / 2, -0.5, \"Model Sharding Dimension\", ha='center', fontsize=14)\n",
    "    ax.text(-0.5, data_dim / 2, \"Data Sharding\\nDimension\", va='center', fontsize=14, rotation=90)\n",
    "    \n",
    "    # Draw data sharding illustration\n",
    "    ax.add_patch(plt.Rectangle((-3, 0), 2, data_dim, fill=True, \n",
    "                               edgecolor='black', facecolor='lightblue', alpha=0.7))\n",
    "    for i in range(data_dim):\n",
    "        ax.text(-2, i + 0.5, f\"Data Shard {i}\", ha='center', va='center', fontsize=10)\n",
    "        ax.arrow(-1.5, i + 0.5, 1, 0, head_width=0.2, head_length=0.2, fc='black', ec='black')\n",
    "    \n",
    "    # Draw model sharding illustration\n",
    "    ax.add_patch(plt.Rectangle((0, -3), model_dim, 2, fill=True, \n",
    "                               edgecolor='black', facecolor='lightgreen', alpha=0.7))\n",
    "    for j in range(model_dim):\n",
    "        ax.text(j + 0.5, -2, f\"Model\\nShard {j}\", ha='center', va='center', fontsize=10)\n",
    "        ax.arrow(j + 0.5, -1.5, 0, 1, head_width=0.2, head_length=0.2, fc='black', ec='black')\n",
    "    \n",
    "    # Set title\n",
    "    ax.set_title(\"Fully Sharded Data Parallel (FSDP) Strategy\", fontsize=16)\n",
    "    \n",
    "    # Set limits and turn off axis\n",
    "    ax.set_xlim(-4, model_dim + 1)\n",
    "    ax.set_ylim(-4, data_dim + 1)\n",
    "    ax.set_aspect('equal')\n",
    "    ax.axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Visualize the FSDP strategy\n",
    "visualize_fsdp(mesh_shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data Parallel Training with Flax NNX\n",
    "\n",
    "Now let's see how we can use Flax NNX to simplify distributed training. Flax provides high-level abstractions that make it easier to define and train neural networks in a distributed setting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install Flax NNX if not already installed\n",
    "try:\n",
    "    import flax.nnx as nnx\n",
    "except ImportError:\n",
    "    !pip install -q flax\n",
    "    import flax.nnx as nnx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Define a Flax MLP Model\n",
    "\n",
    "Let's define our MLP using Flax NNX module system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nnx.Module):\n",
    "    \"\"\"A simple multi-layer perceptron using Flax NNX.\"\"\"\n",
    "    \n",
    "    def __init__(self, layer_sizes, *, key):\n",
    "        super().__init__()\n",
    "        self.layer_sizes = layer_sizes\n",
    "        \n",
    "        # Create layers\n",
    "        self.layers = []\n",
    "        for i in range(len(layer_sizes) - 1):\n",
    "            in_dim, out_dim = layer_sizes[i], layer_sizes[i + 1]\n",
    "            k = jax.random.fold_in(key, i)\n",
    "            dense = nnx.Linear(in_dim, out_dim, key=k)\n",
    "            self.layers.append(dense)\n",
    "    \n",
    "    def __call__(self, x):\n",
    "        \"\"\"Forward pass through the MLP.\"\"\"\n",
    "        activations = x\n",
    "        \n",
    "        # Apply each layer with ReLU activation except the last layer\n",
    "        for i, layer in enumerate(self.layers[:-1]):\n",
    "            activations = layer(activations)\n",
    "            activations = jax.nn.relu(activations)\n",
    "        \n",
    "        # Last layer without activation (for regression)\n",
    "        return self.layers[-1](activations)\n",
    "\n",
    "# Define a simple SGD optimizer\n",
    "class SGD(nnx.Optimizer):\n",
    "    \"\"\"Simple SGD optimizer for Flax NNX.\"\"\"\n",
    "    \n",
    "    def __init__(self, learning_rate=0.01):\n",
    "        super().__init__()\n",
    "        self.learning_rate = learning_rate\n",
    "    \n",
    "    def update(self, gradients, state):\n",
    "        return state - self.learning_rate * gradients\n",
    "\n",
    "# Define MSE loss function\n",
    "def flax_mse_loss(model, x_batch, y_batch):\n",
    "    \"\"\"Mean squared error loss function for Flax model.\"\"\"\n",
    "    predictions = model(x_batch)\n",
    "    return jnp.mean((predictions - y_batch) ** 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Data Parallel Training with Flax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the model\n",
    "init_key = jax.random.PRNGKey(123)\n",
    "flax_model = MLP(layer_sizes, key=init_key)\n",
    "\n",
    "# Create optimizer\n",
    "optimizer = SGD(learning_rate=learning_rate)\n",
    "opt_state = optimizer.init(flax_model)\n",
    "\n",
    "# Define data-parallel training step\n",
    "def flax_dp_train_step(model, opt_state, x_batch, y_batch):\n",
    "    \"\"\"Data-parallel training step using Flax NNX.\"\"\"\n",
    "    # Compute loss and gradients\n",
    "    loss_value, gradients = jax.value_and_grad(flax_mse_loss)(model, x_batch, y_batch)\n",
    "    \n",
    "    # Update model parameters\n",
    "    opt_state = optimizer.update(gradients, opt_state)\n",
    "    \n",
    "    return opt_state, loss_value\n",
    "\n",
    "# JIT-compile the training step with data-parallel spec\n",
    "flax_dp_train_step_jit = pjit(\n",
    "    flax_dp_train_step,\n",
    "    in_shardings=(None, None, P(batch_dim), P(batch_dim)),  # Replicate model, shard data\n",
    "    out_shardings=(None, None)  # Replicate outputs\n",
    ")\n",
    "\n",
    "# Training loop with Flax data parallelism\n",
    "flax_dp_losses = []\n",
    "start_time = time.time()\n",
    "\n",
    "with Mesh(device_mesh, (batch_dim,)):\n",
    "    for epoch in range(num_epochs):\n",
    "        # Shuffle data at the beginning of each epoch\n",
    "        key, subkey = jax.random.split(key)\n",
    "        perm = jax.random.permutation(subkey, num_samples)\n",
    "        x_shuffled = x_data[perm]\n",
    "        y_shuffled = y_data[perm]\n",
    "        \n",
    "        epoch_losses = []\n",
    "        \n",
    "        for step in range(dp_steps_per_epoch):\n",
    "            # Get global batch\n",
    "            idx_start = step * global_batch_size\n",
    "            idx_end = idx_start + global_batch_size\n",
    "            x_batch = x_shuffled[idx_start:idx_end]\n",
    "            y_batch = y_shuffled[idx_start:idx_end]\n",
    "            \n",
    "            # Perform data-parallel training step\n",
    "            opt_state, loss = flax_dp_train_step_jit(flax_model, opt_state, x_batch, y_batch)\n",
    "            epoch_losses.append(loss)\n",
    "        \n",
    "        # Compute average loss for this epoch\n",
    "        avg_loss = sum(epoch_losses) / len(epoch_losses)\n",
    "        flax_dp_losses.append(avg_loss)\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {avg_loss:.6f}\")\n",
    "\n",
    "end_time = time.time()\n",
    "flax_dp_time = end_time - start_time\n",
    "print(f\"Flax data-parallel training time: {flax_dp_time:.2f} seconds\")\n",
    "print(f\"Speedup over single device: {single_device_time / flax_dp_time:.2f}x\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Fully Sharded Data Parallel Training with Flax NNX\n",
    "\n",
    "Finally, let's implement FSDP using Flax NNX."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Defining FSDP Training with Flax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the model for FSDP\n",
    "flax_fsdp_model = MLP(layer_sizes, key=init_key)\n",
    "fsdp_opt_state = optimizer.init(flax_fsdp_model)\n",
    "\n",
    "# Define FSDP training step\n",
    "def flax_fsdp_train_step(model, opt_state, x_batch, y_batch):\n",
    "    \"\"\"FSDP training step using Flax NNX.\"\"\"\n",
    "    # Compute loss and gradients\n",
    "    loss_value, gradients = jax.value_and_grad(flax_mse_loss)(model, x_batch, y_batch)\n",
    "    \n",
    "    # Update model parameters\n",
    "    opt_state = optimizer.update(gradients, opt_state)\n",
    "    \n",
    "    return opt_state, loss_value\n",
    "\n",
    "# Create model parameter sharding rules for Flax module\n",
    "def get_flax_module_sharding_rules(model):\n",
    "    \"\"\"Create sharding rules for Flax NNX module parameters.\"\"\"\n",
    "    rules = {}\n",
    "    \n",
    "    # For each layer, shard weights and biases\n",
    "    for i, layer in enumerate(model.layers):\n",
    "        # Shard weights along the output dimension (second axis)\n",
    "        rules[f\"layers/{i}/kernel\"] = P(None, model_axis)\n",
    "        # Shard bias along its only dimension\n",
    "        rules[f\"layers/{i}/bias\"] = P(model_axis)\n",
    "    \n",
    "    return rules\n",
    "\n",
    "# Get sharding rules for the model\n",
    "model_rules = get_flax_module_sharding_rules(flax_fsdp_model)\n",
    "\n",
    "# Set up FSDP pjit with module sharding rules\n",
    "flax_fsdp_train_step_jit = pjit(\n",
    "    flax_fsdp_train_step,\n",
    "    in_shardings=(model_rules, model_rules, P(data_axis, None), P(data_axis, None)),\n",
    "    out_shardings=(model_rules, None)\n",
    ")\n",
    "\n",
    "# Training loop with Flax FSDP\n",
    "flax_fsdp_losses = []\n",
    "start_time = time.time()\n",
    "\n",
    "with Mesh(fsdp_device_mesh, (data_axis, model_axis)):\n",
    "    for epoch in range(num_epochs):\n",
    "        # Shuffle data at the beginning of each epoch\n",
    "        key, subkey = jax.random.split(key)\n",
    "        perm = jax.random.permutation(subkey, num_samples)\n",
    "        x_shuffled = x_data[perm]\n",
    "        y_shuffled = y_data[perm]\n",
    "        \n",
    "        epoch_losses = []\n",
    "        \n",
    "        for step in range(dp_steps_per_epoch):\n",
    "            # Get batch\n",
    "            idx_start = step * global_batch_size\n",
    "            idx_end = idx_start + global_batch_size\n",
    "            x_batch = x_shuffled[idx_start:idx_end]\n",
    "            y_batch = y_shuffled[idx_start:idx_end]\n",
    "            \n",
    "            # Perform FSDP training step\n",
    "            fsdp_opt_state, loss = flax_fsdp_train_step_jit(\n",
    "                flax_fsdp_model, fsdp_opt_state, x_batch, y_batch)\n",
    "            epoch_losses.append(loss)\n",
    "        \n",
    "        # Compute average loss for this epoch\n",
    "        avg_loss = sum(epoch_losses) / len(epoch_losses)\n",
    "        flax_fsdp_losses.append(avg_loss)\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {avg_loss:.6f}\")\n",
    "\n",
    "end_time = time.time()\n",
    "flax_fsdp_time = end_time - start_time\n",
    "print(f\"Flax FSDP training time: {flax_fsdp_time:.2f} seconds\")\n",
    "print(f\"Speedup over single device: {single_device_time / flax_fsdp_time:.2f}x\")\n",
    "print(f\"Speedup over Flax data parallelism: {flax_dp_time / flax_fsdp_time:.2f}x\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Performance Comparison\n",
    "\n",
    "Let's compare the performance of all training approaches we've implemented."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect timing results\n",
    "timings = {\n",
    "    'Single Device': single_device_time,\n",
    "    'Data Parallel (JAX)': dp_time,\n",
    "    'FSDP (JAX)': fsdp_time,\n",
    "    'Data Parallel (Flax)': flax_dp_time,\n",
    "    'FSDP (Flax)': flax_fsdp_time\n",
    "}\n",
    "\n",
    "# Calculate speedups relative to single device\n",
    "speedups = {k: single_device_time / v for k, v in timings.items()}\n",
    "\n",
    "# Plot results\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "# Plot training times\n",
    "methods = list(timings.keys())\n",
    "times = list(timings.values())\n",
    "ax1.bar(methods, times, color='skyblue')\n",
    "ax1.set_ylabel('Training Time (seconds)')\n",
    "ax1.set_title('Training Time Comparison')\n",
    "ax1.set_xticklabels(methods, rotation=45, ha='right')\n",
    "for i, v in enumerate(times):\n",
    "    ax1.text(i, v + 0.1, f\"{v:.2f}s\", ha='center')\n",
    "\n",
    "# Plot speedups\n",
    "speeds = list(speedups.values())\n",
    "ax2.bar(methods, speeds, color='lightgreen')\n",
    "ax2.set_ylabel('Speedup (relative to single device)')\n",
    "ax2.set_title('Speedup Comparison')\n",
    "ax2.set_xticklabels(methods, rotation=45, ha='right')\n",
    "ax2.axhline(y=1, color='red', linestyle='--')\n",
    "for i, v in enumerate(speeds):\n",
    "    ax2.text(i, v + 0.1, f\"{v:.2f}x\", ha='center')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Conclusion\n",
    "\n",
    "In this tutorial, we've explored different approaches to parallel training of neural networks in JAX:\n",
    "\n",
    "1. **Data Parallel (DP) Training**\n",
    "   - Replicates the model across devices\n",
    "   - Shards the data batch across devices\n",
    "   - Good for small to medium-sized models\n",
    "   - Limited by model size fitting on a single device\n",
    "\n",
    "2. **Fully Sharded Data Parallel (FSDP) Training**\n",
    "   - Shards both the model parameters and data across devices\n",
    "   - Uses a 2D mesh to organize sharding dimensions\n",
    "   - Better memory efficiency for large models\n",
    "   - Can achieve higher throughput with sufficient devices\n",
    "\n",
    "3. **Using Flax NNX for Distributed Training**\n",
    "   - Provides higher-level abstractions for model definition\n",
    "   - Simplifies the implementation of training loops\n",
    "   - Integrates well with JAX's parallelization primitives\n",
    "   - Offers a more user-friendly interface for complex models\n",
    "\n",
    "Key takeaways:\n",
    "\n",
    "- Data parallel training is straightforward to implement and provides good speedup for many scenarios\n",
    "- FSDP is more complex but can handle larger models and potentially provide better scaling\n",
    "- Flax NNX provides helpful abstractions that make distributed training more accessible\n",
    "- The choice between DP and FSDP depends on your model size, device memory constraints, and scaling requirements\n",
    "\n",
    "When scaling to very large models, like those used in NLP, FSDP becomes increasingly important as it allows you to train models that wouldn't fit in a single device's memory. This approach forms the foundation for training massive language models efficiently."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}