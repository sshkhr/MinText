{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# JAX Parallelization: Sharded Matrices and Collective Operations\n",
    "\n",
    "This tutorial explores how to effectively use JAX's distributed array abstractions and collective operations for efficient model parallelism. We'll cover:\n",
    "\n",
    "1. **Introduction to JAX and its parallelism features**\n",
    "2. **Setting up a JAX environment for multi-device computation**\n",
    "3. **Sharded Matrices: Partitioning arrays across devices**\n",
    "4. **Collective Operations: Efficient communication between devices**\n",
    "5. **Practical examples: Implementing distributed algorithms**\n",
    "\n",
    "Let's begin!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Introduction to JAX\n",
    "\n",
    "JAX is a high-performance numerical computing library that combines NumPy's familiar API with the power of automatic differentiation and hardware acceleration on GPUs and TPUs. Developed by Google Research, JAX enables writing high-performance code that can run efficiently on a single device or scale across multiple devices.\n",
    "\n",
    "Key features of JAX include:\n",
    "\n",
    "- **NumPy-like API**: Familiar interface for array operations\n",
    "- **Automatic differentiation**: Compute gradients with `grad`, `value_and_grad`, etc.\n",
    "- **Acceleration**: Seamless execution on GPUs and TPUs\n",
    "- **Function transformations**: `jit`, `vmap`, `pmap`, etc.\n",
    "- **Parallelism primitives**: Distribute computation across devices\n",
    "\n",
    "In this tutorial, we'll focus specifically on JAX's capabilities for distributed computation using sharded matrices and collective operations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Setting Up for Multi-Device Computation\n",
    "\n",
    "Let's start by setting up our environment and exploring the available devices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install JAX if needed\n",
    "# !pip install --upgrade jax jaxlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# os.environ[\"XLA_FLAGS\"] = '--xla_force_host_platform_device_count=8' # Use 8 CPU devices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from jax.sharding import Mesh, PartitionSpec as P\n",
    "from jax.experimental import mesh_utils\n",
    "import numpy as np\n",
    "import time\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JAX version: 0.6.1\n",
      "Available devices: [CpuDevice(id=0), CpuDevice(id=1), CpuDevice(id=2), CpuDevice(id=3), CpuDevice(id=4), CpuDevice(id=5), CpuDevice(id=6), CpuDevice(id=7)]\n",
      "Device count: 8\n"
     ]
    }
   ],
   "source": [
    "# Check available devices\n",
    "print(f\"JAX version: {jax.__version__}\")\n",
    "print(f\"Available devices: {jax.devices()}\")\n",
    "print(f\"Device count: {jax.device_count()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a Device Mesh\n",
    "\n",
    "JAX uses the concept of a device mesh to organize available devices. A mesh is a multi-dimensional array of devices that can be addressed along different axes. This allows us to partition our data and computation along different dimensions of the mesh."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1D Mesh: Mesh('devices': 8, axis_types=(Auto,))\n",
      "2D Mesh: Mesh('rows': 2, 'cols': 2, axis_types=(Auto, Auto))\n"
     ]
    }
   ],
   "source": [
    "# Create a simple 1D mesh using all available devices\n",
    "devices = jax.devices()\n",
    "mesh_1d = Mesh(devices, axis_names=('devices',))\n",
    "print(f\"1D Mesh: {mesh_1d}\")\n",
    "\n",
    "# If you have multiple devices, you can create a 2D mesh\n",
    "if len(devices) >= 4:\n",
    "    # Reshape into a 2D mesh (for example, 2x2 for 4 devices)\n",
    "    devices_array = np.array(devices[:4]).reshape((2, 2))\n",
    "    mesh_2d = Mesh(devices_array, axis_names=('rows', 'cols'))\n",
    "    print(f\"2D Mesh: {mesh_2d}\")\n",
    "else:\n",
    "    print(\"Not enough devices for a 2D mesh demonstration\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Sharded Matrices in JAX\n",
    "\n",
    "Sharded matrices are arrays that are split across multiple devices. JAX provides abstractions to create and operate on these distributed arrays efficiently.\n",
    "\n",
    "### Basic Concepts\n",
    "\n",
    "- **Mesh**: A logical arrangement of devices\n",
    "- **PartitionSpec (P)**: Specifies how to partition an array across mesh dimensions\n",
    "- **jax.device_put()**: Places an array on a specific device or according to a sharding\n",
    "\n",
    "Let's see how to create sharded arrays using JAX's sharding API:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matrix shape: (8000, 8000), Size in memory: 244.14 MB\n"
     ]
    }
   ],
   "source": [
    "# Let's create a large matrix and shard it\n",
    "matrix_size = 8000  # Adjust based on your device memory\n",
    "large_matrix = jnp.ones((matrix_size, matrix_size))\n",
    "print(f\"Matrix shape: {large_matrix.shape}, Size in memory: {large_matrix.size * 4 / (1024**2):.2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sharded matrix type: <class 'jaxlib._jax.ArrayImpl'>\n",
      "Sharding spec: NamedSharding(mesh=Mesh('devices': 8, axis_types=(Auto,)), spec=PartitionSpec('devices', None), memory_kind=unpinned_host)\n"
     ]
    }
   ],
   "source": [
    "# Create a sharded version of the matrix\n",
    "with mesh_1d:\n",
    "    # Define the partition spec - shard along the first dimension\n",
    "    partition_spec = P('devices', None)\n",
    "    \n",
    "    # Create a shardings object\n",
    "    from jax.sharding import NamedSharding\n",
    "    shardings = NamedSharding(mesh_1d, partition_spec)\n",
    "    \n",
    "    # Create the sharded array\n",
    "    sharded_matrix = jax.device_put(large_matrix, shardings)\n",
    "    \n",
    "    print(f\"Sharded matrix type: {type(sharded_matrix)}\")\n",
    "    print(f\"Sharding spec: {shardings}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inspecting Sharded Arrays\n",
    "\n",
    "We can inspect how the array is distributed across devices:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Global shape: (8000, 8000)\n",
      "Sharding: NamedSharding(mesh=Mesh('devices': 8, axis_types=(Auto,)), spec=PartitionSpec('devices', None), memory_kind=unpinned_host)\n",
      "Number of chunks: 1\n",
      "First chunk shape: (8000, 8000)\n"
     ]
    }
   ],
   "source": [
    "# Inspecting the sharding\n",
    "print(f\"Global shape: {sharded_matrix.shape}\")\n",
    "\n",
    "# How is the array sharded?\n",
    "sharding = sharded_matrix.sharding\n",
    "print(f\"Sharding: {sharding}\")\n",
    "\n",
    "# Get the local arrays on each device\n",
    "local_chunks = jax.experimental.multihost_utils.process_allgather(sharded_matrix)\n",
    "print(f\"Number of chunks: {len(local_chunks)}\")\n",
    "if len(local_chunks) > 0:\n",
    "    print(f\"First chunk shape: {local_chunks[0].shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performance Benefits\n",
    "\n",
    "Let's compare the performance of operations on sharded vs. non-sharded matrices:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Single device time: 0.1912 seconds\n",
      "Sharded time: 0.2180 seconds\n",
      "Speedup: 0.88x\n"
     ]
    }
   ],
   "source": [
    "# Define a simple matrix multiplication operation\n",
    "@jax.jit\n",
    "def matrix_multiply(x):\n",
    "    return jnp.matmul(x, x)\n",
    "\n",
    "# Create a smaller matrix for this benchmark\n",
    "benchmark_size = 4000  # Adjust based on your hardware\n",
    "matrix = jnp.ones((benchmark_size, benchmark_size))\n",
    "\n",
    "# Benchmark on a single device\n",
    "start = time.time()\n",
    "result_single = matrix_multiply(matrix).block_until_ready()\n",
    "time_single = time.time() - start\n",
    "print(f\"Single device time: {time_single:.4f} seconds\")\n",
    "\n",
    "# Benchmark with sharding (if multiple devices are available)\n",
    "if jax.device_count() > 1:\n",
    "    with mesh_1d:\n",
    "        shardings = NamedSharding(mesh_1d, P('devices', None))\n",
    "        sharded_matrix = jax.device_put(matrix, shardings)\n",
    "        \n",
    "        # Define a sharded computation\n",
    "        @jax.jit\n",
    "        def sharded_matrix_multiply(x):\n",
    "            return jnp.matmul(x, x)\n",
    "        \n",
    "        start = time.time()\n",
    "        result_sharded = sharded_matrix_multiply(sharded_matrix).block_until_ready()\n",
    "        time_sharded = time.time() - start\n",
    "        print(f\"Sharded time: {time_sharded:.4f} seconds\")\n",
    "        print(f\"Speedup: {time_single / time_sharded:.2f}x\")\n",
    "else:\n",
    "    print(\"Multiple devices required for sharding benchmark\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Different Sharding Strategies\n",
    "\n",
    "JAX supports various sharding strategies, including:\n",
    "\n",
    "1. **Row Sharding**: Distribute rows across devices\n",
    "2. **Column Sharding**: Distribute columns across devices\n",
    "3. **2D Block Sharding**: Distribute blocks in a grid pattern\n",
    "\n",
    "Let's implement and compare these strategies:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a matrix for our demonstrations\n",
    "demo_matrix = jnp.ones((1024, 1024))\n",
    "\n",
    "with mesh_1d:\n",
    "    # 1. Row sharding\n",
    "    row_spec = P('devices', None)\n",
    "    row_shardings = NamedSharding(mesh_1d, row_spec)\n",
    "    row_sharded = jax.device_put(demo_matrix, row_shardings)\n",
    "    \n",
    "    # 2. Column sharding\n",
    "    col_spec = P(None, 'devices')\n",
    "    col_shardings = NamedSharding(mesh_1d, col_spec)\n",
    "    col_sharded = jax.device_put(demo_matrix, col_shardings)\n",
    "    \n",
    "    print(f\"Row sharding: {row_shardings}\")\n",
    "    print(f\"Column sharding: {col_shardings}\")\n",
    "    \n",
    "    # If we have a 2D mesh, we can do 2D block sharding\n",
    "    if 'mesh_2d' in locals():\n",
    "        with mesh_2d:\n",
    "            # 3. 2D Block sharding\n",
    "            block_spec = P('rows', 'cols')\n",
    "            block_shardings = NamedSharding(mesh_2d, block_spec)\n",
    "            block_sharded = jax.device_put(demo_matrix, block_shardings)\n",
    "            print(f\"Block sharding: {block_shardings}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Choosing the Right Sharding Strategy\n",
    "\n",
    "The optimal sharding strategy depends on:\n",
    "\n",
    "1. **Computation pattern**: Match sharding to your algorithm's access patterns\n",
    "2. **Device topology**: Consider the physical layout of your devices\n",
    "3. **Operation types**: Different operations benefit from different sharding strategies\n",
    "\n",
    "For example:\n",
    "- Matrix multiplication benefits from row/column sharding\n",
    "- Convolutional operations may benefit from spatial partitioning\n",
    "- Element-wise operations work well with any partitioning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Collective Operations in JAX\n",
    "\n",
    "Collective operations enable efficient communication between devices. These are essential for distributed algorithms where devices need to share or aggregate information.\n",
    "\n",
    "Common collective operations include:\n",
    "- **all-reduce**: Aggregate values across devices (sum, mean, etc.)\n",
    "- **all-gather**: Collect values from all devices\n",
    "- **reduce-scatter**: Combine reduce and scatter operations\n",
    "- **all-to-all**: Exchange data between all devices\n",
    "\n",
    "Let's explore these operations in JAX:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### All-Reduce\n",
    "\n",
    "All-reduce aggregates values across devices using an operation like sum, mean, max, etc. Each device ends up with the same aggregated result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from jax.experimental import pjit\n",
    "\n",
    "# Create device-specific values\n",
    "def create_device_values(mesh):\n",
    "    # Create different values for each device\n",
    "    device_values = []\n",
    "    for i, device in enumerate(mesh.devices.flat):\n",
    "        value = jnp.ones((10, 10)) * (i + 1)  # Each device gets a different value\n",
    "        device_values.append(jax.device_put(value, device))\n",
    "    return device_values\n",
    "\n",
    "with mesh_1d:\n",
    "    # Example of all-reduce using pjit\n",
    "    @jax.jit\n",
    "    def all_reduce_sum(x):\n",
    "        # Explicit all-reduce\n",
    "        return jax.lax.psum(x, axis_name='devices')\n",
    "    \n",
    "    # Create a sharded array where each device has a different value\n",
    "    n_devices = jax.device_count()\n",
    "    sharded_values = jnp.arange(1, n_devices + 1).reshape((n_devices, 1))\n",
    "    shardings = NamedSharding(mesh_1d, P('devices', None))\n",
    "    sharded_array = jax.device_put(sharded_values, shardings)\n",
    "    \n",
    "    # Perform the all-reduce\n",
    "    result = all_reduce_sum(sharded_array)\n",
    "    print(f\"Original values:\\n{sharded_values}\")\n",
    "    print(f\"After all-reduce sum:\\n{result}\")\n",
    "    \n",
    "    # Expected result: each device should have the sum of all values\n",
    "    expected_sum = jnp.sum(jnp.arange(1, n_devices + 1))\n",
    "    print(f\"Expected sum: {expected_sum}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### All-Gather\n",
    "\n",
    "All-gather collects values from all devices, resulting in each device having a complete copy of all values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with mesh_1d:\n",
    "    # Example of all-gather\n",
    "    @jax.jit\n",
    "    def all_gather(x):\n",
    "        # Explicit all-gather\n",
    "        return jax.lax.all_gather(x, axis_name='devices', axis=0)\n",
    "    \n",
    "    # Use the same sharded array from before\n",
    "    result = all_gather(sharded_array)\n",
    "    print(f\"Original values:\\n{sharded_values}\")\n",
    "    print(f\"After all-gather:\\n{result}\")\n",
    "    \n",
    "    # Expected result: each device should have all values\n",
    "    expected_gather = jnp.arange(1, n_devices + 1).reshape((n_devices, 1))\n",
    "    print(f\"Expected result:\\n{expected_gather}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### All-to-All\n",
    "\n",
    "All-to-all exchanges slices of data between all devices. This is useful for operations like matrix transposition or redistributing data with a different sharding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if jax.device_count() >= 2:\n",
    "    with mesh_1d:\n",
    "        # Example of all-to-all\n",
    "        @jax.jit\n",
    "        def all_to_all(x):\n",
    "            # Explicit all-to-all - reshape to have a split dimension\n",
    "            return jax.lax.all_to_all(x, axis_name='devices', split_axis=0, concat_axis=1)\n",
    "        \n",
    "        # Create a matrix where each device has part of the rows\n",
    "        n_devices = jax.device_count()\n",
    "        data = jnp.arange(n_devices * n_devices).reshape((n_devices, n_devices))\n",
    "        shardings = NamedSharding(mesh_1d, P('devices', None))\n",
    "        sharded_data = jax.device_put(data, shardings)\n",
    "        \n",
    "        # Perform the all-to-all\n",
    "        result = all_to_all(sharded_data)\n",
    "        print(f\"Original data:\\n{data}\")\n",
    "        print(f\"After all-to-all:\\n{result}\")\n",
    "        \n",
    "        # This effectively transposes the sharding from rows to columns\n",
    "        print(f\"Original sharding: {sharded_data.sharding}\")\n",
    "        print(f\"Result sharding: {result.sharding}\")\n",
    "else:\n",
    "    print(\"Need at least 2 devices for all-to-all demonstration\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performance Considerations for Collective Operations\n",
    "\n",
    "When working with collective operations, consider:\n",
    "\n",
    "1. **Communication overhead**: Collective operations require device-to-device communication\n",
    "2. **Data size**: Larger transfers take more time\n",
    "3. **Network topology**: The physical connections between devices matter\n",
    "4. **Frequency**: Minimize the number of collective operations in your code\n",
    "\n",
    "Let's benchmark a simple all-reduce operation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if jax.device_count() > 1:\n",
    "    # Benchmark all-reduce with different data sizes\n",
    "    sizes = [10, 100, 1000, 10000]\n",
    "    times = []\n",
    "    \n",
    "    with mesh_1d:\n",
    "        for size in sizes:\n",
    "            # Create data\n",
    "            data = jnp.ones((size, size))\n",
    "            shardings = NamedSharding(mesh_1d, P('devices', None))\n",
    "            sharded_data = jax.device_put(data, shardings)\n",
    "            \n",
    "            # Define and compile the all-reduce\n",
    "            @jax.jit\n",
    "            def all_reduce(x):\n",
    "                return jax.lax.psum(x, axis_name='devices')\n",
    "            \n",
    "            # Warm-up\n",
    "            all_reduce(sharded_data).block_until_ready()\n",
    "            \n",
    "            # Benchmark\n",
    "            start = time.time()\n",
    "            all_reduce(sharded_data).block_until_ready()\n",
    "            elapsed = time.time() - start\n",
    "            times.append(elapsed)\n",
    "            \n",
    "            print(f\"Size {size}x{size}, Time: {elapsed:.6f} seconds\")\n",
    "    \n",
    "    # Plot the results\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(sizes, times, 'o-')\n",
    "    plt.xscale('log')\n",
    "    plt.xlabel('Matrix Size')\n",
    "    plt.ylabel('Time (seconds)')\n",
    "    plt.title('All-Reduce Performance by Data Size')\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"Multiple devices required for benchmark\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Practical Example: Distributed Matrix Multiplication\n",
    "\n",
    "Let's implement a distributed matrix multiplication algorithm using sharded matrices and collective operations.\n",
    "\n",
    "We'll implement a simple version of the cannon algorithm for distributed matrix multiplication:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_matrices(n):\n",
    "    \"\"\"Create two n x n matrices.\"\"\"\n",
    "    a = jnp.ones((n, n))\n",
    "    b = jnp.ones((n, n))\n",
    "    return a, b\n",
    "\n",
    "def baseline_matmul(a, b):\n",
    "    \"\"\"Standard matrix multiplication on a single device.\"\"\"\n",
    "    return jnp.matmul(a, b)\n",
    "\n",
    "def distributed_matmul(a, b, mesh):\n",
    "    \"\"\"Distributed matrix multiplication using sharded arrays.\"\"\"\n",
    "    with mesh:\n",
    "        # Shard matrices along rows and columns\n",
    "        row_spec = P('devices', None)\n",
    "        col_spec = P(None, 'devices')\n",
    "        \n",
    "        # Put A in row-sharded form and B in column-sharded form\n",
    "        a_shardings = NamedSharding(mesh, row_spec)\n",
    "        b_shardings = NamedSharding(mesh, col_spec)\n",
    "        \n",
    "        a_sharded = jax.device_put(a, a_shardings)\n",
    "        b_sharded = jax.device_put(b, b_shardings)\n",
    "        \n",
    "        # Define the distributed multiplication\n",
    "        @jax.jit\n",
    "        def sharded_matmul(a, b):\n",
    "            # Compute local product\n",
    "            local_product = jnp.matmul(a, b)\n",
    "            # Reduce across devices to get final result\n",
    "            return jax.lax.psum(local_product, axis_name='devices')\n",
    "        \n",
    "        return sharded_matmul(a_sharded, b_sharded)\n",
    "\n",
    "# Benchmark the two approaches\n",
    "if jax.device_count() > 1:\n",
    "    # Parameters\n",
    "    n = 2000  # Matrix size\n",
    "    \n",
    "    # Create matrices\n",
    "    a, b = create_matrices(n)\n",
    "    \n",
    "    # Baseline (single device)\n",
    "    baseline_fn = jax.jit(baseline_matmul)\n",
    "    baseline_fn(a, b).block_until_ready()  # Warm-up\n",
    "    \n",
    "    start = time.time()\n",
    "    c_baseline = baseline_fn(a, b).block_until_ready()\n",
    "    baseline_time = time.time() - start\n",
    "    print(f\"Baseline time: {baseline_time:.4f} seconds\")\n",
    "    \n",
    "    # Distributed\n",
    "    start = time.time()\n",
    "    c_distributed = distributed_matmul(a, b, mesh_1d).block_until_ready()\n",
    "    distributed_time = time.time() - start\n",
    "    print(f\"Distributed time: {distributed_time:.4f} seconds\")\n",
    "    \n",
    "    # Verify results are similar\n",
    "    diff = jnp.abs(c_baseline - c_distributed).max()\n",
    "    print(f\"Maximum difference: {diff}\")\n",
    "    \n",
    "    # Calculate speedup\n",
    "    speedup = baseline_time / distributed_time\n",
    "    print(f\"Speedup: {speedup:.2f}x\")\n",
    "else:\n",
    "    print(\"Multiple devices required for benchmark\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Advanced Topics\n",
    "\n",
    "### Automatic Sharding with SPMD (Single Program Multiple Data)\n",
    "\n",
    "JAX provides the `pjit` (Partitioned JIT) API for automatic sharding. With `pjit`, you specify the sharding of inputs and outputs, and JAX determines the optimal intermediate shardings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from jax.experimental import pjit\n",
    "\n",
    "def auto_sharded_matmul(a, b, mesh):\n",
    "    \"\"\"Matrix multiplication with automatic sharding.\"\"\"\n",
    "    with mesh:\n",
    "        # Define input and output specs\n",
    "        in_specs = (P('devices', None), P(None, 'devices'))  # Row and column sharding\n",
    "        out_spec = P(None, None)  # Output on all devices\n",
    "        \n",
    "        # Define the pjit function\n",
    "        pjit_matmul = pjit.pjit(\n",
    "            lambda x, y: jnp.matmul(x, y),\n",
    "            in_shardings=in_specs,\n",
    "            out_shardings=out_spec\n",
    "        )\n",
    "        \n",
    "        return pjit_matmul(a, b)\n",
    "\n",
    "if jax.device_count() > 1:\n",
    "    # Try the auto-sharded version\n",
    "    a, b = create_matrices(2000)\n",
    "    \n",
    "    start = time.time()\n",
    "    c_auto = auto_sharded_matmul(a, b, mesh_1d).block_until_ready()\n",
    "    auto_time = time.time() - start\n",
    "    print(f\"Auto-sharded time: {auto_time:.4f} seconds\")\n",
    "    \n",
    "    # Compare with our manual implementation\n",
    "    if 'distributed_time' in locals():\n",
    "        print(f\"Manual vs Auto ratio: {distributed_time / auto_time:.2f}x\")\n",
    "else:\n",
    "    print(\"Multiple devices required for demonstration\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom Sharding Rules\n",
    "\n",
    "JAX allows defining custom partitioning rules for operations. This is useful for operations where the default partitioning might not be optimal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Advanced topic: Custom partitioning rules\n",
    "from functools import partial\n",
    "\n",
    "def custom_matmul_rule(mesh):\n",
    "    # This is a simplified example - real custom rules would be more complex\n",
    "    def matmul_with_custom_rule(x, y):\n",
    "        # Custom implementation that's aware of sharding\n",
    "        return jnp.matmul(x, y)\n",
    "    \n",
    "    return matmul_with_custom_rule\n",
    "\n",
    "# Example usage (conceptual)\n",
    "# my_custom_matmul = custom_matmul_rule(mesh_1d)\n",
    "# result = my_custom_matmul(sharded_a, sharded_b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Real-World Application: Distributed Training\n",
    "\n",
    "Let's see how sharded matrices and collective operations are used in a simplified distributed training loop:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(rng_key):\n",
    "    \"\"\"Create a simple linear model.\"\"\"\n",
    "    return jnp.ones((1000, 100))\n",
    "\n",
    "def create_batch(rng_key, batch_size=32):\n",
    "    \"\"\"Create a batch of data.\"\"\"\n",
    "    x = jax.random.normal(rng_key, (batch_size, 1000))\n",
    "    y = jax.random.normal(rng_key, (batch_size, 100))\n",
    "    return x, y\n",
    "\n",
    "def loss_fn(params, x, y):\n",
    "    \"\"\"Compute MSE loss.\"\"\"\n",
    "    preds = jnp.matmul(x, params)\n",
    "    return jnp.mean((preds - y) ** 2)\n",
    "\n",
    "def grad_fn(params, x, y):\n",
    "    \"\"\"Compute gradients.\"\"\"\n",
    "    return jax.grad(loss_fn)(params, x, y)\n",
    "\n",
    "def distributed_training_step(params, batch, mesh, learning_rate=0.01):\n",
    "    \"\"\"Perform a distributed training step.\"\"\"\n",
    "    x, y = batch\n",
    "    \n",
    "    with mesh:\n",
    "        # Shard parameters and data\n",
    "        param_spec = P('devices', None)\n",
    "        data_spec = P('devices', None)\n",
    "        \n",
    "        param_shardings = NamedSharding(mesh, param_spec)\n",
    "        data_shardings = NamedSharding(mesh, data_spec)\n",
    "        \n",
    "        sharded_params = jax.device_put(params, param_shardings)\n",
    "        sharded_x = jax.device_put(x, data_shardings)\n",
    "        sharded_y = jax.device_put(y, data_shardings)\n",
    "        \n",
    "        # Define sharded computation\n",
    "        @jax.jit\n",
    "        def sharded_update(params, x, y):\n",
    "            # Compute local gradients\n",
    "            grads = grad_fn(params, x, y)\n",
    "            \n",
    "            # Average gradients across devices\n",
    "            grads = jax.lax.pmean(grads, axis_name='devices')\n",
    "            \n",
    "            # Update parameters\n",
    "            new_params = params - learning_rate * grads\n",
    "            \n",
    "            # Compute loss (just for monitoring)\n",
    "            loss = loss_fn(params, x, y)\n",
    "            avg_loss = jax.lax.pmean(loss, axis_name='devices')\n",
    "            \n",
    "            return new_params, avg_loss\n",
    "        \n",
    "        new_params, loss = sharded_update(sharded_params, sharded_x, sharded_y)\n",
    "        return new_params, loss\n",
    "\n",
    "# Example training loop\n",
    "if jax.device_count() > 1:\n",
    "    # Initialize model and data\n",
    "    rng_key = jax.random.PRNGKey(42)\n",
    "    model_params = create_model(rng_key)\n",
    "    \n",
    "    # Train for a few steps\n",
    "    for step in range(5):\n",
    "        # Create a new batch\n",
    "        rng_key, subkey = jax.random.split(rng_key)\n",
    "        batch = create_batch(subkey)\n",
    "        \n",
    "        # Update parameters\n",
    "        model_params, loss = distributed_training_step(model_params, batch, mesh_1d)\n",
    "        print(f\"Step {step}, Loss: {loss:.6f}\")\n",
    "else:\n",
    "    print(\"Multiple devices required for distributed training\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Conclusion\n",
    "\n",
    "In this tutorial, we've explored JAX's powerful capabilities for distributed computation using sharded matrices and collective operations. We've covered:\n",
    "\n",
    "1. **Setting up a device mesh** for organizing available devices\n",
    "2. **Creating and using sharded matrices** to distribute data across devices\n",
    "3. **Different sharding strategies** and when to use them\n",
    "4. **Collective operations** for efficient device-to-device communication\n",
    "5. **Implementing distributed algorithms** using these primitives\n",
    "6. **Automatic sharding** with pjit for easier distributed programming\n",
    "7. **Application to distributed training** for machine learning models\n",
    "\n",
    "JAX's sharding capabilities enable efficient scaling of numerical computations across multiple devices, making it a powerful tool for large-scale machine learning and scientific computing.\n",
    "\n",
    "### Further Resources\n",
    "\n",
    "- [JAX Documentation](https://jax.readthedocs.io/)\n",
    "- [JAX Docs: Distributed arrays and automatic parallelization](https://docs.jax.dev/en/latest/notebooks/Distributed_arrays_and_automatic_parallelization.html)\n",
    "- [JAX Docs: Manual parallelism with `shard_map`](https://docs.jax.dev/en/latest/notebooks/shard_map.html)\n",
    "- [How to Scale Your Model: Sharded Matrices and How to Multiply Them](https://jax-ml.github.io/scaling-book/sharding/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jax-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
