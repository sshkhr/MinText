{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o5p_3ph2u3sv"
      },
      "source": [
        "# Parallelization: Sharded Matrices and Collective Operations\n",
        "\n",
        "This tutorial covers the basics of sharded matrix operations and collection primitives that are often used in distributed training and inference in deep learning."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qloP9r0Tu3sw"
      },
      "source": [
        "## 0. Why JAX?\n",
        "\n",
        "[JAX](https://docs.jax.dev/) is a high-performance numerical computing library that combines NumPy's familiar API with the power of automatic differentiation and hardware acceleration on GPUs and TPUs. Developed by Google Research, JAX enables writing high-performance code that can run efficiently on a single device or scale across multiple devices.\n",
        "\n",
        "\n",
        "We use JAX for this notebook (and in the MinText library) because of several reasons:\n",
        "\n",
        "1. Beginner-friendly automatic parallelization using `jax.jit`\n",
        "2. Ability to simulate multiple devices using `\"XLA_FLAGS\"`\n",
        "3. Google Colab provides an 8 device runtime, v2-8 TPU, for free. This consists of 8x8 GB TPU cores which adds up to a total of 64 GB VRAM compute. So you can actually run distributed operations over 8 devices.\n",
        "4. There are already several great pedagogical style libraries in Pytorch (such as [HuggingFace Nanotron](https://github.com/huggingface/nanotron)) which serve a similar purpose. The key concepts from these tutorials can often be directly translated to PyTorch."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RgGURMiBu3sw"
      },
      "source": [
        "## 1. Multi-Device Computation\n",
        "\n",
        "Choose the v2-8 TPU runtime in Google Colab to run this notebook. Once you restart the  and exploring the available devices."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2i4n1zkju3sw"
      },
      "outputs": [],
      "source": [
        "# Install JAX if needed\n",
        "# !pip install --upgrade jax jaxlib"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "bJytD437u3sw"
      },
      "outputs": [],
      "source": [
        "# Uncomment this to simulate running the code on 8 CPU devices (use for local runs)\n",
        "# import os\n",
        "# os.environ[\"XLA_FLAGS\"] = '--xla_force_host_platform_device_count=8' # Use 8 CPU devices"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "JvYD0GEWu3sx"
      },
      "outputs": [],
      "source": [
        "import jax\n",
        "import jax.numpy as jnp\n",
        "from jax.sharding import Mesh, NamedSharding, PartitionSpec as P\n",
        "from jax.experimental import mesh_utils\n",
        "import numpy as np\n",
        "import time\n",
        "import matplotlib.pyplot as plt\n",
        "from typing import Optional"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "Dk-ci7Ozu3sx",
        "outputId": "29cefd99-c530-4e46-a70f-acf876f536b4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "JAX version: 0.5.2\n",
            "Available devices: [TpuDevice(id=0, process_index=0, coords=(0,0,0), core_on_chip=0), TpuDevice(id=1, process_index=0, coords=(0,0,0), core_on_chip=1), TpuDevice(id=2, process_index=0, coords=(1,0,0), core_on_chip=0), TpuDevice(id=3, process_index=0, coords=(1,0,0), core_on_chip=1), TpuDevice(id=4, process_index=0, coords=(0,1,0), core_on_chip=0), TpuDevice(id=5, process_index=0, coords=(0,1,0), core_on_chip=1), TpuDevice(id=6, process_index=0, coords=(1,1,0), core_on_chip=0), TpuDevice(id=7, process_index=0, coords=(1,1,0), core_on_chip=1)]\n",
            "Device count: 8\n"
          ]
        }
      ],
      "source": [
        "# Check available devices\n",
        "print(f\"JAX version: {jax.__version__}\")\n",
        "print(f\"Available devices: {jax.devices()}\")\n",
        "print(f\"Device count: {jax.device_count()}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "If you cannot access the v2-8 TPU from Google Colab (you timed out or are running this locally) restart this notebook and un-comment the `os.environ[\"XLA_FLAGS\"] = '--xla_force_host_platform_device_count=8'` flag to simulate multiple devices on your CPU.\n",
        "\n",
        "The output of the cell above should then change to look something like this:\n",
        "\n",
        "```bash\n",
        "Available devices: [CpuDevice(id=0), CpuDevice(id=1), CpuDevice(id=2), CpuDevice(id=3), CpuDevice(id=4), CpuDevice(id=5), CpuDevice(id=6), CpuDevice(id=7)]\n",
        "Device count: 8\n",
        "```"
      ],
      "metadata": {
        "id": "_HjN6kuixEnI"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_NVkU2c4u3sx"
      },
      "source": [
        "### Creating a Device Mesh\n",
        "\n",
        "JAX uses the concept of a device mesh to organize available devices. A device can be a CPU (or a CPU core), GPU, or TPU for JAX's purpose. A mesh is a multi-dimensional array of devices that can be addressed along different axes. This allows us to partition our data and computation along different dimensions of the mesh."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "fnLKxdOyu3sx",
        "outputId": "aa9f67ec-d0b3-4124-c09b-66b40b696daf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2D Mesh: Mesh('x': 2, 'y': 2)\n",
            "[0, 1, 2, 3]\n"
          ]
        }
      ],
      "source": [
        "devices = jax.devices()\n",
        "\n",
        "# If you have multiple devices, you can create a 2D mesh\n",
        "if len(devices) >= 4:\n",
        "    mesh_2d = jax.make_mesh((2, 2), ('x', 'y'))\n",
        "    print(f\"2D Mesh: {mesh_2d}\")\n",
        "    print([d.id for d in mesh_2d.devices.flat])\n",
        "else:\n",
        "    print(\"Not enough devices for a 2D mesh demonstration\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fL_6yWgyu3sx"
      },
      "source": [
        "## 2. Sharded Matrices\n",
        "\n",
        "Sharded matrices are arrays that are split across multiple devices. JAX provides abstractions to create and operate on these distributed arrays efficiently.\n",
        "\n",
        "### Basic Concepts\n",
        "\n",
        "- **Mesh**: A logical arrangement of devices\n",
        "- **PartitionSpec (P)**: Specifies how to partition an array across mesh dimensions\n",
        "- **jax.device_put()**: Places an array on a specific device or according to a sharding\n",
        "\n",
        "Let's see how to create sharded arrays using JAX's sharding API:"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "![Sharding](https://github.com/jax-ml/scaling-book/blob/main/assets/img/sharding-example.png?raw=true)"
      ],
      "metadata": {
        "id": "qfGkerAz1Enu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<sup> Image Source: [How To Scale Your Model](https://jax-ml.github.io/scaling-book) </sup>"
      ],
      "metadata": {
        "id": "hfdTmAIu1R2b"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "biXk36aWu3sx",
        "outputId": "fa955f08-f46e-466c-9ad4-96597d4e9bef",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Matrix shape: (8192, 8192), Size in memory: 256.00 MB\n"
          ]
        }
      ],
      "source": [
        "# Let's create a large matrix and shard it\n",
        "matrix_size = 8192  # Adjust based on your device memory\n",
        "matrix = jnp.ones((matrix_size, matrix_size))\n",
        "print(f\"Matrix shape: {matrix.shape}, Size in memory: {matrix.size * 4 / (1024**2):.2f} MB\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "9CJJEff5u3sx",
        "outputId": "20bd8a95-a80a-4d56-85e3-e5276bcebb1f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 271
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sharded matrix type: <class 'jaxlib.xla_extension.ArrayImpl'>\n",
            "Sharding spec: NamedSharding(mesh=Mesh('x': 2, 'y': 2), spec=PartitionSpec('x', 'y'), memory_kind=device)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[38;2;255;255;255;48;2;57;59;121m            \u001b[0m\u001b[38;2;255;255;255;48;2;222;158;214m            \u001b[0m\n",
              "\u001b[38;2;255;255;255;48;2;57;59;121m            \u001b[0m\u001b[38;2;255;255;255;48;2;222;158;214m            \u001b[0m\n",
              "\u001b[38;2;255;255;255;48;2;57;59;121m   \u001b[0m\u001b[38;2;255;255;255;48;2;57;59;121mTPU 0\u001b[0m\u001b[38;2;255;255;255;48;2;57;59;121m    \u001b[0m\u001b[38;2;255;255;255;48;2;222;158;214m   \u001b[0m\u001b[38;2;255;255;255;48;2;222;158;214mTPU 1\u001b[0m\u001b[38;2;255;255;255;48;2;222;158;214m    \u001b[0m\n",
              "\u001b[38;2;255;255;255;48;2;57;59;121m            \u001b[0m\u001b[38;2;255;255;255;48;2;222;158;214m            \u001b[0m\n",
              "\u001b[38;2;255;255;255;48;2;57;59;121m            \u001b[0m\u001b[38;2;255;255;255;48;2;222;158;214m            \u001b[0m\n",
              "\u001b[38;2;255;255;255;48;2;57;59;121m            \u001b[0m\u001b[38;2;255;255;255;48;2;222;158;214m            \u001b[0m\n",
              "\u001b[38;2;255;255;255;48;2;173;73;74m            \u001b[0m\u001b[38;2;0;0;0;48;2;181;207;107m            \u001b[0m\n",
              "\u001b[38;2;255;255;255;48;2;173;73;74m            \u001b[0m\u001b[38;2;0;0;0;48;2;181;207;107m            \u001b[0m\n",
              "\u001b[38;2;255;255;255;48;2;173;73;74m   \u001b[0m\u001b[38;2;255;255;255;48;2;173;73;74mTPU 2\u001b[0m\u001b[38;2;255;255;255;48;2;173;73;74m    \u001b[0m\u001b[38;2;0;0;0;48;2;181;207;107m   \u001b[0m\u001b[38;2;0;0;0;48;2;181;207;107mTPU 3\u001b[0m\u001b[38;2;0;0;0;48;2;181;207;107m    \u001b[0m\n",
              "\u001b[38;2;255;255;255;48;2;173;73;74m            \u001b[0m\u001b[38;2;0;0;0;48;2;181;207;107m            \u001b[0m\n",
              "\u001b[38;2;255;255;255;48;2;173;73;74m            \u001b[0m\u001b[38;2;0;0;0;48;2;181;207;107m            \u001b[0m\n",
              "\u001b[38;2;255;255;255;48;2;173;73;74m            \u001b[0m\u001b[38;2;0;0;0;48;2;181;207;107m            \u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #393b79\">            </span><span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #de9ed6\">            </span>\n",
              "<span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #393b79\">            </span><span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #de9ed6\">            </span>\n",
              "<span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #393b79\">   TPU 0    </span><span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #de9ed6\">   TPU 1    </span>\n",
              "<span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #393b79\">            </span><span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #de9ed6\">            </span>\n",
              "<span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #393b79\">            </span><span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #de9ed6\">            </span>\n",
              "<span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #393b79\">            </span><span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #de9ed6\">            </span>\n",
              "<span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #ad494a\">            </span><span style=\"color: #000000; text-decoration-color: #000000; background-color: #b5cf6b\">            </span>\n",
              "<span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #ad494a\">            </span><span style=\"color: #000000; text-decoration-color: #000000; background-color: #b5cf6b\">            </span>\n",
              "<span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #ad494a\">   TPU 2    </span><span style=\"color: #000000; text-decoration-color: #000000; background-color: #b5cf6b\">   TPU 3    </span>\n",
              "<span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #ad494a\">            </span><span style=\"color: #000000; text-decoration-color: #000000; background-color: #b5cf6b\">            </span>\n",
              "<span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #ad494a\">            </span><span style=\"color: #000000; text-decoration-color: #000000; background-color: #b5cf6b\">            </span>\n",
              "<span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #ad494a\">            </span><span style=\"color: #000000; text-decoration-color: #000000; background-color: #b5cf6b\">            </span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "# Define the partition spec - shard along the first dimension\n",
        "partition_spec = P('x', 'y')\n",
        "\n",
        "# Create a shardings object\n",
        "from jax.sharding import NamedSharding\n",
        "shardings = NamedSharding(mesh_2d, partition_spec)\n",
        "\n",
        "# Create the sharded array\n",
        "sharded_matrix = jax.device_put(x=matrix, device=shardings)\n",
        "\n",
        "print(f\"Sharded matrix type: {type(sharded_matrix)}\")\n",
        "print(f\"Sharding spec: {shardings}\")\n",
        "\n",
        "jax.debug.visualize_array_sharding(sharded_matrix)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Aa9Xp22Pu3sx"
      },
      "source": [
        "### Inspecting Sharded Matrices\n",
        "\n",
        "We can inspect how the array is distributed across devices:"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Global matrix shape\", sharded_matrix.shape)\n",
        "\n",
        "# Get the local arrays on each device\n",
        "print(\"Shapes of Matrix Shards:\")\n",
        "for shard in sharded_matrix.addressable_shards:\n",
        "  print(shard.device, shard.index, shard.data.shape)"
      ],
      "metadata": {
        "id": "BPvuoHw96fHZ",
        "outputId": "2f0df8c8-e612-44dc-b063-a419082ab240",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Global matrix shape (8192, 8192)\n",
            "Shapes of Matrix Shards:\n",
            "TPU_0(process=0,(0,0,0,0)) (slice(0, 4096, None), slice(0, 4096, None)) (4096, 4096)\n",
            "TPU_1(process=0,(0,0,0,1)) (slice(0, 4096, None), slice(4096, 8192, None)) (4096, 4096)\n",
            "TPU_2(process=0,(1,0,0,0)) (slice(4096, 8192, None), slice(0, 4096, None)) (4096, 4096)\n",
            "TPU_3(process=0,(1,0,0,1)) (slice(4096, 8192, None), slice(4096, 8192, None)) (4096, 4096)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "![Array Sharding Visualization](https://jax-ml.github.io/scaling-book/assets/img/sharding-colored4.png)"
      ],
      "metadata": {
        "id": "ra5f9aOe9936"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<sup> Image Source: [How To Scale Your Model](https://jax-ml.github.io/scaling-book) </sup>"
      ],
      "metadata": {
        "id": "KUtX2m3S9937"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v1aOZjpYu3sx"
      },
      "source": [
        "### Performance Benefits\n",
        "\n",
        "Let's compare the performance of element-wise operations on sharded vs. non-sharded matrices (we will matrix-level operations in a bit)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "c3y0rJzMu3sx",
        "outputId": "d855e50f-b968-47e2-9e5b-6976eff817d4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.88 ms ± 36.4 µs per loop (mean ± std. dev. of 5 runs, 5 loops each)\n"
          ]
        }
      ],
      "source": [
        "# `matrix` is present on a single device\n",
        "%timeit -n 5 -r 5 jnp.sin(matrix).block_until_ready()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# `sharded_matrix` is distributed across 4 devices\n",
        "%timeit -n 5 -r 5 jnp.sin(sharded_matrix).block_until_ready()"
      ],
      "metadata": {
        "id": "hZpzir6m_D6u",
        "outputId": "8cb3b340-04a8-419b-a57c-956fff6d0989",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1.64 ms ± 55.9 µs per loop (mean ± std. dev. of 5 runs, 5 loops each)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9PFgKkSJu3sy"
      },
      "source": [
        "### Different Sharding Strategies\n",
        "\n",
        "Each axis of the matrix can be sharded across each possible axis in the device mesh. This gives rise to a combinatorial number of possible shardings."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# A small helper function to define a sharding mesh\n",
        "default_mesh = jax.make_mesh((2, 2), ('a', 'b'))\n",
        "\n",
        "def mesh_sharding(\n",
        "    pspec: P, mesh: Optional[Mesh] = None,\n",
        "  ) -> NamedSharding:\n",
        "  if mesh is None:\n",
        "    mesh = default_mesh\n",
        "  return NamedSharding(mesh, pspec)"
      ],
      "metadata": {
        "id": "3UCpiympBJB6"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "dc-8P5xVu3sy",
        "outputId": "cd549c0f-4eb5-4819-b7d3-95fcabf93a68",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 215
        }
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[38;2;255;255;255;48;2;57;59;121m            \u001b[0m\u001b[38;2;255;255;255;48;2;222;158;214m            \u001b[0m\n",
              "\u001b[38;2;255;255;255;48;2;57;59;121m            \u001b[0m\u001b[38;2;255;255;255;48;2;222;158;214m            \u001b[0m\n",
              "\u001b[38;2;255;255;255;48;2;57;59;121m   \u001b[0m\u001b[38;2;255;255;255;48;2;57;59;121mTPU 0\u001b[0m\u001b[38;2;255;255;255;48;2;57;59;121m    \u001b[0m\u001b[38;2;255;255;255;48;2;222;158;214m   \u001b[0m\u001b[38;2;255;255;255;48;2;222;158;214mTPU 1\u001b[0m\u001b[38;2;255;255;255;48;2;222;158;214m    \u001b[0m\n",
              "\u001b[38;2;255;255;255;48;2;57;59;121m            \u001b[0m\u001b[38;2;255;255;255;48;2;222;158;214m            \u001b[0m\n",
              "\u001b[38;2;255;255;255;48;2;57;59;121m            \u001b[0m\u001b[38;2;255;255;255;48;2;222;158;214m            \u001b[0m\n",
              "\u001b[38;2;255;255;255;48;2;57;59;121m            \u001b[0m\u001b[38;2;255;255;255;48;2;222;158;214m            \u001b[0m\n",
              "\u001b[38;2;255;255;255;48;2;173;73;74m            \u001b[0m\u001b[38;2;0;0;0;48;2;181;207;107m            \u001b[0m\n",
              "\u001b[38;2;255;255;255;48;2;173;73;74m            \u001b[0m\u001b[38;2;0;0;0;48;2;181;207;107m            \u001b[0m\n",
              "\u001b[38;2;255;255;255;48;2;173;73;74m   \u001b[0m\u001b[38;2;255;255;255;48;2;173;73;74mTPU 2\u001b[0m\u001b[38;2;255;255;255;48;2;173;73;74m    \u001b[0m\u001b[38;2;0;0;0;48;2;181;207;107m   \u001b[0m\u001b[38;2;0;0;0;48;2;181;207;107mTPU 3\u001b[0m\u001b[38;2;0;0;0;48;2;181;207;107m    \u001b[0m\n",
              "\u001b[38;2;255;255;255;48;2;173;73;74m            \u001b[0m\u001b[38;2;0;0;0;48;2;181;207;107m            \u001b[0m\n",
              "\u001b[38;2;255;255;255;48;2;173;73;74m            \u001b[0m\u001b[38;2;0;0;0;48;2;181;207;107m            \u001b[0m\n",
              "\u001b[38;2;255;255;255;48;2;173;73;74m            \u001b[0m\u001b[38;2;0;0;0;48;2;181;207;107m            \u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #393b79\">            </span><span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #de9ed6\">            </span>\n",
              "<span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #393b79\">            </span><span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #de9ed6\">            </span>\n",
              "<span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #393b79\">   TPU 0    </span><span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #de9ed6\">   TPU 1    </span>\n",
              "<span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #393b79\">            </span><span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #de9ed6\">            </span>\n",
              "<span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #393b79\">            </span><span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #de9ed6\">            </span>\n",
              "<span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #393b79\">            </span><span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #de9ed6\">            </span>\n",
              "<span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #ad494a\">            </span><span style=\"color: #000000; text-decoration-color: #000000; background-color: #b5cf6b\">            </span>\n",
              "<span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #ad494a\">            </span><span style=\"color: #000000; text-decoration-color: #000000; background-color: #b5cf6b\">            </span>\n",
              "<span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #ad494a\">   TPU 2    </span><span style=\"color: #000000; text-decoration-color: #000000; background-color: #b5cf6b\">   TPU 3    </span>\n",
              "<span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #ad494a\">            </span><span style=\"color: #000000; text-decoration-color: #000000; background-color: #b5cf6b\">            </span>\n",
              "<span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #ad494a\">            </span><span style=\"color: #000000; text-decoration-color: #000000; background-color: #b5cf6b\">            </span>\n",
              "<span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #ad494a\">            </span><span style=\"color: #000000; text-decoration-color: #000000; background-color: #b5cf6b\">            </span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "# Shard first axis of matrix along mesh axis 'a', second axis of matrix along mesh axis 'b'\n",
        "ix_jy_sharding = jax.device_put(matrix, mesh_sharding(P('a', 'b')))\n",
        "jax.debug.visualize_array_sharding(ix_jy_sharding)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Shard first axis of matrix along mesh axis 'b', second axis of matrix along mesh axis 'a'\n",
        "iy_jx_sharding = jax.device_put(matrix, mesh_sharding(P('b', 'a')))\n",
        "jax.debug.visualize_array_sharding(iy_jx_sharding)"
      ],
      "metadata": {
        "id": "Pn0Ihnt4BdgE",
        "outputId": "075b05a2-d025-4b65-8629-cb313a2c1625",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 215
        }
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[38;2;255;255;255;48;2;57;59;121m            \u001b[0m\u001b[38;2;255;255;255;48;2;222;158;214m            \u001b[0m\n",
              "\u001b[38;2;255;255;255;48;2;57;59;121m            \u001b[0m\u001b[38;2;255;255;255;48;2;222;158;214m            \u001b[0m\n",
              "\u001b[38;2;255;255;255;48;2;57;59;121m   \u001b[0m\u001b[38;2;255;255;255;48;2;57;59;121mTPU 0\u001b[0m\u001b[38;2;255;255;255;48;2;57;59;121m    \u001b[0m\u001b[38;2;255;255;255;48;2;222;158;214m   \u001b[0m\u001b[38;2;255;255;255;48;2;222;158;214mTPU 2\u001b[0m\u001b[38;2;255;255;255;48;2;222;158;214m    \u001b[0m\n",
              "\u001b[38;2;255;255;255;48;2;57;59;121m            \u001b[0m\u001b[38;2;255;255;255;48;2;222;158;214m            \u001b[0m\n",
              "\u001b[38;2;255;255;255;48;2;57;59;121m            \u001b[0m\u001b[38;2;255;255;255;48;2;222;158;214m            \u001b[0m\n",
              "\u001b[38;2;255;255;255;48;2;57;59;121m            \u001b[0m\u001b[38;2;255;255;255;48;2;222;158;214m            \u001b[0m\n",
              "\u001b[38;2;255;255;255;48;2;173;73;74m            \u001b[0m\u001b[38;2;0;0;0;48;2;181;207;107m            \u001b[0m\n",
              "\u001b[38;2;255;255;255;48;2;173;73;74m            \u001b[0m\u001b[38;2;0;0;0;48;2;181;207;107m            \u001b[0m\n",
              "\u001b[38;2;255;255;255;48;2;173;73;74m   \u001b[0m\u001b[38;2;255;255;255;48;2;173;73;74mTPU 1\u001b[0m\u001b[38;2;255;255;255;48;2;173;73;74m    \u001b[0m\u001b[38;2;0;0;0;48;2;181;207;107m   \u001b[0m\u001b[38;2;0;0;0;48;2;181;207;107mTPU 3\u001b[0m\u001b[38;2;0;0;0;48;2;181;207;107m    \u001b[0m\n",
              "\u001b[38;2;255;255;255;48;2;173;73;74m            \u001b[0m\u001b[38;2;0;0;0;48;2;181;207;107m            \u001b[0m\n",
              "\u001b[38;2;255;255;255;48;2;173;73;74m            \u001b[0m\u001b[38;2;0;0;0;48;2;181;207;107m            \u001b[0m\n",
              "\u001b[38;2;255;255;255;48;2;173;73;74m            \u001b[0m\u001b[38;2;0;0;0;48;2;181;207;107m            \u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #393b79\">            </span><span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #de9ed6\">            </span>\n",
              "<span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #393b79\">            </span><span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #de9ed6\">            </span>\n",
              "<span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #393b79\">   TPU 0    </span><span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #de9ed6\">   TPU 2    </span>\n",
              "<span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #393b79\">            </span><span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #de9ed6\">            </span>\n",
              "<span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #393b79\">            </span><span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #de9ed6\">            </span>\n",
              "<span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #393b79\">            </span><span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #de9ed6\">            </span>\n",
              "<span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #ad494a\">            </span><span style=\"color: #000000; text-decoration-color: #000000; background-color: #b5cf6b\">            </span>\n",
              "<span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #ad494a\">            </span><span style=\"color: #000000; text-decoration-color: #000000; background-color: #b5cf6b\">            </span>\n",
              "<span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #ad494a\">   TPU 1    </span><span style=\"color: #000000; text-decoration-color: #000000; background-color: #b5cf6b\">   TPU 3    </span>\n",
              "<span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #ad494a\">            </span><span style=\"color: #000000; text-decoration-color: #000000; background-color: #b5cf6b\">            </span>\n",
              "<span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #ad494a\">            </span><span style=\"color: #000000; text-decoration-color: #000000; background-color: #b5cf6b\">            </span>\n",
              "<span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #ad494a\">            </span><span style=\"color: #000000; text-decoration-color: #000000; background-color: #b5cf6b\">            </span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Shard first axis of matrix along mesh axis 'a', replicate second axis of matrix along each shard\n",
        "ix_j_sharding = jax.device_put(matrix, mesh_sharding(P('a', None)))\n",
        "jax.debug.visualize_array_sharding(ix_j_sharding, use_color=False)"
      ],
      "metadata": {
        "id": "bk-iDwLkB1HB",
        "outputId": "680db174-d635-45bd-8319-a94d0a8406e8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 199
        }
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┌───────────────────────┐\n",
              "│                       │\n",
              "│        TPU \u001b[1;36m0\u001b[0m,\u001b[1;36m1\u001b[0m        │\n",
              "│                       │\n",
              "│                       │\n",
              "├───────────────────────┤\n",
              "│                       │\n",
              "│        TPU \u001b[1;36m2\u001b[0m,\u001b[1;36m3\u001b[0m        │\n",
              "│                       │\n",
              "│                       │\n",
              "└───────────────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┌───────────────────────┐\n",
              "│                       │\n",
              "│        TPU <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>        │\n",
              "│                       │\n",
              "│                       │\n",
              "├───────────────────────┤\n",
              "│                       │\n",
              "│        TPU <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span>        │\n",
              "│                       │\n",
              "│                       │\n",
              "└───────────────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Partition second axis of x over second mesh axis 'b', replicate first axis of matrix along each shard\n",
        "i_jy_shard = jax.device_put(matrix, mesh_sharding(P(None, 'b')))\n",
        "jax.debug.visualize_array_sharding(i_jy_shard, use_color=False)"
      ],
      "metadata": {
        "id": "qglM7sfHCFd6",
        "outputId": "f78716ea-7de7-4e8a-8cfa-2e8927b52be6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 199
        }
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┌──────────┬──────────┐\n",
              "│          │          │\n",
              "│          │          │\n",
              "│          │          │\n",
              "│          │          │\n",
              "│ TPU \u001b[1;36m0\u001b[0m,\u001b[1;36m2\u001b[0m  │ TPU \u001b[1;36m1\u001b[0m,\u001b[1;36m3\u001b[0m  │\n",
              "│          │          │\n",
              "│          │          │\n",
              "│          │          │\n",
              "│          │          │\n",
              "└──────────┴──────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┌──────────┬──────────┐\n",
              "│          │          │\n",
              "│          │          │\n",
              "│          │          │\n",
              "│          │          │\n",
              "│ TPU <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span>  │ TPU <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span>  │\n",
              "│          │          │\n",
              "│          │          │\n",
              "│          │          │\n",
              "│          │          │\n",
              "└──────────┴──────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "For a 2D matrix being sharded along a 2D device mesh, here are all the possible sharding strategies"
      ],
      "metadata": {
        "id": "EK_qrIQqIODG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![Possible Array Shardings](https://jax-ml.github.io/scaling-book/assets/img/sharding-colored5.png)"
      ],
      "metadata": {
        "id": "Sr3T40pqCdwX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<sup> Image Source: [How To Scale Your Model](https://jax-ml.github.io/scaling-book) </sup>"
      ],
      "metadata": {
        "id": "rJy0WY50CdwY"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UKtWvBtju3sy"
      },
      "source": [
        "### Choosing the Right Sharding Strategy\n",
        "\n",
        "As we discuss collectives (next section) and different parallelism strategies in (next tutorials), we will slowly do a deeper dive into how to chose sharding strategies based on your use case."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wvNt9kOLu3sy"
      },
      "source": [
        "## 3. Collective Operations\n",
        "\n",
        "Collective operations are essential for distributed algorithms where devices need to share or aggregate information. In deep learning, these are utilized in performing computations with sharded arrays."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Matrix Operations With Sharded Matrices"
      ],
      "metadata": {
        "id": "eYxP1wOzEGu1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "Q4FlzP66EJLF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Case 1: No Contracting Dimension"
      ],
      "metadata": {
        "id": "42gwCTSPERfY"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "XwrnJc3OEMGS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tbfYuMEau3sy"
      },
      "source": [
        "#### Case 2 (All-Gather): One matrix has a sharded contracting dimension\n",
        "\n",
        "All-reduce aggregates values across devices using an operation like sum, mean, max, etc. Each device ends up with the same aggregated result."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aEmIPGpRu3sy"
      },
      "outputs": [],
      "source": [
        "from jax.experimental import pjit\n",
        "\n",
        "# Create device-specific values\n",
        "def create_device_values(mesh):\n",
        "    # Create different values for each device\n",
        "    device_values = []\n",
        "    for i, device in enumerate(mesh.devices.flat):\n",
        "        value = jnp.ones((10, 10)) * (i + 1)  # Each device gets a different value\n",
        "        device_values.append(jax.device_put(value, device))\n",
        "    return device_values\n",
        "\n",
        "with mesh_1d:\n",
        "    # Example of all-reduce using pjit\n",
        "    @jax.jit\n",
        "    def all_reduce_sum(x):\n",
        "        # Explicit all-reduce\n",
        "        return jax.lax.psum(x, axis_name='devices')\n",
        "\n",
        "    # Create a sharded array where each device has a different value\n",
        "    n_devices = jax.device_count()\n",
        "    sharded_values = jnp.arange(1, n_devices + 1).reshape((n_devices, 1))\n",
        "    shardings = NamedSharding(mesh_1d, P('devices', None))\n",
        "    sharded_array = jax.device_put(sharded_values, shardings)\n",
        "\n",
        "    # Perform the all-reduce\n",
        "    result = all_reduce_sum(sharded_array)\n",
        "    print(f\"Original values:\\n{sharded_values}\")\n",
        "    print(f\"After all-reduce sum:\\n{result}\")\n",
        "\n",
        "    # Expected result: each device should have the sum of all values\n",
        "    expected_sum = jnp.sum(jnp.arange(1, n_devices + 1))\n",
        "    print(f\"Expected sum: {expected_sum}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "57IU22SOu3sy"
      },
      "source": [
        "#### Case 3 (All-Reduce): Both Matrices have sharded contracting dimensions\n",
        "\n",
        "All-gather collects values from all devices, resulting in each device having a complete copy of all values."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q3AHaoIlu3sy"
      },
      "outputs": [],
      "source": [
        "with mesh_1d:\n",
        "    # Example of all-gather\n",
        "    @jax.jit\n",
        "    def all_gather(x):\n",
        "        # Explicit all-gather\n",
        "        return jax.lax.all_gather(x, axis_name='devices', axis=0)\n",
        "\n",
        "    # Use the same sharded array from before\n",
        "    result = all_gather(sharded_array)\n",
        "    print(f\"Original values:\\n{sharded_values}\")\n",
        "    print(f\"After all-gather:\\n{result}\")\n",
        "\n",
        "    # Expected result: each device should have all values\n",
        "    expected_gather = jnp.arange(1, n_devices + 1).reshape((n_devices, 1))\n",
        "    print(f\"Expected result:\\n{expected_gather}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Other Collectives"
      ],
      "metadata": {
        "id": "DF4DVnDiFH00"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Reduce-Scatter"
      ],
      "metadata": {
        "id": "aQGQ9-52FNl_"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SL5QXIWlu3sy"
      },
      "source": [
        "#### All-to-All\n",
        "\n",
        "All-to-all exchanges slices of data between all devices. This is useful for operations like matrix transposition or redistributing data with a different sharding."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wLFnWWJ2u3sy"
      },
      "outputs": [],
      "source": [
        "if jax.device_count() >= 2:\n",
        "    with mesh_1d:\n",
        "        # Example of all-to-all\n",
        "        @jax.jit\n",
        "        def all_to_all(x):\n",
        "            # Explicit all-to-all - reshape to have a split dimension\n",
        "            return jax.lax.all_to_all(x, axis_name='devices', split_axis=0, concat_axis=1)\n",
        "\n",
        "        # Create a matrix where each device has part of the rows\n",
        "        n_devices = jax.device_count()\n",
        "        data = jnp.arange(n_devices * n_devices).reshape((n_devices, n_devices))\n",
        "        shardings = NamedSharding(mesh_1d, P('devices', None))\n",
        "        sharded_data = jax.device_put(data, shardings)\n",
        "\n",
        "        # Perform the all-to-all\n",
        "        result = all_to_all(sharded_data)\n",
        "        print(f\"Original data:\\n{data}\")\n",
        "        print(f\"After all-to-all:\\n{result}\")\n",
        "\n",
        "        # This effectively transposes the sharding from rows to columns\n",
        "        print(f\"Original sharding: {sharded_data.sharding}\")\n",
        "        print(f\"Result sharding: {result.sharding}\")\n",
        "else:\n",
        "    print(\"Need at least 2 devices for all-to-all demonstration\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0M2Sxz09u3sy"
      },
      "source": [
        "### Performance Considerations for Collective Operations\n",
        "\n",
        "When working with collective operations, consider:\n",
        "\n",
        "1. **Communication overhead**: Collective operations require device-to-device communication\n",
        "2. **Data size**: Larger transfers take more time\n",
        "3. **Network topology**: The physical connections between devices matter\n",
        "4. **Frequency**: Minimize the number of collective operations in your code\n",
        "\n",
        "Let's benchmark a simple all-reduce operation:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jeZqfrxLu3sy"
      },
      "outputs": [],
      "source": [
        "if jax.device_count() > 1:\n",
        "    # Benchmark all-reduce with different data sizes\n",
        "    sizes = [10, 100, 1000, 10000]\n",
        "    times = []\n",
        "\n",
        "    with mesh_1d:\n",
        "        for size in sizes:\n",
        "            # Create data\n",
        "            data = jnp.ones((size, size))\n",
        "            shardings = NamedSharding(mesh_1d, P('devices', None))\n",
        "            sharded_data = jax.device_put(data, shardings)\n",
        "\n",
        "            # Define and compile the all-reduce\n",
        "            @jax.jit\n",
        "            def all_reduce(x):\n",
        "                return jax.lax.psum(x, axis_name='devices')\n",
        "\n",
        "            # Warm-up\n",
        "            all_reduce(sharded_data).block_until_ready()\n",
        "\n",
        "            # Benchmark\n",
        "            start = time.time()\n",
        "            all_reduce(sharded_data).block_until_ready()\n",
        "            elapsed = time.time() - start\n",
        "            times.append(elapsed)\n",
        "\n",
        "            print(f\"Size {size}x{size}, Time: {elapsed:.6f} seconds\")\n",
        "\n",
        "    # Plot the results\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.plot(sizes, times, 'o-')\n",
        "    plt.xscale('log')\n",
        "    plt.xlabel('Matrix Size')\n",
        "    plt.ylabel('Time (seconds)')\n",
        "    plt.title('All-Reduce Performance by Data Size')\n",
        "    plt.grid(True)\n",
        "    plt.show()\n",
        "else:\n",
        "    print(\"Multiple devices required for benchmark\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kwJVgEohu3sy"
      },
      "source": [
        "## 5. Practical Example: Distributed Matrix Multiplication\n",
        "\n",
        "Let's implement a distributed matrix multiplication algorithm using sharded matrices and collective operations.\n",
        "\n",
        "We'll implement a simple version of the cannon algorithm for distributed matrix multiplication:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B0gB_Epmu3sy"
      },
      "outputs": [],
      "source": [
        "def create_matrices(n):\n",
        "    \"\"\"Create two n x n matrices.\"\"\"\n",
        "    a = jnp.ones((n, n))\n",
        "    b = jnp.ones((n, n))\n",
        "    return a, b\n",
        "\n",
        "def baseline_matmul(a, b):\n",
        "    \"\"\"Standard matrix multiplication on a single device.\"\"\"\n",
        "    return jnp.matmul(a, b)\n",
        "\n",
        "def distributed_matmul(a, b, mesh):\n",
        "    \"\"\"Distributed matrix multiplication using sharded arrays.\"\"\"\n",
        "    with mesh:\n",
        "        # Shard matrices along rows and columns\n",
        "        row_spec = P('devices', None)\n",
        "        col_spec = P(None, 'devices')\n",
        "\n",
        "        # Put A in row-sharded form and B in column-sharded form\n",
        "        a_shardings = NamedSharding(mesh, row_spec)\n",
        "        b_shardings = NamedSharding(mesh, col_spec)\n",
        "\n",
        "        a_sharded = jax.device_put(a, a_shardings)\n",
        "        b_sharded = jax.device_put(b, b_shardings)\n",
        "\n",
        "        # Define the distributed multiplication\n",
        "        @jax.jit\n",
        "        def sharded_matmul(a, b):\n",
        "            # Compute local product\n",
        "            local_product = jnp.matmul(a, b)\n",
        "            # Reduce across devices to get final result\n",
        "            return jax.lax.psum(local_product, axis_name='devices')\n",
        "\n",
        "        return sharded_matmul(a_sharded, b_sharded)\n",
        "\n",
        "# Benchmark the two approaches\n",
        "if jax.device_count() > 1:\n",
        "    # Parameters\n",
        "    n = 2000  # Matrix size\n",
        "\n",
        "    # Create matrices\n",
        "    a, b = create_matrices(n)\n",
        "\n",
        "    # Baseline (single device)\n",
        "    baseline_fn = jax.jit(baseline_matmul)\n",
        "    baseline_fn(a, b).block_until_ready()  # Warm-up\n",
        "\n",
        "    start = time.time()\n",
        "    c_baseline = baseline_fn(a, b).block_until_ready()\n",
        "    baseline_time = time.time() - start\n",
        "    print(f\"Baseline time: {baseline_time:.4f} seconds\")\n",
        "\n",
        "    # Distributed\n",
        "    start = time.time()\n",
        "    c_distributed = distributed_matmul(a, b, mesh_1d).block_until_ready()\n",
        "    distributed_time = time.time() - start\n",
        "    print(f\"Distributed time: {distributed_time:.4f} seconds\")\n",
        "\n",
        "    # Verify results are similar\n",
        "    diff = jnp.abs(c_baseline - c_distributed).max()\n",
        "    print(f\"Maximum difference: {diff}\")\n",
        "\n",
        "    # Calculate speedup\n",
        "    speedup = baseline_time / distributed_time\n",
        "    print(f\"Speedup: {speedup:.2f}x\")\n",
        "else:\n",
        "    print(\"Multiple devices required for benchmark\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "739BfzUPu3sy"
      },
      "source": [
        "## 6. Advanced Topics\n",
        "\n",
        "### Automatic Sharding with SPMD (Single Program Multiple Data)\n",
        "\n",
        "JAX provides the `pjit` (Partitioned JIT) API for automatic sharding. With `pjit`, you specify the sharding of inputs and outputs, and JAX determines the optimal intermediate shardings."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "76Jy_ICau3sy"
      },
      "outputs": [],
      "source": [
        "from jax.experimental import pjit\n",
        "\n",
        "def auto_sharded_matmul(a, b, mesh):\n",
        "    \"\"\"Matrix multiplication with automatic sharding.\"\"\"\n",
        "    with mesh:\n",
        "        # Define input and output specs\n",
        "        in_specs = (P('devices', None), P(None, 'devices'))  # Row and column sharding\n",
        "        out_spec = P(None, None)  # Output on all devices\n",
        "\n",
        "        # Define the pjit function\n",
        "        pjit_matmul = pjit.pjit(\n",
        "            lambda x, y: jnp.matmul(x, y),\n",
        "            in_shardings=in_specs,\n",
        "            out_shardings=out_spec\n",
        "        )\n",
        "\n",
        "        return pjit_matmul(a, b)\n",
        "\n",
        "if jax.device_count() > 1:\n",
        "    # Try the auto-sharded version\n",
        "    a, b = create_matrices(2000)\n",
        "\n",
        "    start = time.time()\n",
        "    c_auto = auto_sharded_matmul(a, b, mesh_1d).block_until_ready()\n",
        "    auto_time = time.time() - start\n",
        "    print(f\"Auto-sharded time: {auto_time:.4f} seconds\")\n",
        "\n",
        "    # Compare with our manual implementation\n",
        "    if 'distributed_time' in locals():\n",
        "        print(f\"Manual vs Auto ratio: {distributed_time / auto_time:.2f}x\")\n",
        "else:\n",
        "    print(\"Multiple devices required for demonstration\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zVjjyO9xu3sy"
      },
      "source": [
        "### Custom Sharding Rules\n",
        "\n",
        "JAX allows defining custom partitioning rules for operations. This is useful for operations where the default partitioning might not be optimal."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i84aKJ1ou3sy"
      },
      "outputs": [],
      "source": [
        "# Advanced topic: Custom partitioning rules\n",
        "from functools import partial\n",
        "\n",
        "def custom_matmul_rule(mesh):\n",
        "    # This is a simplified example - real custom rules would be more complex\n",
        "    def matmul_with_custom_rule(x, y):\n",
        "        # Custom implementation that's aware of sharding\n",
        "        return jnp.matmul(x, y)\n",
        "\n",
        "    return matmul_with_custom_rule\n",
        "\n",
        "# Example usage (conceptual)\n",
        "# my_custom_matmul = custom_matmul_rule(mesh_1d)\n",
        "# result = my_custom_matmul(sharded_a, sharded_b)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "93CxXrqgu3sz"
      },
      "source": [
        "## 8. Conclusion\n",
        "\n",
        "In this tutorial, we've explored JAX's powerful capabilities for distributed computation using sharded matrices and collective operations. We've covered:\n",
        "\n",
        "1. **Setting up a device mesh** for organizing available devices\n",
        "2. **Creating and using sharded matrices** to distribute data across devices\n",
        "3. **Different sharding strategies** and when to use them\n",
        "4. **Collective operations** for efficient device-to-device communication\n",
        "5. **Implementing distributed algorithms** using these primitives\n",
        "6. **Automatic sharding** with pjit for easier distributed programming\n",
        "7. **Application to distributed training** for machine learning models\n",
        "\n",
        "JAX's sharding capabilities enable efficient scaling of numerical computations across multiple devices, making it a powerful tool for large-scale machine learning and scientific computing.\n",
        "\n",
        "### Further Resources\n",
        "\n",
        "- [JAX Documentation](https://jax.readthedocs.io/)\n",
        "- [JAX Docs: Distributed arrays and automatic parallelization](https://docs.jax.dev/en/latest/notebooks/Distributed_arrays_and_automatic_parallelization.html)\n",
        "- [JAX Docs: Manual parallelism with `shard_map`](https://docs.jax.dev/en/latest/notebooks/shard_map.html)\n",
        "- [How to Scale Your Model: Sharded Matrices and How to Multiply Them](https://jax-ml.github.io/scaling-book/sharding/)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hovJ0-9yu3sz"
      },
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.9"
    },
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "gpuType": "V28"
    },
    "accelerator": "TPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}